{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#the-practical-haplotype-graph-phg","title":"The Practical Haplotype Graph (PHG)","text":"<p>With improved sequencing technology, sequencing costs are decreasing rapidly. But bioinformatics challenges for processing data and inferring genotypes have increased. To address this, we have developed a general, graph-based, computational framework called the Practical Haplotype Graph (PHG), that can be used with a variety of skim sequencing methods to infer high-density genotypes directly from low-coverage sequence. The idea behind the PHG is that in a given breeding program, all parental genotypes can be sequenced at high coverage, and loaded as parental haplotypes in a relational database. Progeny can then be sequenced at low coverage and used to infer which parental haplotypes/genotypes from the database are most likely present in a given progeny individual. </p> <p>The PHG is a trellis graph-based representation of genic and intergenic regions (called reference ranges) which represent diversity across and between taxa.  It can be used to: create custom genomes for alignment, call rare alleles, impute genotypes, and efficiently store genomic data from many lines (i.e. reference, assemblies, and other lines). Skim sequences generated for a given taxon are aligned to the graph to identify the haplotype node at a given anchor. All the anchors for a given taxon are processed through a Hidden Markov Model (HMM) to identify the most likely path through the graph. Path information is used to identify variants (SNPs). Low cost sequencing technologies, coupled with the PHG, facilitate the genotyping of large number of samples to increase the size of training populations for genomic selection models. This can in turn increase predictive accuracy and selection intensity in a breeding program.</p> <p>A more detailed introduction can be seen in Peter Bradbury's PHG presentation slides from the 2018 Plant and Animal Genome conference (PAG).</p>"},{"location":"#pipline-versions","title":"Pipline versions","text":""},{"location":"#phg-version-10-and-beyond-variants-stored-in-external-gvcf-files","title":"PHG version 1.0 and beyond (variants stored in external GVCF files)","text":"<p>PHG Versions 1.0 and later store haplotype variant information in external GVCF files.  These files need to be bgzipped and tabix'd and then stored to a central location that is accessible for download when necessary for PHG graph processing.  The location of these files is stored in the PHG database table \"genome_file_data\" when the haplotypes they reference are loaded to the PHG database.  This method of variant storage/retrieval allows for the data to be stored in a standard format available for procesing with other bioinformatics applications.</p> <p>For instructions on using a database based on the latest PHG version please click here</p>"},{"location":"#phg-versions-0040-and-below-variants-stored-in-database-tables","title":"PHG versions 0.0.40 and below (variants stored in database tables )","text":"<p>The original versions of the PHG software stored variant information in database tables.  While it was convenient to have all the data in a single place, these tables became very large and adversely affected database performance. As the number of variants increases, the processing time involving these variants also increases, often to levels where the program appears hung.  </p> <p>The PHG software has since been re-written and starting with PHG version 1.0, variant data is now stored external to the database in GVCF formatted files.  These files are bgzipped and have an accompanying tabix'd file. The GVCF files are named .gvcf.gz and the tabix'd files are named .gvcf.gz.tbi.  It is the responsibility of the user to arrange for these files to be stored to an accessible, central location from which they can be downloaded when necessary for PHG graph processing. <p>While the PHG software uses liquibase migration tools to migrate basic database changes from one PHG version to another, it is not available to upgrade from a PHG version 0.0.40 or below to PHG version 1.0 or above. The changes between these 2 versions are too great for liquibase to be viable. </p> <p>If you have an existing PHG database created with version 0.0.40 or earlier you can continue to process using your existing PHG docker image.  Alternatively, you may rebuild your database using the new software. </p> <p>For instructions on using a database based on the older PHG version please click here </p>"},{"location":"#asking-questions","title":"Asking questions","text":"<p>Please use Biostars to ask for help. Instructions for using Biostars are here</p>"},{"location":"#reporting-bugs","title":"Reporting bugs","text":"<p>The PHG is under active development. If you find a bug, please submit a pull request so that we can address it.</p>"},{"location":"#other-ways-to-use-the-phg","title":"Other ways to use the PHG","text":"<p>The rPHG package allows users to explore PHG databases in R.</p> <p>The rTASSEL package allows users to run TASSEL through R.</p>"},{"location":"#phg-papers","title":"PHG Papers","text":"<p>Jensen et al. 2020: A sorghum practical haplotype graph facilitates genome\u2010wide imputation and cost\u2010effective genomic prediction</p>"},{"location":"#wheat-phg-hackathon-february-24-28-2020-cornell-university-ithaca","title":"Wheat PHG Hackathon (February 24-28, 2020): Cornell University - Ithaca","text":"<p>Presentations</p>"},{"location":"#wheat-cap-phg-workshop-july-8-12-2019-cornell-university-ithaca","title":"Wheat CAP PHG workshop (July 8-12, 2019): Cornell University - Ithaca","text":"<p>Agenda</p> <p>Presentations</p>"},{"location":"#phg-workshop-august-17-18-2018-irri-philippines","title":"PHG Workshop (August 17-18, 2018): IRRI - Philippines","text":"<p>Agenda</p> <p>Presentations</p>"},{"location":"#phg-workshop-june-4-8-2018-cornell-university-ithaca","title":"PHG Workshop (June 4-8, 2018): Cornell University - Ithaca","text":"<p>Agenda</p> <p>Presentations</p>"},{"location":"#phg-pag-january-13-17-2018-plant-and-animal-genome-xxvi-conference-san-diego","title":"PHG @ PAG (January 13-17, 2018): Plant and Animal Genome XXVI Conference - San Diego","text":"<p>2018 PAG poster</p> <p>PHG presentation slides</p>"},{"location":"Home_variantTables/","title":"How to use the PHG","text":"<p>A Practical Haplotype Graph can be used for many types of data processing and analysis. You can build a PHG from genome assembly-quality genomes or whole-genome resequencing data and use it to impute either variants or haplotypes, both for taxa within the database and for new taxa that are not included in the DB. A non-exhaustive list of possible use cases includes:</p> <ol> <li>I want to populate a db using WGS</li> <li>I want to populate a db using Assemblies </li> <li>I want to create consensus haplotypes</li> <li>I have skim sequencing and I want variant information</li> <li>I have sparse SNPs and I want to imputate more SNPs</li> <li>I have a bunch of old/varied sequencing data and I want to merge them to get one set of variant calls on the same set of loci</li> <li>I want to phase my heterozygous material by finding paths through db</li> <li>I want to phase the assemblies I am putting into the db</li> <li>I want haplotype information so I can choose populations to fine map traits</li> <li>I want haplotype information for association analysis and/or GS</li> <li>I want to understand recombination/haplotype formation in my population/species</li> <li>I want to identify ancestral haplotypes &amp; study haplotype quantity/diversity</li> <li>I want to study rare haplotypes</li> <li>I have some material and want to figure out what its parents are</li> <li>I have some material and want to figure out what it is similar/related to</li> <li>I want to do chromosome painting</li> <li>I want to check the quality of some assemblies</li> </ol> <p>The data needed for any of these use cases can be produced through one of two pipelines. Users can either create and populate a new PHG database, or can download and use an existing PHG database to impute variant or haplotype information. Wrapper scripts that run the whole pipeline are available, or you can follow the decision flow charts to move through each pipeline on your own. More information about each step can be found by clicking the link associated with that step. </p>"},{"location":"Home_variantTables/#download-and-install-the-phg-docker","title":"Download and install the PHG Docker","text":"<p>The PHG code can be downloaded and run with either Docker, Singularity, or Conda. Click on the Step 0 image to learn more.</p> <p>Step 0: Download PHG Docker </p>"},{"location":"Home_variantTables/#create-a-phg-database-and-populate-it-with-haplotypes","title":"Create a PHG database and populate it with haplotypes","text":"<p>The first step is building an empty PHG database and optionally splitting reference range intervals into different groups of interest. The second step is adding haplotypes (from assemblies, WGS, or both) to that database, with an optional consensus building step. These steps can be run simultaneously. Click on the image to learn more.</p> <p>Step 1: create an initial PHG with reference data </p> <p>Step 2: add haplotypes </p>"},{"location":"Home_variantTables/#use-an-existing-phg-database-to-impute-variants-or-haplotypes","title":"Use an existing PHG database to impute variants or haplotypes","text":"<p>Once you have a PHG database with haplotypes, the next step is to use that database to impute variants or haplotypes for new taxa.  If you have not already downloaded the PHG docker, follow Step 0 above. You may also need to update the PHG database (Step 2.5)  before moving on to Step 3. The pipeline plugins, MakeInitialPHGDBPipelinePlugin, PopulatePHGDBPipelinePlugin, and ImputePipelinePlugin, always run this check as a first step. As a result, if you use those plugins, step 2.5 does not need to be run. Also, if you have just completed Step 2 then you can skip step 2.5. If you are running one of the component plugins outside the pipeline and want to make sure that your database is in sync with the current version of the PHG software, then run step 2.5.</p> <p>Step 2.5: Optional (recommended) Update PHG database schema</p> <p>Step 3: impute variants or haplotypes </p>"},{"location":"Home_variantTables/#additional-information","title":"Additional information","text":""},{"location":"Home_variantTables/#phg-config-files","title":"PHG config files","text":"<p>The current version of the pipeline expects almost all parameters to be set in a config file. While it is possible to  set parameters from the command line for individual plugins, the number of parameters makes doing so impractical. Also, the pipeline plugins call other plugins in turn. As a result when using the pipeline plugins, some parameters must be specified in a config file. Because of the number of parameters, we provide two separate sample config files, one for steps 1 and 2 (creating a database and populating it with haplotypes)  and one for step 3 (imputing variants). See those pages for details.</p>"},{"location":"Home_variantTables/#phg-key-files","title":"PHG key files","text":"<p>You will need three types of key files with various portions of the PHG code. They are as follows:</p> <ol> <li>Assemblies key file - associates chromosome fastas for assembly genomes with the name you ewant that taxon to have within the PHG database. These can include information for a single assembly, or for multiple assemblies.</li> <li>Haplotype key file - associates specific WGS files with the name you want that taxon to have within the PHG database.</li> <li>Pathfinding key file - associates paired-end read files for pathfinding step.</li> </ol>"},{"location":"Home_variantTables/#example-database","title":"Example database","text":"<p>A small example database for testing the PHG can be found on the Example Database page </p>"},{"location":"Home_variantTables/#phg-application-programming-interface-api","title":"PHG Application Programming Interface (API)","text":"<p>API Documentation</p> <p></p>"},{"location":"Home_variantTables/#phg-database-schema","title":"PHG Database Schema","text":"<p>Return to Wiki home</p>"},{"location":"Home_variantsInGVCFFiles/","title":"How to use the PHG","text":"<p>A Practical Haplotype Graph can be used for many types of data processing and analysis. You can build a PHG from genome assembly-quality genomes or whole-genome resequencing data and use it to impute either variants or haplotypes, both for taxa within the database and for new taxa that are not included in the DB. A non-exhaustive list of possible use cases includes:</p> <ol> <li>I want to populate a db using WGS</li> <li>I want to populate a db using Assemblies </li> <li>I want to create consensus haplotypes</li> <li>I have skim sequencing and I want variant information</li> <li>I have sparse SNPs and I want to imputate more SNPs</li> <li>I have a bunch of old/varied sequencing data and I want to merge them to get one set of variant calls on the same set of loci</li> <li>I want to phase my heterozygous material by finding paths through db</li> <li>I want to phase the assemblies I am putting into the db</li> <li>I want haplotype information so I can choose populations to fine map traits</li> <li>I want haplotype information for association analysis and/or GS</li> <li>I want to understand recombination/haplotype formation in my population/species</li> <li>I want to identify ancestral haplotypes &amp; study haplotype quantity/diversity</li> <li>I want to study rare haplotypes</li> <li>I have some material and want to figure out what its parents are</li> <li>I have some material and want to figure out what it is similar/related to</li> <li>I want to do chromosome painting</li> <li>I want to check the quality of some assemblies</li> </ol> <p>The data needed for any of these use cases can be produced through one of two pipelines. Users can either create and populate a new PHG database, or can download and use an existing PHG database to impute variant or haplotype information. Follow the decision flow charts to see how to move through each pipeline. More information about each step can be found by clicking the link associated with that step. </p>"},{"location":"Home_variantsInGVCFFiles/#download-and-install-the-phg-docker","title":"Download and install the PHG Docker","text":"<p>The PHG code can be downloaded and run with either Docker or Singularity.</p> <p>A. Run PHG with Docker</p> <p>B. Run PHG with Singularity</p>"},{"location":"Home_variantsInGVCFFiles/#create-a-phg-database-and-populate-it-with-haplotypes","title":"Create a PHG database and populate it with haplotypes","text":"<p>The first step is building an empty PHG database and optionally splitting reference range intervals into different groups of interest. The second step is adding haplotypes (from assemblies, WGS, or both) to that database, with an optional consensus building step. These steps can be run simultaneously. Click on the image to learn more.</p> <p>Step 1: create an initial PHG with reference data </p> <p>Step 2: add haplotypes </p>"},{"location":"Home_variantsInGVCFFiles/#use-an-existing-phg-database-to-impute-variants-or-haplotypes","title":"Use an existing PHG database to impute variants or haplotypes","text":"<p>Once you have a PHG database with haplotypes, the next step is to use that database to impute variants or haplotypes for new taxa.  If you have not already downloaded the PHG docker, follow Step 0 above. You may also need to update the PHG database (Step 2.5)  before moving on to Step 3. The pipeline plugins, MakeInitialPHGDBPipelinePlugin, PopulatePHGDBPipelinePlugin, and ImputePipelinePlugin, always run this check as a first step. As a result, if you use those plugins, step 2.5 does not need to be run. Also, if you have just completed Step 2 then you can skip step 2.5. If you are running one of the component plugins outside the pipeline and want to make sure that your database is in sync with the current version of the PHG software, then run step 2.5.</p> <p>Step 2.5: Optional (see above) Update PHG database schema</p> <p>Step 3: impute variants or haplotypes </p>"},{"location":"Home_variantsInGVCFFiles/#additional-information","title":"Additional information","text":""},{"location":"Home_variantsInGVCFFiles/#phg-config-files","title":"PHG config files","text":"<p>The current version of the pipeline expects almost all parameters to be set in a config file. While it is possible to  set parameters from the command line for individual plugins, the number of parameters makes doing so impractical. Also, the pipeline plugins call other plugins in turn. As a result when using the pipeline plugins, some parameters must be specified in a config file. Because of the number of parameters, we provide two separate sample config files, one for steps 1 and 2 (creating a database and populating it with haplotypes)  and one for step 3 (imputing variants). See those pages for details.</p>"},{"location":"Home_variantsInGVCFFiles/#phg-key-files","title":"PHG key files","text":"<p>You will need three types of key files with various portions of the PHG code. They are as follows:</p> <ol> <li>Assemblies key file - associates chromosome fastas for assembly genomes with the name you ewant that taxon to have within the PHG database. These can include information for a single assembly, or for multiple assemblies.</li> <li>Haplotype key file - associates specific WGS files with the name you want that taxon to have within the PHG database.</li> <li>Pathfinding key file - associates paired-end read files for pathfinding step.</li> </ol>"},{"location":"Home_variantsInGVCFFiles/#example-database","title":"Example database","text":"<p>A small example database for testing the PHG can be found on the Example Database page </p>"},{"location":"Home_variantsInGVCFFiles/#phg-application-programming-interface-api","title":"PHG Application Programming Interface (API)","text":"<p>API Documentation</p> <p></p>"},{"location":"Home_variantsInGVCFFiles/#phg-database-schema","title":"PHG Database Schema","text":"<p>Return to Wiki Home</p>"},{"location":"Home_version1/","title":"Home version1","text":""},{"location":"Home_version1/#practical-haplotype-graph-phg","title":"Practical Haplotype Graph (PHG)","text":"<p>With improved sequencing technology, sequencing costs are declining very rapidly. But bioinformatics challenges for processing data and inferring genotypes have increased. To address this, we have developed a general, graph-based, computational framework called the Practical Haplotype Graph (PHG), that can be used with a variety of skim sequencing methods to infer high-density genotypes directly from low-coverage sequence. The idea behind the PHG is that in a given breeding program, all parental genotypes can be sequenced at high coverage, and loaded as parental haplotypes in a relational database. Progeny can then be sequenced at low coverage and used to infer which parental haplotypes/genotypes from the database are most likely present in a given progeny.   </p> <p>Practical Haplotype Graph:  The PHG is a trellis graph based representation of genic and intergenic regions (called reference ranges) which represent diversity across and between taxa.  It can be used to: create custom genomes for alignment, call rare alleles, impute genotypes, and efficiently store genomic data from many lines (i.e. reference, assemblies, and other lines). Skim sequences generated for a given taxon are aligned to consensus sequences in the PHG to identify the haplotype node at a given anchor. All the anchors for a given taxon are processed through a Hidden Markov Model (HMM) to identify the most likely path through the graph. Path information is used to identify the variants (SNPs). Low cost sequencing technologies, coupled with the PHG, facilitate the genotyping of large number of samples to increase the size of training populations for genomic selection models. This can in turn increase predictive accuracy and selection intensity in a breeding program.</p> <p>A more detailed introduction can be seen in Peter Bradbury's PHG presentation slides from the 2018 Plant and Animal Genome conference (PAG).</p> <p>View also the 2018 PAG poster.</p>"},{"location":"Home_version1/#phg-docker-pipeline","title":"PHG Docker Pipeline","text":""},{"location":"Home_version1/#phg-application-programming-interface-api","title":"PHG Application Programming Interface (API)","text":"<p>API Documentation</p> <p></p>"},{"location":"Home_version1/#phg-database-schema","title":"PHG Database Schema","text":""},{"location":"Home_version1/#phg-database-support","title":"PHG Database Support","text":"<p>The PHG pipeline currently supports SQLite and postgres databases outside of a docker, as well as postgres database inside a docker. Our intent is for users to migrate to the dockerized postgres database.</p> <p>Access to PHG databases in all cases is through a config file that specifies the host:port, user,  password, db name, and the database type (sqlite or postgres).  An example of an SQLite config file is below. NOTE: The user and password parameters are not used for SQLite.  The sqlite db file must include a path that is relative to it's location within the PHG docker scripts. This will be discussed when each script is discussed.</p> <pre><code>#!python\n\nhost=localHost\nuser=sqlite\npassword=sqlite\nDB=/tempFileDir/outputDir/phgTestDB_mapq48.db\nDBtype=sqlite\n</code></pre> <p>An example of a postgres config file is below:</p> <pre><code>#!python\n\nhost=199.17.0.7:5432\nuser=postgres\npassword=postgresPWD\nDB=phgdb\nDBtype=postgres\n</code></pre>"},{"location":"Home_version1/#phg-postgres-docker","title":"PHG Postgres Docker","text":""},{"location":"Home_version1/#phg-database-migration","title":"PHG Database Migration","text":""},{"location":"Home_version1/#phg-docker-instances","title":"PHG Docker Instances","text":"<p>The PHG is deployed as a Docker image.  Docker is an open source platform that wraps applications into an image to be deployed on a host system.  Docker containers are instances of a Docker image that may run simultaneously on the same host.  See the Docker documentation here for details.</p> <p>The PHG Docker image is built on the GATK image from docker hub.  To this image the following analysis tools are added:  VIM, BWA, minimap2, jbwa, htslib, bcftools, SAMTools and TASSEL-5 with the PHG jar.  In addition, scripts for loading and processing the PHG are added. </p> <p>To pull the PHG images from docker hub, run the following commands (the second pull is only necessary if you are pulling a new PHG image to run against an existing database file):</p> <pre><code>#!python\n\ndocker pull maizegenetics/phg\ndocker pull maize genetics/phg_liquibase\n\n</code></pre> <p>If on a Cornell CBSU machine, use docker1 as below:</p> <pre><code>#!python\n\ndocker1 pull maizegenetics/phg\ndocker1 pull maizegenetics/phg_liquibase\n</code></pre> <p>ON cbsu:  NOTE: If unable to pull a docker image from docker hub on a Cornell CBSU machine, check   if you are on an enhanced security machine.  cbsumm01, 03, 11 and others are.   You can see this if you go to basic biohpc reservations pages, and look   at the machines you're able to reserve.  They will have a note in the description   about enhanced security.</p> <p>\"certificate signed by unknown authority\"  (see above - enhanced security machine)</p> <p></p>"},{"location":"Home_version1/#docker-pipeline-phase-1-details","title":"Docker Pipeline Phase 1 Details","text":"<p>Phase 1 of the pipeline is concerned with populating the database with reference, assembly (optional) and haplotype sequence data.  In addition, consensus anchor sequences are determined from those haplotype sequences currently in the database.  To create the data for each of these steps there is a shell script provided within the Docker image. These scripts live at the root directory in the Docker linux tree.   The functionality performed in each script is defined below.  </p> <p>These scripts may be run from within a Docker container or on the command line that creates the docker container.  The description of each script includes an example Docker command that creates a container and runs the specified script.</p> <p>The first step is to create reference intervals to be used in the pipeline.  The reference intervals are generally conserved regions identified from your reference genome.  They may be based on any algorithm you choose.  Sometimes it is the gene regions from gff files, but could be exon or any other regions, or even something arbitary, e.g. every 1000 bps of the reference genome.  Any region not specified in the reference intervals file will be considered as an inter-genic (or non-conserved) region.</p> <p>The output required is a tab-delimited file in bedfile format (positions are 0-based, start position is inclusive, end position is exclusive), containing columns for \"chr\", \"startpos\" and \"endpos\" as the first 3 columns.  Other columnns and data may be present but will be ignored.  Headerlines must begin with # and will be skipped.  </p> <p>After successfully creating the reference intervals file the LoadGenomeIntervals.sh script must be run to create the initial database with reference intervals and haplotypes for the reference genome.</p>"},{"location":"Home_version1/#loadgenomeintervalssh","title":"LoadGenomeIntervals.sh","text":"<p>The following pipeline steps may be run or skipped, depending on your intended use of the PHG database.  See the individual wiki pages for details.</p>"},{"location":"Home_version1/#loadassemblyanchorssh","title":"LoadAssemblyAnchors.sh","text":""},{"location":"Home_version1/#parallelassemblyanchorsloadsh","title":"ParallelAssemblyAnchorsLoad.sh","text":""},{"location":"Home_version1/#createhaplotypes","title":"CreateHaplotypes","text":""},{"location":"Home_version1/#createconsensish","title":"CreateConsensi.sh","text":""},{"location":"Home_version1/#docker-pipeline-phase-2-details","title":"Docker Pipeline Phase 2 Details","text":"<p>Phase 2 of the pipeline uses the stored haplotype graph data to infer genotypes from skim sequences and is split into 2 sub-phases. A path through the graph may be determined based on specific haplotype nodes or based on consensus sequences.  Once a path is determined, it is stored in the database paths table.  The haplotype node ids from the haplotypes table that comprise the path are exported to a file.  The paths can then be used to export a VCF file containing the Genotyped SNPs for the taxon processed.</p> <p>The Two sub phases are as follows.</p> <ul> <li> <p>The first phase(Phase 2a) simply extracts the haplotypes from the DB and writes a pangenome fasta file.  Then the Fasta File is indexed using minimap2. This step only needs to be executed once</p> </li> <li> <p>Phase 2b then takes this index file and aligns reads using FindPathMinimap2.sh then can export the Path to a VCF using ExportPath.sh</p> </li> </ul> <p>The shell scripts to find and export the paths are located in the root directory of the Docker image.  These scripts are described below. </p> <p>As with the Phase 1 pipeline, these scripts may be run from within a Docker container or on the command line that creates the docker container.  The description of each script includes an example Docker command that creates a container and runs the specified script.</p>"},{"location":"Home_version1/#indexpangenomesh","title":"IndexPangenome.sh","text":""},{"location":"Home_version1/#findpathminimap2sh","title":"FindPathMinimap2.sh","text":""},{"location":"Home_version1/#exportpathsh","title":"ExportPath.sh","text":""},{"location":"Home_version1/#how-to-ask-for-help-and-report-bugs","title":"How to ask for help and report bugs","text":""},{"location":"Home_version1/#asking-questions","title":"Asking questions","text":"<p>Please use Biostars to ask for help. Instructions for using Biostars are here</p>"},{"location":"Home_version1/#phg-workshop-june-4-8-2018-cornell-university-ithaca","title":"PHG Workshop (June 4-8, 2018): Cornell University - Ithaca","text":"<p>Agenda</p> <p>Presentations</p>"},{"location":"Home_version1/#phg-workshop-august-17-18-2018-irri-philippines","title":"PHG Workshop (August 17-18, 2018): IRRI - Philippines","text":"<p>Agenda</p> <p>Presentations</p>"},{"location":"PHG_GUI/","title":"Documentation for Genotyping using the PHG within TASSEL GUI","text":""},{"location":"PHG_GUI/#steps-original-pipeline","title":"Steps Original Pipeline:","text":"<ol> <li>Update Config file with local directory.  On windows, the '/' character can be used to find the correct path</li> <li>Extract the HaplotypeGraph from the DB using HaplotypeGraphBuilderPlugin.</li> <li>Create Kmer index.</li> <li>Extract out Haplotype Counts for the Skim Sequence reads using FastqToHapCountPlugin.</li> <li>Find Paths through the HaplotypeGraph for each taxon using HapCountBestPathToTextPlugin.</li> <li>Create a VCF file using PathsToVCFPlugin.</li> </ol>"},{"location":"PHG_GUI/#create-screen-shots-for-each-step-showing-how-it-works","title":"Create Screen shots for each Step showing how it works.","text":""},{"location":"PHG_GUI/#step-1-update-the-config-file","title":"Step 1.  Update the Config file:","text":"<p>To make the PHG TASSEL code to work, you will need to Update the config.txt file to match your local folder structures.  In particular, DB= needs to have the full path to your local Database. </p> <p>Windows Note: You can use the '/' character to separate folders in this path.  If you use the default \"\\\" you will need to change it to \"/\" or \"\\\" for it to work correctly.</p> <p></p>"},{"location":"PHG_GUI/#step-2-extract-the-haplotype-graph","title":"Step 2. Extract the Haplotype Graph","text":"<p>Click the PHG menu and select \"Haplotype Graph Builder\"</p> <p></p> <p>In the Pop-up, fill in the Config File path and the Consensus Method and make sure \"Include Variant Contexts\" is checked.  Also optionally you can specify the BED file of the regions you are interested of genotyping. This BED file will need to match the reference ranges used to create the DB. </p> <p>You can use the Browse button to find the config file and the Anchor BED file in your file system.  The Consensus Method will be dependent on the database.  This is likely CONSENSUS in the tutorial data.  </p> <p>To make sure we have pulled down the Graph correctly, it may be a good idea to view the graph.  Make sure the PHG object is selected in the File List and then select \"View Graph\" from the PHG menu and click the \"OK\" button when the window pops up.  This will open up a new window where the graph can be viewed.</p> <p></p>"},{"location":"PHG_GUI/#step-3-create-kmer-index","title":"Step 3. Create Kmer Index","text":"<p>This step will create a k-mer index based on the haplotypes in the Graph loaded by the previous step.  By default this creates 32-mers and will maintain a list of each haplotype which contains that 32-mer.  </p> <p>This will save a serialized binary file to be loaded in the haplotype count step.  In the future this may be stored in the Database for ease of use.</p> <p>To Run, make sure  you have selected the PHG object from the Data Tree and then select \"Index Kmers\" from the PHG menu.</p> <p>You will then need to specify a Kmer map file to be exported.  Use the \"Browse\" button to find a location and give it a file name.  The extension should be set to \".ser\" for serialized Java Object.  If you would like to tune the kmers, set the Kmer Size to some other number. Please note that this step will take a bit of time(5-10 minutes on the training dataset) and can use a bit of RAM depending on the size of your Graph.</p> <p></p>"},{"location":"PHG_GUI/#step-4-extract-out-haplotype-counts-for-the-skim-sequence-reads-using-fastqtohapcountplugin","title":"Step 4.  Extract out Haplotype Counts for the Skim Sequence reads using FastqToHapCountPlugin.","text":"<p>This step will extract out the K-mers found in the Skim Sequence reads and attempt to generate counts for each haplotype in the graph.  </p> <p>For this to work, be sure to select both the PHG object and the Kmer_index object from the file tree in TASSEL.  This can be done by highlighting one object and then holding Command(Mac) or Control(Windows) and select the other object.  </p> <p>If you do not have a Kmer_index object in the File Tree, but have already created the kmer file from a previous Genotyping run, you can import the kmer file from your file system in the plugin.  This import may take a bit of time.</p> <p>Once both are selected(Or just the PHG if you need to load the file), click on Kmer Hap Counter in the PHG menu.</p> <p>Then the plugin will appear.  Fill in the Config file by clicking the Browse Button and finding the file in your file system.   Then Specify the Read Directory to be the folder location where your samples to be genotyped exist.  If you only want to run one sample through the Genotyping process, you will need to copy that file into a folder by itself. Optionally if you would like to export the count files for further analysis, specify the ExportHaploFile to be a new directory and a name that will be used to generate a count file for each taxon.  Finally, specify a method name for record in the DB.  This method name is needed so you can extract out a given set of Haplotype Counts from the Database easily.  It will be used in the next step.  Click \"Ok\" to begin the kmer counting.</p> <p></p>"},{"location":"PHG_GUI/#step-5-find-paths-through-the-haplotypegraph-for-each-taxon-using-hapcountbestpathtotextplugin","title":"Step 5. Find Paths through the HaplotypeGraph for each taxon using HapCountBestPathToTextPlugin.","text":"<p>This step uses the counts generate for each taxon and attempts to find the most probable path through the graph.  This is to figure out what haplotype is the most likely for each Reference Range in the Graph.  This is accomplished using an HMM where the counts create the Emission Probabilities.</p> <p></p> <p>To run this, first specify the Config File using the \"Browse\" button.  You will then need to specify the \"Output Dir\" which is the output directory to hold the Path files, a \"HapCount Method\" which will match the method named in the previous step and a new \"Path method\" which defines the method for the paths to be stored in the DB.</p>"},{"location":"PHG_GUI/#step-6-load-in-the-paths","title":"Step 6. Load in the Paths","text":"<p>This step loads in the Path files created in the previous step.  This will allow those paths to be used when making a VCF file.  Click on the \"Paths Import\" menu option in the PHG menu and browse for the Input Path directory.  Click \"OK\" and the Paths will be loaded into TASSEL</p> <p></p>"},{"location":"PHG_GUI/#step-7-create-a-vcf-file-using-pathstovcfplugin","title":"Step 7. Create a VCF file using PathsToVCFPlugin.","text":"<p>Once we have the Paths loaded into TASSEL, we can combine the paths for each taxon and make a single VCF file. We create the VCF file by following each Path for each taxon and extracting out the SNPs for each Haplotype along the Path.  Then we Combine the VCF records for all the taxon we genotypes into a single TASSEL GenotypeTable and export it to a file.  This VCF file is the result of the genotyping process.</p> <p>To Run this step, make sure you have the PHG and the Paths object selected in the Data Tree of TASSEL.  Then click on the \"Paths To VCF\" option in the PHG menu.  In the Plugin Pop-up, specify a output VCF file.  You can use the \"Browse\" button to make this easier.  Click OK once you are read to run. </p> <p></p>"},{"location":"getting-help/","title":"How to ask questions about the PHG","text":""},{"location":"getting-help/#step-0-create-a-userid-on-biostars","title":"Step 0: Create a userID on Biostars","text":"<p>Skip to step 1 if you already have a user ID</p> <ul> <li> <p>Click on the Login tab near the top of the home page or on this link: https://www.biostars.org/accounts/signup/</p> </li> <li> <p>On the login page, sign in with either your Google credentials or use your own custom ID and password.</p> </li> </ul>"},{"location":"getting-help/#step-1-ask-your-question","title":"Step 1: Ask your question","text":"<ul> <li> <p>Log in and find the new posts through the \u201cNew Post\u201d button on the top right-hand side of the screen</p> </li> <li> <p>On the New Post page, provide the following:</p> <ul> <li> <p>Post Title: A clearly defined title for your question</p> </li> <li> <p>Post Type: Question</p> </li> <li> <p>Post Tags: To ensure your question is answered by the Buckler Lab staff, please use:</p> <p>phg - questions relating to the practical haplotype graph (PHG) framework</p> <p>rphg - questions relating to the R frontend for the PHG</p> </li> <li> <p>Enter your post below: your clearly stated, unambiguous question in proper formatting.</p> </li> </ul> </li> <li> <p>Click submit </p> </li> </ul>"},{"location":"getting-help/#step-2-wait-for-potential-remedies-and-follow-up-questions","title":"Step 2: Wait for potential remedies and follow up questions","text":"<p>To follow the phg or rphg tags or receive email notifications for PHG or rPHG questions:</p> <ul> <li> <p>Click on your userid on the main biostars page</p> </li> <li> <p>Click edit profile. You can enter tags to show up on myTags or email there</p> </li> </ul> <p>Return to Wiki Home</p>"},{"location":"PHG%20/%20BrAPI%20Server%20on%20CBSU/","title":"BrAPI Server on CBSU","text":""},{"location":"PHG%20/%20BrAPI%20Server%20on%20CBSU/#phg-brapi-setup","title":"PHG / BrAPI Setup","text":"<ul> <li>git clone https://@bitbucket.org/bucklerlab/phg_webktor_service.git <li>cd phg_webktor_service</li> <li>git pull (Do this to get all committed code. This repository should be exactly the master branch. NOT your development environment)</li> <li>Edit phg_webktor_service/src/main/resources/application.conf</li> <li>Edit phg_webktor_service/docker/config.txt</li>"},{"location":"PHG%20/%20BrAPI%20Server%20on%20CBSU/#compile-phg-brapi-server-code","title":"Compile PHG / BrAPI Server Code","text":"<ul> <li>cd phg_webktor_service</li> <li>gradle clean shadowJar</li> <li>cp build/libs/phg_webktor_service-1.0-SNAPSHOT-all.jar docker</li> </ul>"},{"location":"PHG%20/%20BrAPI%20Server%20on%20CBSU/#stopping-phg-brapi-docker-container","title":"Stopping PHG / BrAPI Docker Container","text":"<ul> <li>docker1 ps</li> <li>docker1 kill"},{"location":"PHG%20/%20BrAPI%20Server%20on%20CBSU/#removing-phg-brapi-docker-image","title":"Removing PHG / BrAPI Docker Image","text":"<ul> <li>docker1 rmi biohpc_tmc46/phg_brapi</li> </ul>"},{"location":"PHG%20/%20BrAPI%20Server%20on%20CBSU/#building-phg-brapi-docker-image","title":"Building PHG / BrAPI Docker Image","text":"<ul> <li>cd phg_webktor_service/docker</li> <li>docker1 build -t phg_brapi /workdir/tmc46/phg_brapi/phg_webktor_service/docker/</li> </ul>"},{"location":"PHG%20/%20BrAPI%20Server%20on%20CBSU/#starting-phg-brapi-docker-container","title":"Starting PHG / BrAPI Docker Container","text":"<ul> <li>// Port 80 is external.  Port 8080 is internal</li> <li>docker1 run --rm -p 80:8080 biohpc_tmc46/phg_brapi</li> </ul>"},{"location":"PHG%20/%20BrAPI%20Server%20on%20CBSU/#testing-phg-brapi-server","title":"Testing PHG / BrAPI Server","text":"<ul> <li>curl http://cbsudc01.biohpc.cornell.edu:80/brapi/v2/serverinfo</li> <li>curl http://cbsudc01.biohpc.cornell.edu:80/brapi/v2/samples</li> <li>curl http://cbsudc01.biohpc.cornell.edu:80/brapi/v2/samples/11726</li> </ul>"},{"location":"Pipeline_version1/AssemblyHaplotypesPlugin/","title":"AssemblyHaplotypesPlugin","text":"<p>This plugin takes a reference fasta file containing sequence for a single chromosome and an assembly fasta file with sequence for a single corresponding chromosome.  The fastas are aligned using mummer4 nucmer script with a cluster size of 250 and the --mum parameter (anchor matches are unique in both reference and query).  </p> <p>The nucmer results are filtered using mummer4 delta-filter script with the -g option.  The -g option provides a 1-1 global alignment not allowing rearrangments.  Mummer script show-coords is run on both the original delta file created by nucmer, and the filtered delta created via delta-filter.  Post-processing is done to add back inversions.</p> <p>Mummer script show-snps is run using the post-processed coords file.  The list of final-snps is stored in htsjdk VariantContext records.  The haplotype sequence is pulled from the VariantContext records.  For alignments that map to reference anchor regions, haplotype sequence and variant lists are created and loaded to the PHG database.</p> <p>Both assembly anchor and inter-anchor regions are processed in this method.</p> <p>Tables populated via this method are:</p> <ul> <li>genotypes</li> <li>gametes</li> <li>gamete_groups</li> <li>methods</li> <li>haplotypes</li> <li>gamete_haplotypes</li> </ul> <p>The parameters to this plugin are:</p> <ul> <li>-ref   Reference Genome File for a single chromosome.  (Default is null) (REQUIRED) <li>-assembly  Path to assembly fasta for a single chromosome from which to pull sequence for aligning. <li>-outputDir   Path to output directory including trailing / that will be used when creating temporary output files. <li>-dbConfigFile  Path to config file containing DB parameters host, user, password, DB, type.  Used for making the database connection.  Type must be wither \"sqlite\" or \"postgres\" to identify db type for connection. <li>-version  Version name for the set of DB anchor/inter-anchor coorcinates as stored in the anchor_versions table (Default is null) (REQUIRED) <li>-assemblyName  Name of assembly taxon, to be stored as taxon name in the DB. (Default is null) (REQUIRED) <li>-chrom  Name of chromosome as it appears both for the reference in the db genome_intervals table and in the fasta file idline for the assembly.  The chromosome name can be just a number, a number preceded by \"chr\" or a number preceded by \"chromsome\".  Reference and assembly need to be consistent. <p>An example config file looks as below:</p> <pre><code>#!java\n\nhost=localHost\nuser=sqlite\npassword=sqlite\nDB=/tempFileDir/outputDir/phgSmallSeq.db\nDBtype=sqlite\n</code></pre>"},{"location":"Pipeline_version1/ConfigFile/","title":"ConfigFile Documentation","text":"<p>Most steps of the Practical Haplotype Graph require a Configuration File.  For the most part, users can create one config file and add parameters when needed, reusing it along the way.  </p>"},{"location":"Pipeline_version1/ConfigFile/#sample-configuration-file","title":"Sample Configuration File","text":"<p>Here is a sample configuration file.  This file contains the required parameters and some additional optional ones.  Please refer to each individual pipeline's documentation page in this wiki to get a full list of available parameters and acceptable values for each.</p> <pre><code>#!bash\n\n#database config parameters\nhost=localHost\nuser=sqlite\npassword=sqlite\nDB=/tempFileDir/outputDir/DBFile.db\nDBtype=sqlite\n\n#Java arguments\nXmx=500G\n\n#CreateHaplotype Params\nreferenceFasta=/path/to/reference/ref.fasta\nLoadHaplotypesFromGVCFPlugin.keyFile=/path/to/keyfile/keyFile.txt\nLoadHaplotypesFromGVCFPlugin.gvcfDir=/path/to/gvcfs/\nLoadHaplotypesFromGVCFPlugin.referenceFasta=/path/to/reference/ref.fasta\nLoadHaplotypesFromGVCFPlugin.haplotypeMethodName=GATK_PIPELINE\nLoadHaplotypesFromGVCFPlugin.haplotypeMethodDescription=GVCF_DESCRIPTION\n\n#Haplotype filtering\nmapQ=48\nDP_poisson_min=.01\nDP_poisson_max=.99\nGQ_min=50\nfilterHets=t\n\n#sentieon license\nsentieon_license=cbsulogin2.tc.cornell.edu:8990\n\n#Consensus parameters\n#Optional argument to get out merged VCF files for debugging consensus\nincludeVariants=true\nmxDiv=.001\nmaxError=0.2\n\n#FindPath Config parameters\nBestHaplotypePathPlugin.maxNodes=30\nBestHaplotypePathPlugin.minTaxa=5\nBestHaplotypePathPlugin.minReads=1\nBestHaplotypePathPlugin.maxReads=100\nBestHaplotypePathPlugin.minTransitionProb=0.001\nBestHaplotypePathPlugin.probCorrect=0.99\nBestHaplotypePathPlugin.splitNodes=true\nBestHaplotypePathPlugin.splitProb=0.99\n</code></pre>"},{"location":"Pipeline_version1/CreateAssemblyAnchorsPlugin/","title":"CreateAssemblyAnchorsPlugin","text":"<p>This plugin takes an assembly genome fasta file, splits the sequence into contigs based on the presence of N's.  Each contig begins at a defined allele (base of A,C,G or T) and ends when an N is encountered.  N's are skipped, a new contig begins at the next non-N allele.  The contig fasta is aligned against the reference genome using minimap2.  Samtools mpileup command is used to generate a pileup of read bases aligning to the reference, limiting the pileup to the conserved region intervals.  Bcftools are then used to create a gvcf file.  Fastas created from the gvcf are loaded to the db via the LoadHapSequencesToDBPlugin.</p> <p>Assembly inter-anchor regions are not identified at this time.</p> <p>Tables populated via this method are:</p> <ul> <li>genotypes</li> <li>gametes</li> <li>gamete_groups</li> <li>methods</li> <li>haplotypes</li> <li>gamete_haplotypes</li> </ul> <p>The parameters to this plugin are:</p> <ul> <li>-ref   Reference Genome File for aligning against. (Default is null) (REQUIRED) <li>-genomeFile  Path to assembly genome fasta from which to pull sequence for aligning. <li>-genomeData  A tab-delimited file containing genome specific data with columns: Genotype Hapnumber Dataline Ploidy Reference GenePhased ChromPhased Confidence Method MethodDetails RefVersion.   <li>-outputDir   Path to output directory including trailing / that will be used when creating temporary output files. <li>-intervals  Path to BED formatted file containing the intervals used when creating the reference genome intervals. The positions are 0-based, inclusive/exclusive. This is used in mpileup. (Default is null) (REQUIRED) <li>-configFile  Path to config file containing DB parameters host, user, password, DB, type.  Used for making the database connection.  Type must be wither \"sqlite\" or \"postgres\" to identify db type for connection. <p>An example config file looks as below:</p> <pre><code>#!java\n\nhost=localHost\nuser=sqlite\npassword=sqlite\nDB=/tempFileDir/outputDir/phgSmallSeq.db\nDBtype=sqlite\n</code></pre> <p>The genome data file contents are described below:</p> <ul> <li>Genotype:  the nmae of the line as you want it to appear in the db genotypes table \"name\" column, e.g. \"B104\" or \"B104_haplotype_caller\"</li> <li>Hapnumber:  The 0-based chromosome number.  For inbreds, this is always 0.</li> <li>Dataline: Text description defining the line. This data will be stored in the \"description\" field of the genotypes table.</li> <li>Ploidy:  Number of chromosomes for this genome, the value to be stored in the \"ploidy\" field of the genome_intervals table.  Should be 1 for the reference.</li> <li>Reference:  a boolean field indicating if this genome is the reference (should be true for this plugin)</li> <li>GenePhased: a boolean field indicating if the genes are phased.  Should be false for the reference.</li> <li>ChromPhased:  a boolean field indicating if the chromosomes are phased.  Should be false for the reference.</li> <li>Confidence:  a float field indicating confidence in phasing.  Generally 1 when no phasing.</li> <li>Method:  a mathod name by which this version of the reference data can be identified.</li> <li>MethodDetails:  Text description defining the method used to create the reference (or indicating the AGP version)</li> <li>RefVersion: a version name used to identify the version of this reference data.</li> </ul>"},{"location":"Pipeline_version1/CreateConsensi/","title":"CreateConsensi","text":"<p>The CreateConsensi.sh script is used to create Consensus Haplotypes for each genome interval using a specific set of raw haplotypes.  Basically this script will create a haplotype graph and attempt to create consensus haplotypes for each anchor.  Once a set of consensus haplotypes are created, the script will upload the all consensus haplotypes to the Database.  Once this process is done, a graph can be made using the consensus haplotypes.</p> <p>This processing is done via the RunHapCollapsePlugin.</p>"},{"location":"Pipeline_version1/CreateConsensi/#mount-points-for-use-with-the-phg-docker","title":"Mount Points For use with the PHG Docker","text":"<ul> <li>Mount localMachine:/pathToInputs/config.txt:/tempFileDir/data/config.txt - (required)</li> <li>Mount localMachine:/pathToInputs/refFolder/ to docker:/tempFileDir/data/reference/ - (required)</li> <li>Mount localMachine:/pathsToDB/dbFolder/ to docker /tempFileDir/outputDir/ - (required)</li> </ul>"},{"location":"Pipeline_version1/CreateConsensi/#there-is-one-optional-mount-point-for-getting-out-the-vcf-files-created-from-combining-the-gvcfs","title":"There is one optional mount point for getting out the VCF files created from combining the GVCFs","text":"<ul> <li>This is used in conjunction with the exportMergedVCF config file parameter.  If this parameter is set in the config file, you can mount a local directory to that docker directory to get out the intermediate VCFs.  If this is not set, the code should run faster as there is less IO. </li> </ul>"},{"location":"Pipeline_version1/CreateConsensi/#createconsensish-parameters","title":"CreateConsensi.sh Parameters","text":"<ul> <li>configFile: Path to config file containing DB parameters host, user, password, DB, type. Used for making the database connection. Type must be wither \"sqlite\" or \"postgres\" to identify db type for connection. This file will also contain parameters related to creating the consensus sequences and is described in Relevant Config File Parameters.  In the below example this is /tempFileDir/data/config.txt.  A sample configuration file can be found here: Config File Wiki</li> <li>Reference file: Reference fasta file.  In the below example this is reference.fa and is contained in the mount point /tempFileDir/data/reference/</li> <li>Haplotype Method Name: Name of the raw haplotypes which Consensus should be made.</li> <li>Consensus Method Name: Name to be stored in the database denoting this run of CreateConsensi.</li> </ul>"},{"location":"Pipeline_version1/CreateConsensi/#pipeline-steps","title":"Pipeline Steps","text":"<ol> <li>Create a Haplotype Graph</li> <li>For Each Anchor do the following:<ol> <li>If the graph does not have the variants, extract the variants from the DB for the raw Haplotypes.</li> <li>Take all the GVCF variants and combine them into a single TASSEL GenotypeTable object.</li> <li>Remove any Positions covering an indel.</li> <li>Create Consensus clusters and create GenotypeTables for each cluster.</li> <li>Reintroduce indels which were filtered out if they agree.  If they do not agree, it will follow the indelMergeRule defined in the Configuration File.</li> </ol> </li> <li>Collect up to 1000 Consensus haplotypes and upload them to the DB.</li> </ol>"},{"location":"Pipeline_version1/CreateConsensi/#relevant-config-file-parameters-must-be-in-the-form-paramvalue","title":"Relevant Config File Parameters.  Must be in the form param=value","text":"<p>Sample file is here: Config File Wiki</p>"},{"location":"Pipeline_version1/CreateConsensi/#db-parameters","title":"DB Parameters","text":"<ul> <li>host - host name of the db</li> <li>user - db user name</li> <li>password - password for db</li> <li>DB - name of the db</li> <li>DBtype - sqlite or postgres</li> </ul>"},{"location":"Pipeline_version1/CreateConsensi/#java-arguments","title":"Java Arguments","text":"<ul> <li>Xmx - max heap space for the pipeline.  This is similar to Java's -Xmx argument.</li> </ul>"},{"location":"Pipeline_version1/CreateConsensi/#graph-building-parameters","title":"Graph Building Parameters","text":"<ul> <li>includeVariants - true. This needs to be true to work correctly.  Given the new Variant Storage, the graphs variants can fit into memory much better.</li> </ul>"},{"location":"Pipeline_version1/CreateConsensi/#consensus-parameters","title":"Consensus Parameters","text":"<ul> <li>mxDiv - maximum amount of divergence allowed when clustering.  The default is 0.01. </li> <li>seqErr - Error rate for calling a het vs a homozygote in the consensus haplotype.  The default is 0.01.</li> <li>minSites - minimum number of non-missing sites to be clustered.  Any haplotypes with fewer number of non-missing calls will be ignored.  The default is 20.</li> <li>minTaxa -  minimum number of taxa for a cluster to be created. The default is 2.</li> <li>rankingFile - path to a ranking file.  If your haplotypes were created from Assemblies, you need to produce a method ranking the taxon. This file will take the form: taxon\\tscore where higher scores mean we trust that taxon more highly.  Do no include a header line.  When clustering assemblies, when we have a cluster of similar haplotypes, we choose whichever taxon in that group which has the higher ranking score.  To break ties, be sure to give each taxon a different score.  One simple way to score things is to count the number of haplotypes covered by each taxon in the DB and use that count as the score.  Any other arbitrary ranking can be used.</li> <li>clusteringMode - either upgma(default) or upgma_assembly.  If running WGS haplotypes, either use the default or specify upgma.  If the haplotypes are assemblies, use upgma_assembly.  The differences between the two are that upgma just builds a tree based on a pairwise calculated distance matrix and then will try to merge haplotypes together.  upgma_assembly will also do a pairwise distance matrix and then will select the better haplotype in the group based on the ranking file specified by the \"rankingFile\" parameter.</li> </ul>"},{"location":"Pipeline_version1/CreateConsensi/#example-run-scripts","title":"Example Run Scripts","text":"<p>An example Docker script to run the CreateConsensi.sh shell script is:</p> <pre><code>#!bash\n\ndocker run --name phg_container_consensus --rm \\\n        -v /workdir/user/DockerTuningTests/Reference/:/tempFileDir/data/reference/ \\\n        -v /workdir/user/DockerTuningTests/DockerOutput/:/tempFileDir/outputDir/ \\\n        -v /workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/config.txt:/tempFileDir/data/config.txt \\\n        -t maizegenetics/phg:latest \\\n        /CreateConsensi.sh /tempFileDir/data/config.txt reference.fa GATK_PIPELINE CONSENSUS\n</code></pre> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The \"-t\" directive indicates the Docker image of which this container will be an instance.  The last line tells the Docker container to run the CreateConsensi.sh script which is found in the root directory.  The items following are the parameters to the CreateConsensi.sh script.</p>"},{"location":"Pipeline_version1/CreateHaplotypeKeyFile/","title":"CreateHaplotype Key File","text":""},{"location":"Pipeline_version1/CreateHaplotypeKeyFile/#specification","title":"Specification:","text":"<p>The Keyfile is a tab-separated text file which is used to set up the alignment, HaplotypeCaller and GVCF Upload steps for the PHG CreateHaplotypes Scripts.</p> <p>The PHG will process the following columns:</p> HeaderName Description Required sample_name Name of the taxon to be processed. Yes sample_description Short Description of the sample_name. No, if not specified, an empty description will be used files Comma-separated list of file names to be processed. Yes type Type of the files to be processed.  PHG Currently Supports FASTQ, BAM or GVCF. Yes chrPhased Are the Chromosomes Phased?  This  needs to be 'true' or 'false' Yes for GVCF type genePhased Are the Genes Phased? This  needs to be 'true' or 'false' Yes for GVCF type phasingConf What is the confidence of the phasing?  This needs to be between 0.0 and 1.0. If working with inbreds, this can be set close to 1.0. Yes for GVCF type library_ID What is the library ID of the fastq files.  This is only used if running BWA during CreateHaplotypesFromFastq.groovy Yes only for FASTQ type <p>Because the entries in the 'files' column are comma separated, the PHG can do the following depending on the type:</p> <ul> <li>FASTQ : pairwise or single ended alignment using bwa mem.</li> <li>BAM : Run GATK/Sentieon HaplotypeCaller on all the BAM files specified in the list to create a single GVCF.</li> <li>GVCF : upload haplotypes for taxon with ploidy &gt; 1.  Each file in the list will create a new haplotype.  If using Heterozygous material, we expect you to phase the GVCF file prior to running CreateHaplotypesFromGVCF.groovy.</li> </ul>"},{"location":"Pipeline_version1/CreateHaplotypeKeyFile/#sample-file","title":"Sample File:","text":"<pre><code>#!txt\n\nsample_name sample_description  files   type    chrPhased   genePhased  phasingConf libraryID\nRef Ref line aligned    Ref_R1.fastq    FASTQ   true    true    .99 dummyLib1\nLineA   LineA line aligned  LineA_R1.fastq  FASTQ   true    true    .99 dummyLib1\nLineB   LineB line aligned  LineB_R1.fastq  FASTQ   true    true    .99 dummyLib1\nRefA1   RefA1 line aligned  RefA1_R1.fastq  FASTQ   true    true    .99 dummyLib1\nLineA1  LineA1 line aligned LineA1_R1.fastq FASTQ   true    true    .99 dummyLib1\nLineB1  LineB1 line aligned LineB1_R1.fastq FASTQ   true    true    .99 dummyLib1\nRefA1   RefA1 Aligned using BWA RefA1_dummyLib1_srt_dedup.bam   BAM true    true    .99 null\nRef Ref Aligned using BWA   Ref_dummyLib1_srt_dedup.bam BAM true    true    .99 null\nLineB1  LineB1 Aligned using BWA    LineB1_dummyLib1_srt_dedup.bam  BAM true    true    .99 null\nLineA   LineA Aligned using BWA LineA_dummyLib1_srt_dedup.bam   BAM true    true    .99 null\nLineB   LineB Aligned using BWA LineB_dummyLib1_srt_dedup.bam   BAM true    true    .99 null\nLineA1  LineA1 Aligned using BWA    LineA1_dummyLib1_srt_dedup.bam  BAM true    true    .99 null\nRefA1   RefA1 Aligned using BWA RefA1_haplotype_caller_output_filtered.g.vcf.gz GVCF    true    true    .99 null\nRef Ref Aligned using BWA   Ref_haplotype_caller_output_filtered.g.vcf.gz   GVCF    true    true    .99 null\nLineB1  LineB1 Aligned using BWA    LineB1_haplotype_caller_output_filtered.g.vcf.gz    GVCF    true    true    .99 null\nLineA   LineA Aligned using BWA LineA_haplotype_caller_output_filtered.g.vcf.gz GVCF    true    true    .99 null\nLineB   LineB Aligned using BWA LineB_haplotype_caller_output_filtered.g.vcf.gz GVCF    true    true    .99 null\nLineA1  LineA1 Aligned using BWA    LineA1_haplotype_caller_output_filtered.g.vcf.gz    GVCF    true    true    .99 null\n\n</code></pre>"},{"location":"Pipeline_version1/CreateHaplotypes/","title":"Create Haplotypes","text":""},{"location":"Pipeline_version1/CreateHaplotypes/#overview","title":"Overview","text":"<p>This step allows you to load Haplotypes into the Database.  Currently we support inputs from WGS fastq files, BAM files of WGS reads aligned to a reference and a GVCF file. Depending on the inputs, you will need to run a different Script.</p>"},{"location":"Pipeline_version1/CreateHaplotypes/#note-as-of-phg-version-0020-the-keyfile-plugin-parameter-has-been-renamed-wgskeyfile","title":"Note, as of PHG version 0.0.20, the keyFile plugin parameter has been renamed wgsKeyFile.","text":""},{"location":"Pipeline_version1/CreateHaplotypes/#scripts","title":"Scripts","text":"<ul> <li>CreateHaplotypesFromFastq.groovy</li> <li>CreateHaplotypesFromBAM.groovy</li> <li>CreateHaplotypesFromGVCF.groovy</li> </ul> <p>Please refer to each scripts documentation for more detailed instructions.</p>"},{"location":"Pipeline_version1/CreateHaplotypes/#steps-of-the-pipeline","title":"Steps of the Pipeline","text":"<ol> <li>Check to see if the reference is BWA and fai indexed<ol> <li>If not index using bwa, samtools and picard</li> </ol> </li> <li>Align WGS fastqs to the reference using bwa mem</li> <li>Sort and MarkDuplicates in the output BAM file</li> <li>Filter the BAM file based on Minimum MapQ</li> <li>Run HaplotypeCaller using GATK or Sentieon</li> <li>Filter the GVCF file</li> <li>Extract out the Reference Ranges and load the haplotypes to the DB</li> </ol> <p>Steps 1-7 are completed by CreateHaplotypesFromFastq.groovy, Steps 4-7 are completed by CreateHaplotypesFromBAM.groovy and only step 7 is completed by CreateHaplotypesFromGVCF.groovy.</p> <p>If you have data from multiple different formats, we suggest running CreateHaplotypesFromFastq.groovy first on all your fastq files, then CreateHaplotypesFromBAM.groovy on your BAMs and finally CreateHaplotypesFromGVCF.groovy on your GVCF files.</p> <p>Deprecated Script:</p> <ul> <li>CreateHaplotypes.sh - This script is no longer maintained.</li> </ul>"},{"location":"Pipeline_version1/CreateHaplotypesFromBAM/","title":"Overview","text":"<p>This Groovy Script will create haplotypes originating from BAM Files specified within a keyFile.  This groovy script can be run by itself, but it is also called from CreateHaplotypesFromFastq.groovy (source).  This expects BAM files where WGS short reads have been aligned to a reference using BWA MEM.</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromBAM/#note-as-of-phg-version-0020-the-keyfile-plugin-parameter-has-been-renamed-wgskeyfile","title":"Note, as of PHG version 0.0.20, the keyFile plugin parameter has been renamed wgsKeyFile.","text":""},{"location":"Pipeline_version1/CreateHaplotypesFromBAM/#more-information-about-the-keyfile-format-is-here","title":"More information about the keyfile format is here:","text":"<p>https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/DockerPipeline/CreateHaplotypeKeyFile.</p> <p>Note only records with type BAM and GVCF will be processed by this script.  BAM records will be run though HaplotypeCaller and filtered.  Then the GVCFs coming out of HaplotypeCaller will be added to the DB along with the GVCF records in the keyfile.</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromBAM/#pipeline-steps-run-by-createhaplotypesfrombam","title":"Pipeline Steps run by CreateHaplotypesFromBAM","text":"<ol> <li>Filter the BAM file based on Minimum MapQ</li> <li>Run HaplotypeCaller using GATK or Sentieon</li> <li>Filter the GVCF file</li> <li>Extract out the Reference Ranges and load the haplotypes to the DB</li> </ol>"},{"location":"Pipeline_version1/CreateHaplotypesFromBAM/#example-run-command","title":"Example Run Command","text":"<pre><code>#!bash\n\nCreateHaplotypesFromBAM.groovy -[hd] -config [dbConfigFile]\n</code></pre>"},{"location":"Pipeline_version1/CreateHaplotypesFromBAM/#command-line-flags-note-some-parameters-have-both-a-short-and-long-name","title":"Command Line Flags (note some parameters have both a short and long name)","text":"<pre><code>#!bash\n\n[-h, -help] 'Show usage information'\n[-d, -description] 'Show information about this Pipeline'\n[-config] 'DB Config File(required)'\n</code></pre> <p>This Groovy Script will open up the keyFile(location is specified as the LoadHaplotypesFromGVCFPlugin.keyFile entry in the config file) and will loop over all BAM records and run GATK/Sention's HaplotypeCaller.  If multiple BAM files are stored for a given taxon, they will all be input for that taxon's HaplotypeCaller run.  The resulting GVCFs will then be filtered and uploaded along with GVCF keyfile records to the DB.  If multiple GVCFs are stored for a given taxon, each GVCF will be treated as an independent haplotype.</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromBAM/#sample-script","title":"Sample Script","text":"<pre><code>#!bash\nREF_DIR=/workdir/user/DockerTuningTests/InputFiles/Reference/\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nCONFIG_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/config.txt\nCONFIG_FILE_IN_DOCKER=/tempFileDir/data/config.txt\nDEDUP_BAM_DIR=/workdir/user/DockerTuningTests/InputFiles/WGSBams/\nFILTERED_BAM_DIR=/workdir/user/DockerTuningTests/InputFiles/WGSBams_Filtered/\nGVCF_OUTPUT_DIR=/workdir/user/DockerTuningTests/DockerOutput/gvcfOut/\nKEY_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/keyfile.txt\nKEY_FILE_IN_DOCKER=/tempFileDir/data/keyFile.txt\n\ndocker run --name small_seq_test_container --rm \\\n        -w / \\\n        -v ${REF_DIR}:/tempFileDir/data/reference/ \\\n        -v ${DB}:/tempFileDir/outputDir/phgSmallSeq.db \\\n        -v ${CONFIG_FILE}:${CONFIG_FILE_IN_DOCKER} \\\n        -v ${GVCF_OUTPUT_DIR}:/tempFileDir/data/gvcfs/ \\\n        -v ${DEDUP_BAM_DIR}:/tempFileDir/data/bam/DedupBAMs/ \\\n        -v ${FILTERED_BAM_DIR}:/tempFileDir/data/bam/filteredBAMs/ \\\n        -v ${KEY_FILE}:${KEY_FILE_IN_DOCKER} \\\n        -t maizegenetics/phg /CreateHaplotypesFromBAM.groovy -config ${CONFIG_FILE_IN_DOCKER}\n\n</code></pre> <p>Note that /tempFileDir/data/gvcfs/ and /tempFileDir/data/bam/filteredBAMs/ are locations for intermediate files to be stored.  These are not required, but if you wish to keep the GVCFs and/or the filtered BAM files you should mount these locations otherwise they will not persist.  If you would like to change the docker locations for these files, simply provide a gvcfFileDir and filteredBamDir config parameter in the config file and the script will write the files to that location.</p> <p>Also note the -w / parameter. This is needed to guarantee that the script will run correctly. When running a normal docker, this is likely not needed, but if running on a system like cbsu, the working directory needs to be set to the root directory.</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromBAM/#config-file-parameters","title":"Config File Parameters","text":"<p>This CreateHaplotypes Script will need some configuration parameters set in order to work correctly. You must set referenceFasta, LoadHaplotypesFromGVCFPlugin.keyFile, LoadHaplotypesFromGVCFPlugin.gvcfDir, LoadHaplotypesFromGVCFPlugin.referenceFasta, LoadHaplotypesFromGVCFPlugin.haplotypeMethodName.</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromBAM/#file-directories","title":"File Directories.","text":"<ul> <li>gvcfFileDir=/tempFileDir/data/gvcfs/<ul> <li>The Optional Output GVCF file Directory.  If you need to keep these files, mount a local drive to this location</li> </ul> </li> <li>tempFileDir=/tempFileDir/data/bam/temp/<ul> <li>The Temp file directory.  This Location is used to make any intermediate files when processing.</li> </ul> </li> <li>filteredBamDir=/tempFileDir/data/bam/filteredBAMs/<ul> <li>The Optional Output filtered file Directory. After Filtering the BAM files by MapQ, the files are written here.  Mount a local drive to this location if you need the bams.</li> </ul> </li> <li>dedupedBamDir=/tempFileDir/data/bam/DedupBAMs/<ul> <li>After filtering and Deduplication, the BAM files are stored here. This will be where the script is expecting the input BAM files.</li> </ul> </li> </ul>"},{"location":"Pipeline_version1/CreateHaplotypesFromBAM/#tassel-parameters","title":"TASSEL parameters","text":"<ul> <li>Xmx=10G<ul> <li>Max Java Heap Space used when running TASSEL code.</li> </ul> </li> <li>tasselLocation=/tassel-5-standalone/run_pipeline.pl<ul> <li>Location of TASSEL on machine.  If using PHG docker this is the correct location</li> </ul> </li> </ul>"},{"location":"Pipeline_version1/CreateHaplotypesFromBAM/#phg-createhaplotypes-parameters","title":"PHG CreateHaplotypes Parameters","text":"<ul> <li>referenceFasta<ul> <li>Reference fasta file location.  This is Required. Note, if using Docker, this needs to be the Docker specific path to the reference file.  You will need to mount these files to the Docker in order for it to work.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.wgsKeyFile<ul> <li>Location of the keyfile. This is Required. Note, if using Docker, this needs to be the Docker specific path to the keyfile.  You will need to mount this file to the Docker in order for it to work.  Note before PHG version 0.0.20, this parameter was named keyFile.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.gvcfDir<ul> <li>Directory of the GVCF files you wish to upload. This is Required. Note, if using Docker, this needs to be the Docker specific path to the gvcfs.  You will need to mount these files to the Docker in order for it to work. Note: This needs to match gvcfFileDir in the config file.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.referenceFasta<ul> <li>This should be set to the same location as referenceFasta. This is Required.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.haplotypeMethodName=GATK_PIPELINE<ul> <li>This is the method name which you are going to write to the DB.  This is Required. If you attempt to upload the same gvcfs and do not change this an error will be thrown.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.haplotypeMethodDescription<ul> <li>This is the description for the haplotypeMethodName.  If this is not set and empty description will be written to the DB.  It is strongly suggested that you put something in this field.</li> </ul> </li> <li>extendedWindowSize=1000<ul> <li>This Script will extract out the Focused reference ranges from the Database into a BED file.  When doing the BAM file filtering, we allow the user to add flanking regions around each of the regions.</li> </ul> </li> <li>mapQ=48<ul> <li>Minimum MapQ value allowed in the BAM file filtering.  This is to reduce the number of reads which map to multiple locations across the genome.</li> </ul> </li> </ul>"},{"location":"Pipeline_version1/CreateHaplotypesFromBAM/#gatk-and-sentieon-parameters","title":"GATK and Sentieon Parameters","text":"<ul> <li>gatkPath=/gatk/gatk<ul> <li>Location of GATK on this system.  This is the location within the PHG docker</li> </ul> </li> <li>numThreads=10<ul> <li>Number of threads requested to run HaplotypeCaller.  </li> </ul> </li> <li>sentieon_license<ul> <li>If you have access to Sentieon and wish to use it, provide the sentieon license here.  The Script will automatically attempt to use Sentieon if this parameter is specified.</li> </ul> </li> <li>sentieonPath=/sentieon/bin/sentieon<ul> <li>This is the Docker specific path to sentieon.  If sentieon is installed somewhere else on your system, change this parameter.</li> </ul> </li> </ul>"},{"location":"Pipeline_version1/CreateHaplotypesFromFastq/","title":"Overview","text":"<p>This Groovy Script will create haplotypes originating from a FastqFile specified in a keyfile. This groovy script can be run by itself outside of the docker, but you will need to be sure to setup the output directories correctly.  This script's steps can be viewed here: CreateHaplotypesFromFastq.groovy  If you also have some BAM files aligned for the same taxon you can include these as well.</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromFastq/#note-as-of-phg-version-0020-the-keyfile-plugin-parameter-has-been-renamed-wgskeyfile","title":"Note, as of PHG version 0.0.20, the keyFile plugin parameter has been renamed wgsKeyFile.","text":""},{"location":"Pipeline_version1/CreateHaplotypesFromFastq/#more-information-about-the-keyfile-format-is-here","title":"More information about the keyfile format is here:","text":"<p>https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/DockerPipeline/CreateHaplotypeKeyFile.</p> <p>Note only records with type FASTQ, BAM and GVCF will be processed by this script.  FASTQ records will be aligned to the the reference using BWA MEM.  Then all output BAM files along with the keyfile BAM records will be run though HaplotypeCaller and filtered.  Then the GVCFs coming out of HaplotypeCaller will be added to the DB along with the GVCF records in the keyfile.</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromFastq/#pipeline-steps-run-by-createhaplotypesfromfastq","title":"Pipeline Steps run by CreateHaplotypesFromFastq","text":"<ol> <li>Check to see if the reference is BWA and fai indexed<ol> <li>If not index using bwa, samtools and picard</li> </ol> </li> <li>Align WGS fastqs to the reference using bwa mem</li> <li>Sort and MarkDuplicates in the output BAM file</li> <li>Filter the BAM file based on Minimum MapQ</li> <li>Run HaplotypeCaller using GATK or Sentieon</li> <li>Filter the GVCF file</li> <li>Extract out the Reference Ranges and load the haplotypes to the DB</li> </ol>"},{"location":"Pipeline_version1/CreateHaplotypesFromFastq/#example-run-command","title":"Example Run Command","text":"<pre><code>#!bash\n\nCreateHaplotypesFromFastq.groovy -[hd] -config [dbConfigFile]\n</code></pre>"},{"location":"Pipeline_version1/CreateHaplotypesFromFastq/#command-line-flags-note-some-parameters-have-both-a-short-and-long-name","title":"Command Line Flags (note some parameters have both a short and long name)","text":"<pre><code>#!bash\n\n[-h, -help] 'Show usage information'\n[-d, -description] 'Show information about this Pipeline'\n[-config] 'DB Config File(required)'\n</code></pre> <p>This Groovy Script will open up the keyFile(location is specified as the LoadHaplotypesFromGVCFPlugin.keyFile entry in the config file) and will loop over all FASTQ records and run BWA MEM to create BAM files.  Then all BAM records in the keyfile and those output by BWA MEM are run through GATK/Sention's HaplotypeCaller.  If multiple BAM files are stored for a given taxon, they will all be input for that taxon's HaplotypeCaller run.  The resulting GVCFs will then be filtered and uploaded along with GVCF keyfile records to the DB.  If multiple GVCFs are stored for a given taxon, each GVCF will be treated as an independent haplotype.</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromFastq/#sample-script","title":"Sample Script:","text":"<pre><code>#!bash\nREF_DIR=/workdir/user/DockerTuningTests/InputFiles/Reference/\nFASTQ_DIR=/workdir/user/DockerTuningTests/InputFiles/WGSFastq/\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nCONFIG_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/config.txt\nCONFIG_FILE_IN_DOCKER=/tempFileDir/data/config.txt\nGVCF_OUTPUT_DIR=/workdir/user/DockerTuningTests/DockerOutput/gvcfOut/\nKEY_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/keyfile.txt\nKEY_FILE_IN_DOCKER=/tempFileDir/data/keyFile.txt\n\n\ndocker run --name small_seq_test_container --rm \\\n        -w / \\\n        -v ${REF_DIR}:/tempFileDir/data/reference/ \\\n        -v ${FASTQ_DIR}:/tempFileDir/data/fastq/ \\\n        -v ${DB}:/tempFileDir/outputDir/phgSmallSeq.db \\\n        -v ${CONFIG_FILE}:${CONFIG_FILE_IN_DOCKER} \\\n        -v ${GVCF_OUTPUT_DIR}:/tempFileDir/data/gvcfs/ \\\n        -v ${KEY_FILE}:${KEY_FILE_IN_DOCKER} \\\n        -t maizegenetics/phg /CreateHaplotypesFromFastq.groovy -config ${CONFIG_FILE_IN_DOCKER}\n\n\n</code></pre> <p>Note the -w / parameter.  This is needed to guarantee that the script will run correctly.  When running a normal docker, this is likely not needed, but if running on a system like cbsu, the working directory needs to be set to the root directory.</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromFastq/#config-file-parameters","title":"Config File Parameters","text":"<p>The CreateHaplotypes Script will need some configuration parameters set in order to work correctly.</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromFastq/#file-directories","title":"File Directories.","text":"<ul> <li>gvcfFileDir=/tempFileDir/data/gvcfs/<ul> <li>The Output GVCF file Directory.  If you need to keep these files, mount a local drive to this location</li> </ul> </li> <li>tempFileDir=/tempFileDir/data/bam/temp/<ul> <li>The Temp file directory.  This Location is used to make any intermediate files when processing.</li> </ul> </li> <li>filteredBamDir=/tempFileDir/data/bam/filteredBAMs/<ul> <li>After Filtering the BAM files by MapQ, the files are written here.  Mount a local drive to this location if you need the bams.</li> </ul> </li> <li>dedupedBamDir=/tempFileDir/data/bam/DedupBAMs/<ul> <li>After filtering and Deduplication, the BAM files are stored here.  It is strongly suggested to mount to this location as BWA alignment does take some time.</li> </ul> </li> </ul>"},{"location":"Pipeline_version1/CreateHaplotypesFromFastq/#tassel-parameters","title":"TASSEL parameters","text":"<ul> <li>Xmx=10G<ul> <li>Max Java Heap Space used when running TASSEL code.</li> </ul> </li> <li>tasselLocation=/tassel-5-standalone/run_pipeline.pl<ul> <li>Location of TASSEL on machine.  If using PHG docker this is the correct location</li> </ul> </li> </ul>"},{"location":"Pipeline_version1/CreateHaplotypesFromFastq/#phg-createhaplotypes-parameters","title":"PHG CreateHaplotypes Parameters","text":"<ul> <li>referenceFasta<ul> <li>Reference fasta file location.  This is Required. Note, if using Docker, this needs to be the Docker specific path to the reference file.  You will need to mount these files to the Docker in order for it to work.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.wgsKeyFile<ul> <li>Location of the keyfile. This is Required. Note, if using Docker, this needs to be the Docker specific path to the keyfile.  You will need to mount this file to the Docker in order for it to work.  Note before version 0.0.20, this was named keyFile.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.gvcfDir<ul> <li>Directory of the GVCF files you wish to upload. This is Required. Note, if using Docker, this needs to be the Docker specific path to the gvcfs.  You will need to mount these files to the Docker in order for it to work. Note: This needs to match gvcfFileDir in the config file.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.referenceFasta<ul> <li>This should be set to the same location as referenceFasta. This is Required.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.haplotypeMethodName=GATK_PIPELINE<ul> <li>This is the method name which you are going to write to the DB.  This is Required. If you attempt to upload the same gvcfs and do not change this an error will be thrown.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.haplotypeMethodDescription<ul> <li>This is the description for the haplotypeMethodName.  If this is not set and empty description will be written to the DB.  It is strongly suggested that you put something in this field.</li> </ul> </li> <li>extendedWindowSize=1000<ul> <li>This Script will extract out the Focused reference ranges from the Database into a BED file.  When doing the BAM file filtering, we allow the user to add flanking regions around each of the regions.</li> </ul> </li> <li>mapQ=48<ul> <li>Minimum MapQ value allowed in the BAM file filtering.  This is to reduce the number of reads which map to multiple locations across the genome.</li> </ul> </li> </ul>"},{"location":"Pipeline_version1/CreateHaplotypesFromFastq/#gatk-and-sentieon-parameters","title":"GATK and Sentieon Parameters","text":"<ul> <li>gatkPath=/gatk/gatk<ul> <li>Location of GATK on this system.  This is the location within the PHG docker</li> </ul> </li> <li>numThreads=10<ul> <li>Number of threads requested to run HaplotypeCaller.  </li> </ul> </li> <li>sentieon_license<ul> <li>If you have access to Sentieon and wish to use it, provide the sentieon license here.  The Script will automatically attempt to use Sentieon if this parameter is specified.</li> </ul> </li> <li>sentieonPath=/sentieon/bin/sentieon<ul> <li>This is the Docker specific path to sentieon.  If sentieon is installed somewhere else on your system, change this parameter.</li> </ul> </li> </ul>"},{"location":"Pipeline_version1/CreateHaplotypesFromGVCF/","title":"Overview","text":"<p>This Groovy Script will create haplotypes originating from a set of filtered GVCF files specified in a keyfile.  This groovy script can be run by itself, but it is also called from CreateHaplotypesFromBAM.groovy (source).  Basically if you already have GVCFs that are filtered to your liking and are ready to be uploaded, you should use this script.</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromGVCF/#note-as-of-phg-version-0020-the-keyfile-plugin-parameter-has-been-renamed-wgskeyfile","title":"Note, as of PHG version 0.0.20, the keyFile plugin parameter has been renamed wgsKeyFile.","text":""},{"location":"Pipeline_version1/CreateHaplotypesFromGVCF/#more-information-about-the-keyfile-format-is-here","title":"More information about the keyfile format is here:","text":"<p>https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/DockerPipeline/CreateHaplotypeKeyFile.</p> <p>Note only records with type GVCF will be processed by this plugin.</p> <p>If you need to filter, either use an external tool like bcftools or vcftools.  Alternatively you can use FilterGVCFSingleFilePlugin in PHG. The Documentation for FilterGVCFPlugin describes the possible filters applied to the GVCF file.  </p> <p>For Maize, we set mapQ=48, DP_poisson_min=0.01, DP_poisson_max=0.99, GQ_min=50 and filterHets=t. NOTE THESE ARE FOR MAIZE WGS WITH GOOD COVERAGE &gt;5X.  These can be used as starting points, but you should not follow these values blindly.  </p>"},{"location":"Pipeline_version1/CreateHaplotypesFromGVCF/#we-strongly-suggest-you-use-an-external-tool-so-you-know-exactly-what-filters-are-applied-to-your-gvcf-file","title":"We strongly suggest you use an external tool so you know exactly what filters are applied to your GVCF file.","text":""},{"location":"Pipeline_version1/CreateHaplotypesFromGVCF/#pipeline-steps-run-by-createhaplotypesfromgvcf","title":"Pipeline Steps run by CreateHaplotypesFromGVCF","text":"<ol> <li>Extract out the Reference Ranges and load the haplotypes to the DB</li> </ol>"},{"location":"Pipeline_version1/CreateHaplotypesFromGVCF/#example-run-command","title":"Example Run Command","text":"<pre><code>#!bash\n\nCreateHaplotypesFromGVCF.groovy -[hd] -config [dbConfigFile]\n</code></pre>"},{"location":"Pipeline_version1/CreateHaplotypesFromGVCF/#command-line-flags-note-some-parameters-have-both-a-short-and-long-name","title":"Command Line Flags (note some parameters have both a short and long name)","text":"<pre><code>#!bash\n\n[-h, -help] 'Show usage information'\n[-d, -description] 'Show information about this Pipeline'\n[-config] 'DB Config File(required)'\n</code></pre> <p>This Groovy Script will open up the keyFile(location is specified as the LoadHaplotypesFromGVCFPlugin.keyFile entry in the config file) and will loop over all GVCF records and upload to the DB.  If multiple GVCFs are stored for a given taxon, each GVCF will be treated as an independent haplotype.</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromGVCF/#sample-script","title":"Sample Script:","text":"<pre><code>#!bash\n#Set these properties\nREF_DIR=/workdir/user/DockerTuningTests/InputFiles/Reference/\nGVCF_DIR=/workdir/user/DockerTuningTests/InputFiles/\nGVCF_DIR_IN_DOCKER=/tempFileDir/data/outputs/gvcfs/\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nCONFIG_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/config.txt\nCONFIG_FILE_IN_DOCKER=/tempFileDir/data/config.txt\nKEY_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/keyfile.txt\nKEY_FILE_IN_DOCKER=/tempFileDir/data/keyFile.txt\n\ndocker run --name small_seq_test_container --rm \\\n        -v ${REF_DIR}:/tempFileDir/data/reference/ \\\n        -v ${GVCF_DIR}:${GVCF_DIR_IN_DOCKER} \\\n        -v ${DB}:/tempFileDir/outputDir/phgTestMaizeDB.db \\\n        -v ${CONFIG_FILE}:${CONFIG_FILE_IN_DOCKER} \\\n        -v ${KEY_FILE}:${KEY_FILE_IN_DOCKER} \\\n        -t maizegenetics/phg \\\n        /CreateHaplotypesFromGVCF.groovy \\\n            -config ${CONFIG_FILE_IN_DOCKER}\n\n</code></pre> <p>Note the GVCF_DIR_IN_DOCKER needs to also be specified in the config file under the Parameter:LoadHaplotypesFromGVCFPlugin.gvcfDir</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromGVCF/#config-file-parameters","title":"Config File Parameters","text":"<p>This CreateHaplotypes Script will need some configuration parameters set in order to work correctly.  These must go in you config.txt file. You must set referenceFasta, LoadHaplotypesFromGVCFPlugin.keyFile, LoadHaplotypesFromGVCFPlugin.gvcfDir, LoadHaplotypesFromGVCFPlugin.referenceFasta, LoadHaplotypesFromGVCFPlugin.haplotypeMethodName.</p>"},{"location":"Pipeline_version1/CreateHaplotypesFromGVCF/#file-directories","title":"File Directories.","text":"<ul> <li>tempFileDir=/tempFileDir/data/bam/temp/<ul> <li>The Temp file directory.  This Location is used to make any intermediate files when processing.  For this script, only the BED File will be extracted containing the Focused Reference Ranges setup in a previous step.</li> </ul> </li> </ul>"},{"location":"Pipeline_version1/CreateHaplotypesFromGVCF/#tassel-parameters","title":"TASSEL parameters","text":"<ul> <li>Xmx=10G<ul> <li>Max Java Heap Space used when running TASSEL code.</li> </ul> </li> <li>tasselLocation=/tassel-5-standalone/run_pipeline.pl<ul> <li>Location of TASSEL on machine.  If using PHG docker this is the correct location</li> </ul> </li> </ul>"},{"location":"Pipeline_version1/CreateHaplotypesFromGVCF/#phg-createhaplotypes-parameters","title":"PHG CreateHaplotypes Parameters","text":"<ul> <li>referenceFasta<ul> <li>Reference fasta file location.  This is Required. Note, if using Docker, this needs to be the Docker specific path to the reference file.  You will need to mount these files to the Docker in order for it to work.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.wgsKeyFile<ul> <li>Location of the keyfile. This is Required. Note, if using Docker, this needs to be the Docker specific path to the keyfile.  You will need to mount this file to the Docker in order for it to work.  Note before PHG version 0.0.20, this parameter was named keyFile.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.gvcfDir<ul> <li>Directory of the GVCF files you wish to upload. This is Required. Note, if using Docker, this needs to be the Docker specific path to the gvcfs.  You will need to mount these files to the Docker in order for it to work. </li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.referenceFasta<ul> <li>This should be set to the same location as referenceFasta. This is Required.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.haplotypeMethodName=GATK_PIPELINE<ul> <li>This is the method name which you are going to write to the DB.  This is Required. If you attempt to upload the same gvcfs and do not change this an error will be thrown.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.haplotypeMethodDescription<ul> <li>This is the description for the haplotypeMethodName.  This is Required.</li> </ul> </li> <li>extendedWindowSize=1000<ul> <li>This Script will extract out the Focused reference ranges from the Database into a BED file.  When doing the BAM file filtering, we allow the user to add flanking regions around each of the regions.</li> </ul> </li> <li>numThreads=10<ul> <li>Number of threads requested to run HaplotypeCaller.  </li> </ul> </li> <li>refRangeMethods=refRegionGroup<ul> <li>This is used to extract a BED file out of the DB before the GVCF file is processed.  The BED file is then used to extract out regions of the GVCF used to become the haplotypes.  Typically, refRegionGroup refers to the anchor Reference ranges.  If \"refRegionGroup,refInterRegionGroup\" is used it will create a BED file representing both anchors and inter anchors.  We strongly suggest not setting this parameter in the Config File</li> </ul> </li> </ul>"},{"location":"Pipeline_version1/CreateReferenceIntervals/","title":"CreateReferenceIntervals","text":""},{"location":"Pipeline_version1/CreateReferenceIntervals/#script-purpose","title":"SCRIPT PURPOSE","text":"<p>Generate reference intervals for the Practical Haplotype Graph (PHG)</p>"},{"location":"Pipeline_version1/CreateReferenceIntervals/#notes","title":"NOTES","text":"<ul> <li>This script assumes it is run inside the PHG Docker container with predefined I/O paths</li> <li>The .gff file is assumed to be in JGI format: gene models have the \"gene\" name, and an \"ID=...\" field is present in annotation</li> </ul>"},{"location":"Pipeline_version1/CreateReferenceIntervals/#running-the-script","title":"RUNNING THE SCRIPT","text":"<pre><code>#!bash\n\ndocker run --rm \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\\\n-v /your_data_folder/:/tempFileDir/data \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\\\nmaizegenetics/phg \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\\\n/CreateReferenceIntervals.sh -f your_reference.fasta -a your_reference.gene.gff3 [ ... optional parameters]\n</code></pre>"},{"location":"Pipeline_version1/CreateReferenceIntervals/#required-parameters","title":"REQUIRED PARAMETERS","text":"<pre><code>#!bash\n\n   -f &lt;file name&gt;  \n      name of fasta file containing the reference sequence  \n   -a &lt;file name&gt;  \n      name of genome annotation file in .gff format containing gene model annotation \n\n</code></pre>"},{"location":"Pipeline_version1/CreateReferenceIntervals/#optional-parameters","title":"OPTIONAL PARAMETERS","text":"<pre><code>#!bash\n\n  -k &lt;integer&gt;  \n     Length of kmer used for determining repetitive regions  \n     Default: 11\n  -e &lt;integer&gt;  \n     Number of bases by which to expand gene models for initial reference interval selection  \n     Default: 1000\n  -m &lt;integer&gt;  \n     Distance (in bp) between genes below which gene models are merged  \n     Default: 100\n  -p &lt;double&gt;  \n     Proportion of kmers to be considered repetitive.  \n     This determines the high kmer count tail which is considered repetitive (e.g. the top 0.05 most frequent)  \n     Default: 0.1\n  -n &lt;integer&gt;  \n     Number of kmer copies (genome-wide) above which a kmer is considered repetitive. Overrides -p  \n     Default: none, -p is used by default\n  -l &lt;integer&gt;  \n     The number of bases to consider when evaluating if a location in the genome is repetitive  \n     Default: 100\n  -s &lt;integer&gt;  \n     The step size (in bp) by which to proceed outward from a gene model when evaluating flanking regions  \n     Default: 10\n</code></pre>"},{"location":"Pipeline_version1/CreateReferenceIntervals/#script-results","title":"SCRIPT RESULTS","text":""},{"location":"Pipeline_version1/CreateReferenceIntervals/#output-location-subfolder-in-the-input-data-folder","title":"Output location (subfolder in the input data folder)","text":"<ul> <li>genomic_intervals_unique-timestamp</li> </ul>"},{"location":"Pipeline_version1/CreateReferenceIntervals/#relevant-output-contents","title":"Relevant output contents","text":"<ul> <li>reference_intervals_run.log -- a log file summarizing parameters for the run</li> <li>your_fasta.gene.expand.trimmed.summary_report.tsv -- a summary of seed gene model expansion</li> <li>your_fasta.kmer_count.tsv -- a complete list of kmer counts for kmers with count &gt; 1</li> <li>your_fasta.gene.expand.trimmed.bed -- the final reference intervals, in BED format</li> </ul>"},{"location":"Pipeline_version1/ExportPath/","title":"ExportPath","text":"<p>This script allows paths that have been exported to text files from the HapCountBestPathToTextPlugin to be imported via ImportHaplotypePathFilePlugin and associated with nodes in the haplotype graph, then exported as a VCF file via the PathsToVCFPlugin.  This method needs to be updated to take paths either from files or from the PHG DB paths table.  The end result of this script is a VCF file.</p> <p>The parameters to this shell script are:</p> <ul> <li>Data base config file:  Configuration file containing properties host, user, password, DB and DBtype where DBtype is either sqlite or postgres. A sample config file can be found here:Config File Wiki</li> <li>Consensus method: The Consensus Method used to create haplotypes in graph.</li> <li>Output file: Output VCF file</li> <li>path method: Used to grab the correct paths from the DB.  This needs to match what was used in FindPathMinimap2.sh</li> </ul> <p>When ExportPath.sh runs in a Docker container, the following mounted directories are expected</p> <ul> <li>Mount localMachine:/pathToOutputs/FindPathDir/ to docker:/tempFileDir/outputDir/. To make this work correctly, the DB must also be here.  If the DB is defined in a different place within the config file, you will need to make a new mount point accordingly.</li> <li>Mount localMachine:/pathToInputs/config.txt to docker:/tempFileDir/data/config.txt</li> </ul> <p>In addition, it is expected the database is stored in the User FindPathDir that is mounted below</p> <p>An example Docker script to run the ExportPath.sh shell script is:</p> <pre><code>#!python\n\ndocker run --name cbsu_phg_container_exportPath --rm \\\n        -v /workdir/user/DockerTuningTests/DockerOutput/FindPathDir/:/tempFileDir/outputDir/ \\\n        -v /workdir/user/DockerTuningTests/InputFiles/config.txt:/tempFileDir/data/config.txt \\\n        -t maizegenetics/phg:latest \\\n        /ExportPath.sh config.txt CONSENSUS testOutput1.vcf PATH_METHOD\n</code></pre> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The \"-t\" directive indicates the Docker image of which this container will be an instance.  The last line tells the Docker container to run the ExportPath.sh script which is found in the root directory.  The items following are the parameters to the ExportPath.sh script.</p>"},{"location":"Pipeline_version1/FastqToHapCountPlugin/","title":"FastqToHapCountPlugin","text":"<p>The FastqToHapCountPlugin executes code to count the number of reads which align to a haplotype node. It scores which haplotypes are identical, excluded, or unresolved relative to a perfect hit GenotypeMap. The plugin requires a PHG Haplotype Graph, which can be acquired by chaining the running of HaplotypeGraphBuilderPlugin and this plugin.  The output from the HaplotypeGraphBuilderPlugin will be input to the FastqToHapCountPlugin.  For example:</p> <pre><code>#!java\n     /tassel-5-standalone/run_pipeline_jbwa.pl -Xmx5g -debug -HaplotypeGraphBuilderPlugin \n          -configFile $CONFIGFILE -onlyAnchors true -consensusMethod $CONSENSUS_METHOD \n          -includeVariantContexts true -endPlugin \n          -FastqToHapCountPlugin -taxon $file -configFile $CONFIGFILE -haplotypeFile $HAPLOTYPE_FASTA \n          -method $HAP_COUNT_METHOD -version $VERSION -refFile $REFERENCE_FILE \n          -rawReadFile ${FASTQ_DIR}/${file}.fastq -exportHaploFile ${FASTQ_HAP_COUNT_DIR}/${file}.txt\n</code></pre> <p>The parameters for this plugin are:</p> <ul> <li>-configFile  DB Config File containing properties host,user,password,DB and DBtype where DBtype is either sqlite or postgres (Default=null) (REQUIRED) A sample config file can be found here:Config File Wiki. <li>-haplotypeFile  Fasta file and associated BWA indices for haplotypes stored in the PHG db. (Default=null) (REQUIRED) <li>-refFile  Reference genome file - temporary need until we can back convert coordinates. (Default=null) (REQUIRED) <li>-rawReadFile  Raw Read file aligned to the reference. (Default=null) (REQUIRED) <li>-exportHaploFile  Text file to store haplotype scoring. (Default=null) (OPTIONAL) <li>-taxon  Taxon name to load into genotypes table from linked plugin. (Default=null) (REQUIRED) <li>-debugTaxon  Used for debugging. (Defaulg=null) (OPTIONAL) <li>-method  Name of method used to create hap counts, for the haplotype_counts table. (Default=null) (REQUIRED) <li>-version  Genome intervals version name as stored in the PHG db genome_intervals_version table. (Default=null) (REQUIRED) <li>-loadDB  Whether to populate the haplotype_counts table.  Should be true unless user is testing. (Default=true) (OPTIONAL) <p>Once the haplotype counts have been determined, they are loaded to the database if the loadDB flag is \"true\"..</p>"},{"location":"Pipeline_version1/FilterGVCFPlugin/","title":"FilterGVCFPlugin","text":"<p>This plugin filters a GVCF file and creates fasta sequence for loading to the PHG database.  BCFTools is used with a poissonProbability tuple to filter based on depth.</p> <p>Tables populated via this method are:</p> <ul> <li>genotypes</li> <li>gametes</li> <li>gamete_groups</li> <li>methods</li> <li>haplotypes</li> <li>gamete_haplotypes</li> </ul> <p>The parameters to this plugin are:</p> <ul> <li>-refFile  Reference File used to create the GVCFs. (Default is null) (REQUIRED) <li>-bedFile  Bed file containing the genome interval information. (Default is null) (REQUIRED) <li>-gvcfDir  Directory holding the GVCF files to be filtered. (Default is null) (REQUIRED) <li>-outputDir  Directory to hold the output files. (Default is null) (REQUIRED) <li>-configFile  Config file containing the filtering parameters. If not present, defaults will be used. (Default is null) (OPTIONAL) <li>-lowerPoissonBound  Lower Poisson Bound used for filtering.  (Default: 0.0) (OPTIONAL) <li>-upperPoissonBound  Lower Poisson Bound used for filtering.  (Default: 1.0) (OPTIONAL) <p>The configFile parameter should contain key-value pairs.  If this file is present, the gvcf files will be filtered based on the file's parameters. To read more about the strucure of variant call records and the meaning of the fields below, please see GATK Forum.  The supported key-value pairs for the config file are below.  Any fields left out will be ignored.</p> <ul> <li>exclusionString = String used for bcftools filtering.  If this is specified, no additional terms are added to exclude</li> <li>DP_poisson_min = minimum poisson bound used for filtering(absolute minimum is 0.0)</li> <li>DP_poisson_max = maximum poisson bound used for filtering(absolute maximum is 1.0)</li> <li>DP_min = minimum DP bound: the filtered depth at the sample level. Minimum number of filtered reads that support each of the reported alleles.</li> <li>DP_max = maximum DP bound: maximum number of filtered reads that support each of the reported alleles.</li> <li>GQ_min = minimum GQ bound: minimum confidence level that the genotype assigned to a particular sample is correct.</li> <li>GQ_max = maximum GQ bound: maximum confidence level that the genotype assigned to a particular sample is correct.</li> <li>QUAL_min = base quality minimum bound: confidence there exists variation at a given site.</li> <li>QUAL_max = base quality maximum bound: confidence there exists variation at a given site.</li> <li>filterHets = true/false or t/f, if true will filter sites where 2 or more alleles have above 0 </li> </ul>"},{"location":"Pipeline_version1/FindHaplotypeClustersPlugin/","title":"FindHaplotypeClustersPlugin","text":"<p>The FindHaplotypeClustersPlugin processes a multi-haplotype VCF file from one region, and identifies haplotype clusters to be collapsed into a consensus haplotype.  For every alignment in a genotype table, taxa are filtered by coverage to create clusters.  The clusters are determined based on number of alignments without indels, a specified minimum number of polymorphic sites to calculate identity and a maximum distance from each taxon.  The resulting clusters are filtered again based on a minimum number of taxa in each group.  Those clusters passing the filtering criteria have their sequence pulled and written to a fasta file for loading to the PHG DB haplotypes table.</p> <p>The parameters for this plugin are:</p> <ul> <li>-ref  input Reference fasta for pulling reference sequence where gvcf indicates REFRANGE. (Default=null) (REQUIRED) <li>-haplotypeMethod  Name of haplotype method to sue when retrieving haplotypes form the graph.  Must be a method currently stored in the PHG db methods table.  (Default=null) (REQUIRED) <li>-dbConfigFile  File holding configuration information for connecting to the PHG DB. (Default=null) (REQUIRED) <li>-consensusVCFOutputDir  Directory in which to store the output VCFs from the consensus process. (Default=null) (REQUIRED) <li>-consensusFastaOutputDir  Directory in which to store the output fastas from the consensus process. (Default=null) (REQUIRED) <li>-refVersion  Version name of the reference intervals.  Must match a version name that is stored in the genome_interval_versions DB table. (Default=null) (REQUIRED) <li>-collapseMethod  Name to be stored in the DB for this method used to collapse the haplotypes into consensus sequences. (Default=null) (REQUIRED) <li>collapseMethodDetails  Details describing the collapse method. (Default=null) (REQUIRED)"},{"location":"Pipeline_version1/FindPath/","title":"FindPath","text":"<p>The FindPath.sh script is used to determine the best path through the haplotype graph.  The script uses fastq files to generate haplotype counts using a Haplotype Graph created from the database. The counts are then used with a Hidden Markov Model (HMM) algorithm to find the best path through the graph for each taxon. The list haplotype node ids comprising the path is written to a text file.  The haplotype counts are written to the database table haplotype_counts, the paths data is written to the database table paths.</p> <p>This shell script takes 8 parameters in this order. These parameters are passed to the called plugins.</p> <ul> <li>name of the PHG database from which sequences will be pulled</li> <li>the name of a database configuration file, for connecting to the database in later steps. A sample config file can be found here:Config File Wiki.</li> <li>The name of the method (consensus type or other) used to create the haplotype sequences to be used when processing the skim sequences.</li> <li>The name of a reference fasta file used when processing the db haplotype sequences. (i.e. Zea_mays.AGPv4.dna.toplevel.fa)</li> <li>A method name to store for this haplotype counting instance.</li> <li>A method name to store for the paths table for this execution instance.</li> <li>The name of a file containing the reference ranges to keep (this is optional)</li> </ul> <p>When invoked, the script first looks to see if a haplotype fasta file of the given name (first parameter) exists.  If not, one is created from the graph and indexed (bwa indexed).  The FastqToHapCountPlugin is then run for every file in the fastq directory. This plugin determines the number of reads from the fastq file that match the sequence of the haplotype nodes. It scores which haplotypes are identical, excluded, or unresolved relative to a perfect hit GenotypeMap.  For every taxon represented by a fastq file, the haplotype counts are stored to the database and written to files in the fastq_hap_count directory.</p> <p>Once the haplotype counts have been calculated, HapCountBestPathToTextPlugin is run to choose the best path through the graph for each taxon.  The best paths are written to files in the hap_count_best_path directory.</p> <p>When FindPath.sh is run as part of a Docker container script, the Docker script expects the following directory mount points:</p> <ul> <li>Mount localMachine:/pathToInputs/FastQFiles/ to docker:/tempFileDir/data/fastq</li> <li>Mount localMachine:/pathToInputs/refFolder/ to docker:/tempFileDir/data/reference/</li> <li>Mount localMachine:/pathToOutputs/ to docker:/tempFileDir/outputDir/</li> <li>Mount localMachine:/pathToInputs/config.txt to docker:/tempFileDir/data/config.txt</li> </ul> <p>It is expected the database is stored in the User specified outputDir that is mounted below and the config.txt specifies the database name and login parameters.</p> <p>An example Docker script to run the FindPath.sh shell script is:</p> <pre><code>#!python\n\ndocker1 run --name cbsu_phg_container_findPath --rm \\\n        -v /workdir/user/DockerTuningTests/InputFiles/Reference/:/tempFileDir/data/reference/ \\\n        -v /workdir/user/DockerTuningTests/InputFiles/GBSFastq/:/tempFileDir/data/fastq/ \\\n        -v /workdir/user/DockerTuningTests/DockerOutput/:/tempFileDir/outputDir/ \\\n        -v /workdir/user/DockerTuningTests/InputFiles/config.txt:/tempFileDir/data/config.txt \\\n        -t phgrepository_test:latest \\\n        /FindPath.sh panGenome12Taxa config.txt CONSENSUS Zea_mays.AGPv4.dna.toplevelMtPtv3.fa HAP_COUNT_METHOD  PATH_METHOD\n</code></pre> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The \"-t\" directive indicates the Docker image of which this container will be an instance.  The last line tells the Docker container to run the FindPath.sh script which is found in the root directory.  The items following are the parameters to the FindPath.sh script.</p>"},{"location":"Pipeline_version1/FindPathKeyFiles/","title":"Find Path Key Files","text":"<p>These two keyfiles are tab-separated text files which setup fastq alignment to the pangenome and which readMappings need to be aggregated when finding Paths.</p>"},{"location":"Pipeline_version1/FindPathKeyFiles/#fastqtomappingplugin-keyfile-specification","title":"FastqToMappingPlugin KeyFile Specification:","text":"<p>The PHG will process the following columns:</p> HeaderName Description Required cultivar Name of the taxon to be processed. Yes flowcell_lane Name of the flow cell this sample came from Yes filename Name of the Fastq file to be processed Yes filename2 Second Fastq file to be processed.  If set, minimap2 will operate in paired end mode No PlateID The id of the Plate. No <p>Note that cultivar + flowcell_lane + PlateID must be a unique value across all samples in the keyfile.  If this is not, the PHG will not work correctly. After running through FastqToMapping, an additional column will be added denoting the ReadMappingID values in the DB.  This is purely for reference and is not used currently in the pipeline.</p>"},{"location":"Pipeline_version1/FindPathKeyFiles/#sample-file","title":"Sample File:","text":"<pre><code>#!txt\n\ncultivar    flowcell_lane   filename    PlateID\nRecLineB1RefA1gco4_wgs  wgsFlowcell RecLineB1RefA1gco4_R1.fastq wgs\nRecLineB1RefA1gco4_gbs  gbsFlowcell RecLineB1RefA1gco4_R1_gbs.fastq gbs\nRecRefA1LineBgco6_wgs   wgsFlowcell RecRefA1LineBgco6_R1.fastq  wgs\nRecRefA1LineBgco6_gbs   gbsFlowcell RecRefA1LineBgco6_R1_gbs.fastq  gbs\nLineB1_wgs  wgsFlowcell LineB1_R1.fastq wgs\nLineB1_gbs  gbsFlowcell LineB1_R1_gbs.fastq gbs\nLineA_wgs   wgsFlowcell LineA_R1.fastq  wgs\nLineA_gbs   gbsFlowcell LineA_R1_gbs.fastq  gbs\nLineB_wgs   wgsFlowcell LineB_R1.fastq  wgs\nLineB_gbs   gbsFlowcell LineB_R1_gbs.fastq  gbs\nLineA1_wgs  wgsFlowcell LineA1_R1.fastq wgs\nLineA1_gbs  gbsFlowcell LineA1_R1_gbs.fastq gbs\nRecLineALineB1gco3_wgs  wgsFlowcell RecLineALineB1gco3_R1.fastq wgs\nRecLineALineB1gco3_gbs  gbsFlowcell RecLineALineB1gco3_R1_gbs.fastq gbs\nRecLineB1LineBgco7_wgs  wgsFlowcell RecLineB1LineBgco7_R1.fastq wgs\nRecLineB1LineBgco7_gbs  gbsFlowcell RecLineB1LineBgco7_R1_gbs.fastq gbs\nRefA1_wgs   wgsFlowcell RefA1_R1.fastq  wgs\nRefA1_gbs   gbsFlowcell RefA1_R1_gbs.fastq  gbs\nRef_wgs wgsFlowcell Ref_R1.fastq    wgs\nRef_gbs gbsFlowcell Ref_R1_gbs.fastq    gbs\nRecLineBLineB1gco8_wgs  wgsFlowcell RecLineBLineB1gco8_R1.fastq wgs\nRecLineBLineB1gco8_gbs  gbsFlowcell RecLineBLineB1gco8_R1_gbs.fastq gbs\nRecLineA1LineA1gco2_wgs wgsFlowcell RecLineA1LineA1gco2_R1.fastq    wgs\nRecLineA1LineA1gco2_gbs gbsFlowcell RecLineA1LineA1gco2_R1_gbs.fastq    gbs\nRecLineBLineB1gco5_wgs  wgsFlowcell RecLineBLineB1gco5_R1.fastq wgs\nRecLineBLineB1gco5_gbs  gbsFlowcell RecLineBLineB1gco5_R1_gbs.fastq gbs\nRecLineA1RefA1gco1_wgs  wgsFlowcell RecLineA1RefA1gco1_R1.fastq wgs\nRecLineA1RefA1gco1_gbs  gbsFlowcell RecLineA1RefA1gco1_R1_gbs.fastq gbs\n\n</code></pre>"},{"location":"Pipeline_version1/FindPathKeyFiles/#find-path-keyfile-specification","title":"Find Path Keyfile Specification","text":"<p>Note: This file will likely not be needed to create.  FastqToMappingPlugin will export a sample keyfile given its inputs.  This will probably be good enough for the majority of use cases.</p> <p>The PHG will process the following columns:</p> HeaderName Description Required sampleName Name of the taxon to be processed. Yes ReadMappingIds ReadMappingIds in the DB. All readMappings need to be comma separated. Yes LikelyParents List of taxon which the user believes are the likely parents for this sample.  Currently this is not used. No"},{"location":"Pipeline_version1/FindPathKeyFiles/#sample-file_1","title":"Sample File:","text":"<pre><code>#!txt\n\nSampleName  ReadMappingIds  LikelyParents\nRecLineB1RefA1gco4_wgs  1       \nRecLineB1RefA1gco4_gbs  2       \nRecRefA1LineBgco6_wgs   3       \nRecRefA1LineBgco6_gbs   4       \nLineB1_wgs  5       \nLineB1_gbs  6       \nLineA_wgs   7       \nLineA_gbs   8       \nLineB_wgs   9       \nLineB_gbs   10      \nLineA1_wgs  11      \nLineA1_gbs  12      \nRecLineALineB1gco3_wgs  13      \nRecLineALineB1gco3_gbs  14      \nRecLineB1LineBgco7_wgs  15      \nRecLineB1LineBgco7_gbs  16      \nRefA1_wgs   17      \nRefA1_gbs   18      \nRef_wgs 19      \nRef_gbs 20      \nRecLineBLineB1gco8_wgs  21      \nRecLineBLineB1gco8_gbs  22      \nRecLineA1LineA1gco2_wgs 23      \nRecLineA1LineA1gco2_gbs 24      \nRecLineBLineB1gco5_wgs  25      \nRecLineBLineB1gco5_gbs  26      \nRecLineA1RefA1gco1_wgs  27      \nRecLineA1RefA1gco1_gbs  28      \n\n\n</code></pre>"},{"location":"Pipeline_version1/FindPathMinimap2/","title":"Overview","text":"<p>This shell script will use the Minimap2 index created by IndexPangenome.sh and will align a set of reads to the graph and then will use a HMM to find the most likely path through the graph given the alignments. The database \"paths\" and \"read_mapping_paths\" tables are populated from the plugins called by this script.</p>"},{"location":"Pipeline_version1/FindPathMinimap2/#pipeline-steps","title":"Pipeline Steps","text":"<ol> <li> <p>Run FastqToMappingPlugin to map a set of reads to the pangenome fasta file.  This plugin will make use of a keyfile and will store the mappings in the DB.</p> </li> <li> <p>Run HapCountBestPathToTextPlugin to take the mappings from the DB and use a HMM to find the optimal path through the DB.  This will then store the paths into the DB.</p> </li> </ol>"},{"location":"Pipeline_version1/FindPathMinimap2/#keyfiles","title":"KeyFiles","text":"<p>The FindPathMinimap2 Pipeline will require the use of 2 keyfiles.  The second is autogenerated, but could be changed if different results are desired.  Both files need to be tab-separated.  If there are entries in the keyfile, but not found on the filesystem, the pipelines will skip over those entries.</p> <p>More information about the keyfiles can be found here: https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/DockerPipeline/FindPathKeyFiles</p>"},{"location":"Pipeline_version1/FindPathMinimap2/#example-run-command","title":"Example Run Command","text":"<pre><code>#!bash\n\nFindPathMinimap2.sh [BASE_HAPLOTYPE_NAME] [CONFIG_FILE_NAME] [HAPLOTYPE_METHOD] [HAPLOTYPE_METHOD_FIND_PATH] [HAPCOUNT_METHOD_NAME] [PATH_METHOD_NAME] [READ_KEY_FILE] [PATH_KEY_FILE]\n\n</code></pre>"},{"location":"Pipeline_version1/FindPathMinimap2/#command-line-flags","title":"Command Line Flags","text":"<pre><code>#!bash\nBASE_HAPLOTYPE_NAME: This is the base of the name of the haplotype fasta file. This file should have been indexed in the previous step.\nCONFIG_FILE_NAME: This is the path to the config.txt file used to create the DB.  All the needed DB connection information should be in here.\nHAPLOTYPE_METHOD: Method name of the haplotypes in the graph that were used to generate the haplotype fasta file in IndexPangenome.sh.  This needs to match exactly otherwise the results will not be correct.\nHAPLOTYPE_METHOD_FIND_PATH: This method can be the same as HAPLOTYPE_METHOD, but it is typically used to only include anchor reference ranges when running FindPath.  Typically, finding paths through the interanchor reference ranges can cause additional errors.  And example of what could be used here is this: HAPLOTYPE_METHOD,refRegionGroup to only include the refRegionGroup refRange group in the Graph used for finding the path.\nHAPCOUNT_METHOD_NAME: Name of the Haplotype Mapping method used to upload the ReadMapping files to the DB.  This is currently not used so a Dummy value can be used.  It will be implemented in the future.\nPATH_METHOD_NAME: Name of the Path Mapping method used to upload the Paths to the DB. This method name will be used in the next step (ExportPath.sh) to extract out the paths from the DB.\nREAD_KEY_FILE: This name needs to match the name of the keyfile.  This keyfile will describe what fastq files need to be aligned together and also denotes the required metadata fields which are stored in the DB.\nPATH_KEY_FILE:  This name is what the path finding keyfile will be named.  FastqToMappingPlugin will create this file and then BestHaplotypePathPlugin will use it to find paths.  Note that FastqToMappingPlugin will group reads by taxon and all the mappings for a given taxon will be used when finding the paths.\n\n</code></pre>"},{"location":"Pipeline_version1/FindPathMinimap2/#docker-commands","title":"Docker Commands","text":"<p>When FindPathMinimap2.sh is run as part of a Docker container script, the Docker script expects the following directory mount points:</p> <ul> <li>Mount localMachine:/pathToInputs/FastQFiles/ to docker:/tempFileDir/data/fastq</li> <li>Mount localMachine:/pathToOutputs/ to docker:/tempFileDir/outputDir/</li> <li>Mount localMachine:/pathToPangenomeIndex/ to docker:/tempFileDir/outputDir/pangenome/</li> <li>Mount localMachine:/pathToInputs/config.txt to docker:/tempFileDir/data/config.txt</li> <li>Mount localMachine:/pathToInputs/phg.db to docker:/tempFileDir/outputDir/phgTestMaizeDB.db.  This needs to match what is in the config file.</li> </ul> <p>It is expected the database is stored in the User specified outputDir that is mounted below and the config.txt specifies the database name and login parameters.</p> <p>It is critical that the .mmi file is mounted to /tempFileDir/outputDir/pangenome/ in the docker.  Otherwise this will not work correctly.  </p> <p>If you see this error: ERROR net.maizegenetics.plugindef.AbstractPlugin - Haplotype count methodid not found in db for method : HAP_COUNT_METHOD, it means that something went wrong during the ReadMapping step.  Double check the -v parameters and make sure the mmi file is in /tempFileDir/outputDir/pangenome/</p> <p>An example Docker script to run the FindPathMinimap2.sh shell script is:</p> <pre><code>#!bash\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nPANGENOME_DIR=/workdir/user/DockerTuningTests/DockerOutput/PangenomeFasta/  \n\ndocker run --name small_seq_test_container --rm \\ \n                    -w / \\\n                    -v /workdir/user/DockerTuningTests/DockerOutput/:/tempFileDir/outputDir/ \\\n                    -v /workdir/user/DockerTuningTests/InputFiles/GBSFastq/:/tempFileDir/data/fastq/ \\\n                    -v /workdir/user/DockerTuningTests/InputFiles/config.txt:/tempFileDir/data/configSQLite.txt \\\n                    -v ${DB}:/tempFileDir/outputDir/phgTestMaizeDB.db \\\n                    -v ${PANGENOME_DIR}:/tempFileDir/outputDir/pangenome/ \\\n                    -t maizegenetics/phg:latest \\\n                    /FindPathMinimap2.sh phgSmallSeqSequence configSQLite.txt \\\n                    CONSENSUS CONSENSUS,refRegionGroup \\\n                    HAP_COUNT_METHOD PATH_METHOD \\\n                    /tempFileDir/data/fastq/genotypingKeyFile.txt \\\n                    /tempFileDir/data/fastq/genotypingKeyFile_pathKeyFile.txt\n\n</code></pre> <p>PANGENOME_DIR must match the directory set in the IndexPangenome step.</p> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The \"-t\" directive indicates the Docker image of which this container will be an instance.  The last line tells the Docker container to run the FindPath.sh script which is found in the root directory.  The items following are the parameters to the FindPath.sh script.</p>"},{"location":"Pipeline_version1/HapCountBestPathToTextPlugin/","title":"HapCountBestPathToTextPlugin","text":"<p>The HapCountBestPathToTextPlugin processes haplotype node data either from inclusion files or from the haplotype_counts table of a PHG database instance.  This plugin loops through all the taxon and their inclusion/exclusion count set.  For each taxon, HapCountBestPathPlugin is called to perform the processing. HapCountBestPathPlugin uses a Hidden Markov Model (HMM) to determine the most likely path for a group of haplotype ids.  It determines the nodes on the path through the graph that is most likely to have generated the set of haplotypes.</p> <p>When the DB is used to pull haplotype_counts data, all paths created will be stored in the DB.  If an inclusion file is used, paths are NOT stored to the DB.  This is because the paths table needs the haplotype_counts_id.  When an inclusion file is used, the assumption is that these haplotype counts are NOT stored in the DB.</p> <p>The parameters to this plugin are below, most of which are passed to the HapCountBestPathPlugin::</p> <ul> <li>-configFile  Database Configuration file containing properties host,user,password,DB and DBtype where DBtype is either sqlite or postgres (Default=null) (REQUIRED) <li>-taxa  A comma delimited list of taxa (no spaces allowed) to include in graph. Only nodes containing these taxa will be included in the graph. If no taxa list is supplied, then all taxa in the full graph will be used. (Default=null) (OPTIONAL) <li>-inclusionFileDir  The name of the file containing read inclusion and exclusion counts for hapids. (Default=null) (OPTIONAL) <li>-target  The taxon that will be used to evaluate the node list returned. (Default=null) (OPTIONAL) <li>-refRangeFile  The name of the file containing the reference ranges to keep.  Supplied when only a subset of referene ranges are desired for processing. (Default=null) (OPTIONAL) <li>-refFileName  Reference file name to allow indexing on the fly.  (Default=null) (OPTIONAL) <li>-outputDir  Output directory for writing files. (Default=null) (REQUIRED) <li>-refVersion  Ref version name as stored in the genome_interval_versions table.  Needed for storing data to the haplotype_counts table. (Default=null) (REQUIRED) <li>-hapCountMethod  Name of method used to creates inclusion/exclusion counts in FastqToHapCountPlugin. (Default=null) (REQUIRED) <li>-pMethod <p> Name of method to be stored that was used to create paths through the graph. (Default=null) (REQUIRED)"},{"location":"Pipeline_version1/ImportHaplotypePathFilePlugin/","title":"ImportHaplotypePathFilePlugin","text":"<p>The ImportHaplotypePathFilePlugin takes the haplotypePath files produced by HapCountBestPathToTextPlugin and imports them to a data structure to be used by the PathsToVCFPlugin.  The latter produces VCF files from the paths.</p> <p>This plugin expects a PHG Haplotype Graph to be sent as an incoming parameter.  The plugin requires a PHG Haplotype Graph, which can be adquired by chaining the running of HaplotypeGraphBuilderPlugin and this plugin.  The output from the HaplotypeGraphBuilderPlugin will be input to the ImportHaplotypePathFilePlugin.  In addition, the ImportHaplotypePathFilePlugin provides the input to the PathsToVCFPlugin.  All 3 plugins are likely chained when called via the command line or a shell script.  See ExportPath.sh,</p> <p>The ImporthaplotypePathFilePlugin  takes the following parameters:</p> <ul> <li>-inputFileDirectory  A directory containing the paths text files to imported and used to create the vcf files. (Default=null) (REQUIRED)</li> </ul>"},{"location":"Pipeline_version1/ImportReadMappingToDBPlugin/","title":"ImportReadMappingToDBPlugin","text":""},{"location":"Pipeline_version1/ImportReadMappingToDBPlugin/#overview","title":"Overview","text":"<p>This plugin allows the user to merge the ReadMappings stored in one DB into another.  One DB will be used as the final DB and all the other DBs needing to be merged will have the ReadMappings extracted and then written to the final DB.</p>"},{"location":"Pipeline_version1/ImportReadMappingToDBPlugin/#steps","title":"Steps:","text":"<ol> <li>Loop through each DB in -inputMappingDir(or readMapping file if -loadFromDB = false)</li> <li>Write the ReadMapping to final DB.</li> <li>(Optional) Generate a Path KeyFile for use with Path finding.</li> </ol>"},{"location":"Pipeline_version1/ImportReadMappingToDBPlugin/#example-run-command","title":"Example Run Command","text":"<pre><code>#!bash\n\ntassel-5-standalone/run_pipeline.pl -debug -Xmx100G -ImportReadMappingToDBPlugin -configFileForFinalDB final_DB_config.txt -loadFromDB true -inputMappingDir /path/to/config/files/ -readMappingMethod READ_MAPPING_METHOD -outputKeyFile /path/to/output/pathKeyfile.txt -endPlugin\n</code></pre> <p>If you ran FastqToMappingPlugin with the -debug flag set, you can also upload those ReadMapping text files.</p> <pre><code>#!bash\ntassel-5-standalone/run_pipeline.pl -debug -Xmx100G -ImportReadMappingToDBPlugin -configFileForFinalDB final_DB_config.txt -loadFromDB false -inputMappingDir /path/to/readMapping/files/ -readMappingMethod READ_MAPPING_METHOD -outputKeyFile /path/to/output/pathKeyfile.txt -endPlugin\n\n</code></pre>"},{"location":"Pipeline_version1/ImportReadMappingToDBPlugin/#plugin-parameters","title":"Plugin Parameters","text":"<pre><code>#!bash\n\n-configFileForFinalDB = the config file that holds the DB connection information for the 'final' DB\n-loadFromDB = (true or false) if true, the plugin expects config files in -inputMappingDir, otherwise, it expects Read Mapping files.\n-inputMappingDir = Directory holding all the config or ReadMapping files to be uploaded to the DB\n-readMappingMethod = This must match the Read Mapping Method used in the 'final' DB.  Otherwise a new method will be written\n-outputKeyFile = an automatically generated path key file representing the new ReadMapping IDs written to the DB.  This allows you to immediately run BestHaplotypePathPlugin after this is done.\n</code></pre> <p>Once this is done, you can run BestHaplotypePathPlugin to get Paths.</p>"},{"location":"Pipeline_version1/IndexPangenome/","title":"Overview","text":"<p>This shell script will extract out the haplotypes from the PHG DB and will then run minimap2 to index the pangenome.  This is needed for the next step FindPathsMinimap2.sh  The outputs of this step is a fasta file containing the haplotypes for the graph and a .mmi index file created by minimap indexing said fasta file.</p>"},{"location":"Pipeline_version1/IndexPangenome/#this-only-needs-to-be-run-1-time-for-a-given-graph","title":"This only needs to be run 1 time for a given graph.","text":""},{"location":"Pipeline_version1/IndexPangenome/#pipeline-steps","title":"Pipeline Steps","text":"<ol> <li>The Script will first check to see if the fasta file already is located in the path specified.<ol> <li>If not, Pull out the fasta using WriteFastaFromGraphPlugin</li> </ol> </li> <li>The Script will then check to see if the fasta file has already been indexed with the parameters specified.<ol> <li>If not, Index using minimap2.</li> </ol> </li> </ol>"},{"location":"Pipeline_version1/IndexPangenome/#example-run-command","title":"Example Run Command","text":"<pre><code>#!bash\n\nIndexPangenome.sh [BASE_HAPLOTYPE_NAME] [CONFIG_FILE_NAME] [HAPLOTYPE_METHOD] [NUM_BASES_LOADED] [MINIMIZER_SIZE] [WINDOW_SIZE]\n\n</code></pre>"},{"location":"Pipeline_version1/IndexPangenome/#command-line-flags","title":"Command Line Flags","text":"<pre><code>#!bash\nBASE_HAPLOTYPE_NAME: This is the base of the name of the haplotype fasta file.\nCONFIG_FILE_NAME: This is the path to the config.txt file used to create the DB.  All the needed DB connection information should be in here.\nHAPLOTYPE_METHOD: This is the HAPLOTYPE_METHOD_NAME used when creating the PHG.  This is used to pull out the correct haplotypes from the database.  This Name can also be a list of methods to be pulled at the same time.\nNUM_BASES_LOADED: This is a Minimap2 parameter for how many Base Pairs should be loaded into the database for each batch.  Typically you should set this to be larger than the number of Base Pairs in your Pangenome.  If this is less, Minimap2 is inconsistent in its mapping. We have used 90G for the assembly DB in maize.\nMINIMIZER_SIZE: This is the kmer size used by Minimap2 to do alignments.  We suggest you use a k of 21.\nWINDOW_SIZE: This is the number of consecutive Minimizer windows used by Minimap.  We suggest using 11.\n</code></pre>"},{"location":"Pipeline_version1/IndexPangenome/#full-command-example","title":"Full Command Example","text":"<pre><code>#!bash\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nOUTPUT_DIR=/workdir/user/DockerTuningTests/DockerOutput/PangenomeFasta/\nCONFIG_FILE=/workdir/user/DockerTuningTests/configSQLite.txt\ndocker run --name index_pangenome_container --rm \\\n                    -w / \\\n                    -v ${OUTPUT_DIR}:/tempFileDir/outputDir/pangenome/ \\\n                    -v ${DB}:/tempFileDir/outputDir/phgTestMaizeDB.db \\\n                    -v ${CONFIG_FILE}:/tempFileDir/data/configSQLite.txt \\\n                    -t maizegenetics/phg \\\n                    /IndexPangenome.sh pangenome_fasta configSQLite.txt CONSENSUS 4G 15 10\n</code></pre> <p>This command will extract the pan genome to a fasta named pangenome_fasta.fa and will also create an index file named pangenome_fasta.mmi for use in FindPathMinimap2. It will be saved to the directory specified by OUTPUT_DIR.</p> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.  NOTE:  If you are using postgreSQL instead of SQLite db, you will not need to include the -V for the database file.</p> <p>The \"-t\" directive indicates the Docker image of which this container will be an instance.  The last line tells the Docker container to run the IndexPangenome.sh script which is found in the root directory.  The items following are the parameters to the IndexPangenome.sh script.</p>"},{"location":"Pipeline_version1/LoadAssemblyAnchors/","title":"LoadAssemblyAnchors","text":"<p>The LoadAssemblyAnchors.sh script uses mummer4 scripts to align a single chromosome reference fasta file to a single matching chromosome assembly fasta file.  For example:  run maize b73 chromosome 1 against maize ph207 chromosome 1, or b73 chromosome 2 against ph207 chromosome 2. The Mummer4 nucmer, delta-filter, show-coords and show-snps scripts are run to create the necessary data.  This data is then processed and put in a format necessary for loading to the PHG data base.  View details of this process in the description of the AssemblyHaplotypesPlugin, which performs this functionality.</p> <p>When the files are split by chromosome, the fasta id line should contain the chromosome name.  If additional data is desired on this line, there must be a space after the chromosome name and before the additional data.  The chromosome names should either be just a number, or an alpha numeric string.  Note that any leading \"chr\" or \"chromosome\" will be stripped off.  This could cause inconsistencies in names between the name stored in the database and the name in the fasta files.  It is recommended you do NOT precede your chromosome name with \"chr\" or \"chromosome\"</p> <p>The reference and assembly chromosome names must be consistent in the respective fasta files.</p> <p>When LoadAssemblyAnchors is called as part of a Docker script, the Docker script expects these mount points:</p> <ul> <li>Mount localMachine:/pathToAssemblyFastas/ to docker:/tempFileDir/data/assemblyFasta</li> <li>Mount localMachine:/pathToDataDir/ to docker:/tempFileDir/data/</li> <li>Mount localMachine:/pathToOutputDir/ to docker:/tempFileDir/outputDir/</li> <li>Mount localMachine:/pathToReferenceFastaDir/ to docker:/tempFileDir/data/reference</li> </ul> <p>Note the script will create a subdirectory named \"align\" under the /tempFileDir/outputDir/ directory. This is where the mummer4 programs output will be written.</p> <p>The parameters expected by this shell script are:</p> <ul> <li>DB config File: DB Config File containing properties host,user,password,DB and DBtype where DBtype is either \"sqlite\" or \"postgres\". This file is expected to live in the folder mounted to Docker tempFileDir/data/. A sample config file can be found here:Config File Wiki</li> <li>Reference Fasta File:  The reference fasta file with only 1 chromosome of data.  This file is used when running Mummer4 alignment scripts.  This file is expected to live in the folder mounted to Docker /tempFileDir/data/reference.</li> <li>Assembly Fasta File:  The assembly fasta file with only 1 chromosome of data.  This file is used to align against the respective reference chromosome when the mummer4 scripts are run.  It is expected this file is in the folder mounted to Docker path /tempFileDir/data/assemblyFasta.</li> <li>Assembly name:  name to append to the output files that identifies the assembly processed. This is also the name that will be stored in the database genotypes table.  To differentiate the assembly from other instances of taxa with this name, you may want to name it _Assembly. <li>chrom:  the chromosome that is processed. This must match the chromosome name as stored on the ID line in the fasta file, or can be just the chromosome number if the chromosome is listed as \"chr1\" or \"chromosome1\".  Note if leading 0's are added to the chromosome name, they must be consistently added in both assembly fasta, reference fasta, and this shell parameter.</li> <li>clusterSize:  The minimum length for a cluster of matches used when running mummer4 nucmer program.  Nucmer default is 65 but we have found when running maize assemblies 250 is faster and provides good results.  That is the default we show below.</li> <p>All output from running this script will be written to files in the directory mounted to Docker /tempFileDir/outputDir/align.</p> <p>The assembly entries in the genotypes table will be defaulted to the following values:</p> <ul> <li>line_name:  the assembly name</li> <li>line_data: the mummer script parameters used with this version of PHG</li> <li>Ploidy:  1</li> <li>is_reference:  false</li> <li>isPhasedAcrossGenes: true</li> <li>isPhasesAcrossChromosomes: true</li> </ul> <p>The assembly entries in the gametes table will be defaulted to the following values:</p> <ul> <li>hapNumber: 0</li> <li>Confidence:  1</li> </ul> <p>The assembly entries in the methods table will be defaulted to the following values:</p> <ul> <li>method_type: DBLoadingUtils.MethodTYpe.ASSEMBLY_HAPLOTYPES</li> <li>name:  mummer4</li> <li>description:  the mummer script parameters used with this version of PHG ( same as genotypes:line_data</li> </ul> <p>An example Docker script that would call this shell script is below.  Note: you need to change the mounted user directories to match your own directory configuration.  The docker directories (right side of the count pair beginning with \"tempFileDir\" need to remain as shown here.  The file names, e.g. \"config.txt\" should match the names of your files.</p> <p>This example runs through all 10 chromosomes for a particular assembly.  In this example, the assembly files are broken into fasta files name ph207chr1.fasta, ph297chr2.fasta, etc.  The reference fastas are broken by chromosome and are named b73chr1.fasta, b73chr2.fasta, etc.  You need to change those names to match your fasta file names.</p> <pre><code>#!python\n\n#!/bin/bash\nchromList=(1 2 3 4 5 6 7 8 9 10)\n\nfor chrom in \"${chromList[@]}\"\ndo\n\necho \"Starting chrom ${chrom} \"\n\ndocker run --name phg_assembly_container_chr${chrom} --rm \\\n        -v /workdir/lcj34/mummer4_testing/output:/tempFileDir/outputDir/ \\\n        -v /workdir/lcj34/mummer4_testing/data:/tempFileDir/data/ \\\n        -v /workdir/lcj34/mummer4_testing/refFastas/:/tempFileDir/data/reference/ \\\n        -v /workdir/lcj34/mummer4_testing/assemblyFastas/:/tempFileDir/data/assemblyFasta/ \\\n        -v /workdir/lcj34/mummer4_testing/mummer_from_shellDocker/:/tempFileDir/outputDir/align/ \\\n        -t phgrepository_test:latest \\\n        /LoadAssemblyAnchors.sh configSQLite_docker.txt \\\n                b73chr${chrom}.fasta \\\n                ph207chr${chrom}.fasta \\\n                PH207_Assembly \\\n                ${chrom} \\\n                250\n\n\necho \"Finished chrom  ${chrom} \"\ndone\n\n</code></pre> <p>Note: use docker1 if running on cbsu.</p> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The \"-t\" directive indicates the Docker image of which this container will be an instance.  The last line tells the Docker container to run the LoadAssemblyAnchors.sh script which is found in the root directory.  The items following are the parameters to the LoadAssemblyAnchors.sh script.</p>"},{"location":"Pipeline_version1/LoadConsensusAnchorSequencesPlugin/","title":"LoadConsensusAnchorSequencesPlugin","text":"<p>The LoadConsensusAnchorSequencesPlugin takes an input file in fasta format and adds the sequences to the PHG data base.  The fasta file's idline must be of the format:</p> <pre><code>#!java\n&gt; refChrom:refStartPos:refEndPos;TaxaName_hapNumber:TaxaName2_hapNumber:... gvcfFile\n</code></pre> <p>A semi-colon separates the ref coordinates from the list of taxa.  The ref coordinates are used to identify the anchor to which the sequence belongs.  The taxa names indicate the taxa (haplotypes) whose sequences at the indicated anchor collapse to this consensus.  A space separates the taxaNames from the gvcf file, which is a file name that can be found in the directory indicated by the gvcf file dir parameter.</p> <p>NOTE:  This will run very slow unless the consensus files are concatenated into 1 per chromosome and put into a separate directory containing on the concatenated files for processing.</p> <p>Tables populated via this method are:</p> <ul> <li>genotypes</li> <li>gametes</li> <li>gamete_groups</li> <li>methods</li> <li>genome_interval_versions</li> <li>genome_intervals</li> <li>haplotypes</li> <li>gamete_haplotypes</li> </ul> <p>The parameters to this plugin are:</p> <ul> <li>-inputFile  Input fasta file, or directory containing fasta files, with consensus sequences. (Default=null) (REQUIRED)</li> <li>-version  Version name for this set of anchors as stored in genome_interval_versions table in db.  This is necessary to pull the genomge_interval ID for storing with each haplotype sequence in the haplotypes table. (Default=null) (REQUIRED) <li>-vcfDir  Directory that contains the gvcf files for the consensus, including trailing /. (Default=null) (REQUIRED) <li>-cmethod  Name of method used to collapse the anchors.  If it doesn't exist, will be added to DB methods table. (Default=null) (REQUIRED) <li>-method_details  Description of methods used to collapse the anchor sequences. (Default=null) (OPTIONAL)"},{"location":"Pipeline_version1/LoadGenomeIntervals/","title":"LoadGenomeIntervals","text":"<p>The LoadGenomeIntervals.sh script reads reference genome sequence, genome intervals (anchor bed file) information and populates the initial database tables with reference genome data.  The database can be either postgres or SQLite.  The type of database and the means to access it are provided in a config file.</p> <p>The db loading functionality is performed by a call to LoadGenomeIntervalsToPHGdbPlugin.  When LoadGenomeIntervals.sh is run as part of a Docker container script, the Docker script expects the following directory mount points:</p> <ul> <li>Mount localMachine:/pathToOutputDir/ to docker:/tempFileDir/outputDir/.  (If using sqlite, db mentioned in the config file should be in here.)</li> <li>Mount localMachine:/pathToReferenceDir/ to docker:/tempFileDir/data/reference (holds the reference fasta)</li> <li>Mount localMachine:/pathToDataDir/ to docker:/tempFileDir/data/ (config file and genome data file are here)</li> <li>Mount localMachine:/pathToAnswerDir/ to docker:/tempFileDir/answer/ (anchor bed file must live here)</li> </ul> <p>The parameters to this script are:</p> <ul> <li>Config File:  DB Config File containing properties host,user,password,DB and DBtype where DBtype is either sqlite or postgres.  The shell script assumes the configuration file lives in the mounted /tempFileDir/data/ directory.  If using sqlite, the sqlite database should be written to the directory mounted to /tempFileDir/outputDir/. A sample config file can be found here: Config File Wiki.  Only the first 5 entries in the config file deal with database connection.  The example config file contains additional parameters that are used in other steps of the pipeline.</li> <li>Reference Fasta File: The reference fasta file, which should exist in the folder mounted to the Docker /tempFileDir/data/reference directory.</li> <li>Anchor bed file:  A BED-formatted, tab-delimited file containing the chromosome, start-position, end-position for the genome intervals that will comprise the reference ranges.  This file is expected to live in the folder mounted to the Docker /tempFileDir/answer/ folder.</li> <li>Genome data file:  This is a tab-delimited file containing specific data related to the reference genome.  It must have the following columns:  Genotype Hapnumber Dataline Ploidy Reference GenePhased ChromPhased Confidence Method MethodDetails.  This file is expected to live in the folder mounted to the Docker /tempFileDir/data directory.  See this example genome data file: Sample Genome Data File</li> <li>create_new boolean:  either \"true\" or \"false\", indicating whether a new database instance should be created.  If the value is \"true\", a new db as identified in the config file will be created to hold the reference anchor data.  if \"false\", the data created will be added to an existing database specified by the config file.  The boolean is \"false\" and there is no database of the specified name, an error is thrown and processing stops.  The \"false\" value allows users to add multiple reference versions to a single database.  We recommend against this, and recommend this value be \"true\" to facilitate creation of a new database.  WARNING:  \"true\" will delete any existing database of the specified name before recreating it.</li> </ul> <p>The genome data file contents are described below:</p> <ul> <li>Genotype:  the name of the line as you want it to appear in the db genotypes table \"name\" column, e.g. \"B104\" or \"B104_haplotype_caller\".  These names must be unique for each line.  If your reference line is B73 and you also have a B73 haplotype you want to process from other data, then make the names distinct, e.g. B73Ref and B73.</li> <li>Hapnumber:  The 0-based chromosome number.  For inbreds, this is always 0.</li> <li>Dataline: Text description defining the line. This data will be stored in the \"description\" field of the genotypes table.</li> <li>Ploidy:  Number of chromosomes for this genome, the value to be stored in the \"ploidy\" field of the genome_intervals table.  Should be 1 for the reference.</li> <li>GenePhased: a boolean field indicating if the genes are phased.  Should be false for the reference.</li> <li>ChromPhased:  a boolean field indicating if the chromosomes are phased.  Should be false for the reference.</li> <li>Confidence:  a float field indicating confidence in phasing.  Generally 1 when no phasing.</li> <li>Method:  a mathod name by which this version of the reference data can be identified.</li> <li>MethodDetails:  Text description defining the method used to create the reference (or indicating the AGP version)</li> </ul> <p>An example Docker script to run the LoadGenomesIntervals.sh script is:</p> <pre><code>#!python\n\n# This script assumes the config.txt file lives in the directory mounted to /tempFileDir/data\n# It assumes the reference fasta (in this case, Zea_mays.AGPv4.dna.toplevelMtPtv3.fa) lives\n# in the directory mounted to /tempFileDir/data/reference.\n# It assumes the genome intervals file (in the example below, maizeRefAnchor_intervals_bed) lives\n# in the directory mounted to /tempFileDir/answer.\n# It assumes the genomeData file describing the reference (in the example below, B&amp;3Ref_load_data.txt) lives\n# in the directory mounted to /tempFileDir/data\n# It assumes your sqlite database lives in the directory mounted to /tempFileDir/outputDir/  This in only relevant when running an SQLite database.  This path shows up in the config file, parameter \"db\".\n\n# You must change \"/workdir/user/DockerTuningTests/...\" to match your own directory paths\ndocker run --name load_phg_container --rm \\\n        -v /workdir/user/DockerTuningTests/DockerOutput/:/tempFileDir/outputDir/ \\\n        -v /workdir/user/DockerTuningTests/InputFiles/Reference/:/tempFileDir/data/reference/ \\\n        -v /workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/:/tempFileDir/data/ \\\n        -v /workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/:/tempFileDir/answer/ \\\n        -t maizegenetics/phg:latest \\\n        /LoadGenomeIntervals.sh config.txt Zea_mays.AGPv4.dna.toplevelMtPtv3.fa maizeRefAnchor_intervals.bed B73Ref_load_data.txt true\n</code></pre> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The \"-t\" directive indicates the Docker image of which this container will be an instance.  </p> <p>The last line tells the Docker container to run the LoadGenomeIntervals.sh script which is found in the root directory.  The items following are the parameters to the LoadGenomeIntervals.sh script.</p>"},{"location":"Pipeline_version1/LoadGenomeIntervalsToDBPlugin/","title":"LoadGenomeIntervalsToDBPlugin","text":"<p>This Plugin takes a reference genome fasta, a genome data file and an anchors file.  The anchors file contains columns indicating chrom, start position, end position.  The plugin code grabs sequence from the reference genome fasta based on the coordinates from the anchors file and loads the genome interval sequence to the specified PHG database.  The genome data file contains genome specific details used when adding to the genotypes, gametes and method tables.</p> <p>When finished loading the anchor (conserved) regions, inter-anchor regions are identified and loaded.  For the reference genome, the inter-anchor regions are simply all sequence in regions not included in an anchor region.</p> <p>Tables populated via this method are:</p> <ul> <li>genotypes</li> <li>gametes</li> <li>gamete_groups</li> <li>methods</li> <li>genome_interval_versions</li> <li>genome_intervals</li> <li>haplotypes</li> <li>gamete_haplotypes</li> </ul> <p>The parameters to this plugin are:</p> <ul> <li>-ref   Reference Genome File for aligning against. (Default is null) (REQUIRED) <li>-anchors  CSV file containing chrom, anchor start positions, anchor end position, gene start, gene end. The positions are physical positions (1-based, inclusive/inclusive).  (Default is null) (REQUIRED) <li>-genomeData  A tab-delimited file containing genome specific data with columns: Genotype Hapnumber Dataline Ploidy Reference GenePhased ChromPhased Confidence Method MethodDetails RefVersion.  (Default is null) (REQUIRED) <p>The genome data file contents are described below:</p> <ul> <li>Genotype:  the name of the line as you want it to appear in the db genotypes table \"name\" column, e.g. \"B104\" or \"B104_haplotype_caller\"</li> <li>Hapnumber:  The 0-based chromosome number.  For inbreds, this is always 0.</li> <li>Dataline: Text description defining the line. This data will be stored in the \"description\" field of the genotypes table.</li> <li>Ploidy:  Number of chromosomes for this genome, the value to be stored in the \"ploidy\" field of the genome_intervals table.  Should be 1 for the reference.</li> <li>Reference:  a boolean field indicating if this genome is the reference (should be true for this plugin)</li> <li>GenePhased: a boolean field indicating if the genes are phased.  Should be false for the reference.</li> <li>ChromPhased:  a boolean field indicating if the chromosomes are phased.  Should be false for the reference.</li> <li>Confidence:  a float field indicating confidence in phasing.  Generally 1 when no phasing.</li> <li>Method:  a mathod name by which this version of the reference data can be identified.</li> <li>MethodDetails:  Text description defining the method used to create the reference (or indicating the AGP version)</li> <li>RefVersion: a version name used to identify the version of this reference data.</li> </ul> <p>This plugin expects a data base connection as input.  This can be obtained by chaining the GetDBConnectionPlugin with the LoadGenomeIntervalsToPHGdbPlugin when called from the command line with the TASSEL run_pipeline.pl script.</p> <p>An example of chaining these plugins is below:</p> <pre><code>#!python\n\n/tassel-5-standalone/run_pipeline.pl -Xmx10G -debug -GetDBConnectionPlugin -config dbConfigFile -create true  -endPlugin -LoadGenomeIntervalsToPHGdbPlugin -ref reference -anchors rangesFile -genomeData genomeDataFile  -outputDir &lt;pathToOutPut&gt; -endPlugin\n</code></pre>"},{"location":"Pipeline_version1/LoadHapSequencesToDBPlugin/","title":"LoadHapSequencesToDBPlugin","text":"<p>This plugin takes fasta files created from FilterGVCFPlugin or assembly processing and loads PHG data base.  The fasta id lines contain information identifying the reference genome interval of which the sequence represents, along with the name of the corresponding gvcf file.  The gvcf file is used to extract and stored variant contexts for the sequence.  The fasta id line must be formatted as below:</p> <pre><code>#!java\n&gt;refChrom:refstart:refEnd gvcfFileName\n\n</code></pre> <p>When running the PHG pipeline from the Docker image with the supplied scripts, this plugin is called directly from the FilterGVCFPlugin (for haplotypes) or the Minimap2Plugin (for assembly sequences).  The input files are created by these plugins.</p> <p>The parameters to the LoadHapSequenceToDBPlugin  method are:</p> <ul> <li>-fasta  Fasta file containing haplotype sequences.  (Default = null) (REQUIRED) <li>-genomeData  Path to tab-delimited file containing genome speciic data with header line:\\nGenotype Hapnumber Dataline Ploidy Reference GenePhased ChromPhased Confidence Method MethodDetails RefVersion. (Default = null) (REQUIRED) <li>-gvcf  Directry containing GVCF file used to create the haplotype fasta file.  Directory path only including trailing /. This directory name is pre-pended to the gvcf file name that appears in the fasta id line for each sequence. <p>The genome data file contents are described below:</p> <ul> <li>Genotype:  the nmae of the line as you want it to appear in the db genotypes table \"name\" column, e.g. \"B104\" or \"B104_haplotype_caller\"</li> <li>Hapnumber:  The 0-based chromosome number.  For inbreds, this is always 0.</li> <li>Dataline: Text description defining the line. This data will be stored in the \"description\" field of the genotypes table.</li> <li>Ploidy:  Number of chromosomes for this genome, the value to be stored in the \"ploidy\" field of the genome_intervals table.  Should be 1 for the reference.</li> <li>Reference:  a boolean field indicating if this genome is the reference (should be true for this plugin)</li> <li>GenePhased: a boolean field indicating if the genes are phased.  Should be false for the reference.</li> <li>ChromPhased:  a boolean field indicating if the chromosomes are phased.  Should be false for the reference.</li> <li>Confidence:  a float field indicating confidence in phasing.  Generally 1 when no phasing.</li> <li>Method:  a mathod name by which this version of the reference data can be identified.</li> <li>MethodDetails:  Text description defining the method used to create the reference (or indicating the AGP version)</li> <li>RefVersion: a version name used to identify the version of this reference data.</li> </ul>"},{"location":"Pipeline_version1/MergeGVCFPlugin/","title":"MergeGVCFPlugin","text":"<p>The plugin MergeGVCFPlugin pulls the variant context records stored in the database for all of the taxon at each reference range stored against a specified method id.  The calls and depth information are merged into a single VariantContext record to later be read into the FindHaplotypeClustersPlugin.</p> <p>The plugin requires a PHG Haplotype Graph as well as a GenomeSequence object.  MergeGVCFPlugin is currently invoked from the RunHapCollapsePlugin, which passes these objects via the DataSet parameter.</p> <p>The parameters to this plugin are:</p> <ul> <li>-outputDir   The direcotry where the output VCF files will be stored. (Default=null) (OBSOLETE) <li>-dbConfig &lt; DB Config File&gt; Config file containing properties host,user,password,DB and DBtype where DBtype is either sqlite or postgres.  </li> <li>-hapMethod  Name of the method used to call the haplotypes.  This must match the method name as stored in the database methods table.  It is needed to pull the correct sub-graph."},{"location":"Pipeline_version1/Minimap2NoDocker/","title":"Minimap2NoDocker","text":"<p>Steps to run Minimap2BasedPipeline without needing a docker.</p>"},{"location":"Pipeline_version1/Minimap2NoDocker/#prerequisites","title":"Prerequisites:","text":"<ul> <li>A Database filled with haplotypes</li> <li>The most recent tassel-5-standalone from bitbucket</li> <li>The most recent phg.jar built from practicalhaplotypegraph project.  Put this in tassel-5-standalone/lib/</li> <li>The most recent version of minimap2 downloaded and installed.</li> </ul>"},{"location":"Pipeline_version1/Minimap2NoDocker/#step-1-pull-out-the-fasta-files","title":"Step 1: pull out the fasta files","text":"<pre><code>#!bash\n\ntime ./tassel-5-standalone/run_pipeline.pl -Xmx100g -debug -HaplotypeGraphBuilderPlugin -configFile [CONFIG_FILE_NAME] -methods [HAPLOTYPE_METHOD_NAME] -includeVariantContexts false -endPlugin -WriteFastaFromGraphPlugin -outputFile [OUTPUT_FASTA_NAME] -endPlugin &gt; [LOG_FILE]\n</code></pre>"},{"location":"Pipeline_version1/Minimap2NoDocker/#parameters","title":"Parameters:","text":"<ul> <li>CONFIG_FILE_NAME - The name of the config file you are wanting to run through the pipeline.  This is used to figure out what DB to access.</li> <li>HAPLOTYPE_METHOD_NAME - The name of the Haplotype Methods used when populating the DB.  </li> <li>OUTPUT_FASTA_NAME - The name of the output fasta file you are creating by running this step.</li> <li>LOG_FILE - file to write the logs to.</li> </ul>"},{"location":"Pipeline_version1/Minimap2NoDocker/#step-2-index-the-pangenome-fasta-using-minimap2-30-minutes-for-20-assemblies-with-75k-anchors","title":"Step 2: Index the pangenome fasta using minimap2 (~30 minutes for 20 assemblies with 75k anchors)","text":"<pre><code>#!bash\ntime minimap2/minimap2 -d [OUTPUT_MINIMAP_INDEX] -k 21 -w 11 -I 90G [PANGENOME_FASTA_FILE] \n\n</code></pre>"},{"location":"Pipeline_version1/Minimap2NoDocker/#parameters_1","title":"Parameters:","text":"<ul> <li>OUTPUT_MINIMAP_INDEX - output file name for the minimap2 index file.  This should have a .mmi extension</li> <li>-k minimap2 parameter for kmer-size.  We Tested 21 to be the best for maize.</li> <li>-w minimap2 parameter for window size.  We tested 11 to be the best for maize.</li> <li>-I - minimap2 parameter may need to be dropped if running into memory issues.  This should be above the number of basepairs in the entire pangenome.  If it is too low it will batch the index and cause the mapping to be incorrect.</li> <li>PANGENOME_FASTA_FILE - the output from the last step. </li> </ul>"},{"location":"Pipeline_version1/Minimap2NoDocker/#step-3-run-fastqdirtomappingpluginthis-one-is-the-slowest-step-highly-dependent-on-read-coverage","title":"Step 3: Run FastqDirToMappingPlugin(This one is the slowest step.  Highly dependent on read coverage.)","text":"<pre><code>#!bash\n\ntime ./tassel-5-standalone/run_pipeline.pl -debug -Xmx200G -HaplotypeGraphBuilderPlugin -configFile [CONFIG_FILE] -methods [HAPLOTYPE_METHOD_NAME] -includeVariantContexts false -includeSequences false  -endPlugin -FastqDirToMappingPlugin -minimap2IndexFile [MINIMAP_INDEX_FILE] -fastqDir [READ_DIRECTORY]/ -mappingFileDir [OUTPUT_MAPPING_DIR]/ -paired [PAIRED] -endPlugin &gt; [LOG_FILE]\n</code></pre>"},{"location":"Pipeline_version1/Minimap2NoDocker/#parameters_2","title":"Parameters:","text":"<ul> <li>CONFIG_FILE_NAME - The name of the config file you are wanting to run through the pipeline.  This is used to figure out what DB to access.</li> <li>HAPLOTYPE_METHOD_NAME - The name of the Haplotype Methods used when populating the DB.  </li> <li>MINIMAP_INDEX_FILE - Minimap2 index file made in previous step</li> <li>READ_DIRECTORY - directory holding all the reads you wish to align.  Make sure to have a \"/\" at the end.</li> <li>OUTPUT_MAPPING_DIR - directory where all the mapping files are stored. Make sure to have a \"/\" at the end. This will go away likely by fall 2019.</li> <li>PAIRED - either \"true\" or \"false\". If true, it will attempt to pair up reads based on file name and use the pair to align.  For this to work correctly the file names need to be in the form B73_1.fq and B73_2.fq or even B73_R1.fq and B73_R2.fq.  Basically everything but the last token separated by \"_\" must be the same for the pair.  If false it will map each file independently and make a mapping file for each.  False is generally used for GBS.</li> </ul>"},{"location":"Pipeline_version1/Minimap2NoDocker/#step-4-find-path","title":"Step 4: Find Path","text":"<pre><code>#!bash\n\ntime ./tassel-5-standalone/run_pipeline.pl -debug -Xmx240g -HaplotypeGraphBuilderPlugin -configFile [CONFIG_FILE_NAME] -methods [HAPLOTYPE_METHOD_NAME],[REF_RANGE_METHOD_NAME] -includeVariantContexts false -includeSequences false -endPlugin -HapCountBestPathToTextPlugin -configFile [CONFIG_FILE_NAME] -inclusionFileDir [MAPPING_DIR]/ -outputDir [OUTPUT_PATH_DIR]/ -hapCountMethod [HAP_COUNT_METHOD_NAME] -pMethod [PATH_METHOD_NAME] -endPlugin &gt; [LOG_FILE] 2&gt;&amp;\n</code></pre>"},{"location":"Pipeline_version1/Minimap2NoDocker/#parameters_3","title":"Parameters:","text":"<ul> <li>CONFIG_FILE_NAME - The name of the config file you are wanting to run through the pipeline.  This is used to figure out what DB to access.</li> <li>HAPLOTYPE_METHOD_NAME - The name of the Haplotype Methods used when populating the DB.  </li> <li>REF_RANGE_METHOD_NAME - Name of the reference ranges you wish to run on.  It is highly suggested you use \"refRegionGroup\" as this is the genic regions.  This approach does not work very well with the intergenes.</li> <li>MAPPING_DIR - Name of the mapping directory output from the previous step. OUTPUT_PATH_DIR - Output directory for the path output files.</li> <li>HAP_COUNT_METHOD_NAME - name of the hap counts stored in the DB.  Just put a dummy value here for now.  It will be used in the future.</li> <li>PATH_METHOD_NAME - name of the path method to be stored in the DB.  Just put a dummy value here for now. It will be used in the future.</li> </ul> <p>Depending on what you are attempting to do, you may need to tweak the config file parameters.  In particular:</p> <ul> <li>minReads - this will control the amount of imputation the HMM will do.  If it is set to 0 the HMM will attempt to find the path through all reference ranges even if we did not have any reads supporting those haplotypes.  If set to 1, we require at least one Read hitting haplotypes in the reference range.</li> <li>splitTaxa - should probably set this to true when using assemblies.  Assemblies tend to skip over some reference regions which can bottle neck the HMM.  By doing splitTaxa=true, this is solved.</li> </ul> <p>Here are the parameters we typically use:</p> <pre><code>#!\nmaxNodesPerRange=30\nminTaxaPerRange=1\nminReads=0\nmaxReadsPerKB=1000\nminTransitionProb=0.001\nprobReadMappedCorrectly=0.99\nemissionMethod=allCounts\nsplitTaxa=true\nuseBF=false\n\n</code></pre>"},{"location":"Pipeline_version1/Minimap2NoDocker/#step-5-create-vcfoptional","title":"Step 5: Create VCF(Optional)","text":"<pre><code>#!bash\ntime ./tassel-5-standalone/run_pipeline.pl -debug -Xmx100g -HaplotypeGraphBuilderPlugin -configFile [CONFIG_FILE_NAME] -methods [HAPLOTYPE_METHOD_NAME],[REF_RANGE_METHOD_NAME] -includeVariantContexts true -endPlugin -ImportHaplotypePathFilePlugin -inputFileDirectory [PATH_DIR_NAME]/ -endPlugin -PathsToVCFPlugin -positionVCF [KNOWN_SITES_VCF] -ref [REFERENCE_FILE] -outputFile [OUTPUT_VCF_FILE] -endPlugin &gt; [LOG_FILE] 2&gt;&amp;1\n\n</code></pre>"},{"location":"Pipeline_version1/Minimap2NoDocker/#parameters_4","title":"Parameters:","text":"<ul> <li>CONFIG_FILE_NAME - The name of the config file you are wanting to run through the pipeline.  This is used to figure out what DB to access.</li> <li>HAPLOTYPE_METHOD_NAME - The name of the Haplotype Methods used when populating the DB.  </li> <li>REF_RANGE_METHOD_NAME - Name of the reference ranges you wish to run on.  It is highly suggested you use \"refRegionGroup\" as this is the genic regions.  This approach does not work very well with the intergenes.</li> <li>PATH_DIR_NAME - Directory holding the paths. This was filled in the FindPaths step.</li> <li>KNOWN_SITES_VCF - Optional parameter to force certain sites to be exported.  If you have a known SNP set which you need all the sites to compare, pass that SNP set in as a vcf file.  If not, remove the -positionVCF flag as well.</li> <li>REFERENCE_FILE - reference fasta used when creating the graph.</li> <li>OUTPUT_VCF_FILE - name of the output vcf file.</li> </ul>"},{"location":"Pipeline_version1/PHG_Liquibase/","title":"PHG Liquibase","text":"<p>PHG uses the liquibase data migration tool to update user databases.  This tool works on both sqlite and postgresql databases.</p> <p>Liquibase is run via the maizegenetics/phg_liquibase Docker.  When creating a new database from LoadGenomeIntervals.sh, the db is current and there is no need to run the liquibase Docker. The data migration docker should be run whenever a user is pulling a new PHG image to run with an existing database.  In this case, both the maizegenetics/phg and the maizegenetics/phg_liquibase docker images should be pulled to the user system.  Users may pull the \"latest\" version or any preceding version, as long as both the phg and phg_liquibase dockers are the same version.</p> <p>A sample shell script for running phg_liquibase docker on your db is:</p> <pre><code>#!python\n\n docker run --name liquibase_update --rm \\\n  -v /workdir/lcj34/liquibase/data/:/tempFileDir/data/ \\\n  -v /workdir/lcj34/liquibase/output/:/tempFileDir/outputDir \\\n  -t maizegenetics/phg_liquibase \\\n  /liquibase/RunLiquibaseUpdates.sh configFile.txt\n</code></pre> <p>In the above, mount the directory containing your configuration file to :/tempFileDir/data/ In the above, mount the directory where you would like output written to :/tempFileDir/outputDir In the above, \"configFile.txt\" should be replaced with the name of your configuration file that lives in the directory you mounted to /tempFileDir/data.</p> <p>If you are running with an SQLite database, the database file must live in the output directory you have mounted to /tempFileDir/outputDir.</p> <p>If you are running with a postgreSQL database, the liquibase docker will access the database using information from the configuration file. Please see the \"Rules of Thumb for Connecting to a PHG PostgreSQL docker image\" section of the \"PHG Postgres Docker\" section for details on specifying the postgreSQL host and port parameters.</p> <p>The liquibase migration tool will write 3 files to the specified output directory:   liquibase_error.log   liquibase_output.log   run_yes.txt or run_no.txt</p> <p>If a \"run_no.txt\" file is created, it means your database is too old to be updated.  In this scenario, you must start the database fresh.  The run_no.txt file will contain a message indicating what schema is missing from the database, which should give an indication of the database versioning.</p> <p>If the run_yes.txt file is present, then liquibase attempted to perform the database migrations.  Check the liquibase_error.log and liquibase_output.log files in your mounted output directory to verify the migrations ran without error.</p> <p>The LiquibaseUpdatePlugin called from the liquibase docker will run the liquibase \"update\" command.  The software identifies database changes that have already been run against the database and skips them. </p> <p>While there are not always database changes each time there is a PHG version change, a new liquibase docker image will be created each time a PHG image is created.  This is to remove confusion as to which liquibase docker matches a particular PHG version.  Always run the same version maizegenetics/phg and maizegenetics/phg_liquibase dockers together.</p>"},{"location":"Pipeline_version1/PHG_PostgresDocker/","title":"PHG PostgresDocker","text":""},{"location":"Pipeline_version1/PHG_PostgresDocker/#why-postgresql","title":"Why PostgreSQL?","text":"<p>PostgreSQL is an industry standard object-relational database system providing reliability and data integrity.  It provides good performance, supports many concurrent users, and allows for user extensions.</p>"},{"location":"Pipeline_version1/PHG_PostgresDocker/#why-postgresql-in-a-docker","title":"Why PostgreSQL in a docker?","text":"<p>Providing a containerized version of PostgreSQL removes the need for the user to intall and maintain their own postgres image.  The database file created and manipulated in the postgres docker can be persisted to a shared volume for processing by multiple applications.</p> <p>Having postgres in a docker also ensures the PHG pipeline interacts with a known version of PostgreSQL.</p>"},{"location":"Pipeline_version1/PHG_PostgresDocker/#creating-the-phg-postgresql-docker-image","title":"Creating the PHG PostgreSQL docker image","text":"<p>The postgres docker image may be downloaded from docker hub via the following command.  Replace  with the postgres version you wish to run. <pre><code>#!python\n\n &gt; docker pull postgres:&lt;version&gt;\n</code></pre> <p>If running on a Cornell CBSU machine, replace \"docker\" with \"docker1\".</p> <p>If phg_postgres docker has already been installed and you wish to upgrade, stop and remove any containers associated with the image.  Then remove the existing image using the docker \"rmi\" command.</p>"},{"location":"Pipeline_version1/PHG_PostgresDocker/#connecting-to-a-phg-postgresql-docker-image","title":"Connecting to a PHG PostgreSQL docker image","text":"<p>Connecting to a PHG postgres docker image requires two files:</p> <ol> <li>a parameters file</li> <li>a container commands file.</li> </ol> <p>The container commands file reads a user supplied parameters file, then creates a container against the postgres docker image using sourced parameters.  The container is created in detached mode, which allows for \"docker exec\" commands to be run against it.</p> <p>Here is an example of a container commands file (container_cmdsPostgres.sh):</p> <pre><code>#!python\n\n#!/usr/bin/env bash\nset -u\nset -e\nset -x\n\n# read in parameters\nsource $1\n\n# run the docker in detach mode\ndocker run -i --detach --name $DOCKER_CONTAINER_NAME -e POSTGRES_PASSWORD=$POSTGRES_PASSWORD -v $POSTGRES_DATA_MOUNT:/var/lib/postgresql/data -p $DOCKER_DB_PORT:5432 $DOCKER_IMAGE_NAME\n\n# start the docker\ndocker start $DOCKER_CONTAINER_NAME\n\n# PHG scripts may now be run using the postgres db with the host ip address and DOCKER_DB_PORT assigned above\n</code></pre> <p>A sample parameters file, with a description of each parameter, is below.  Among other parameters, the defining of   a value for the postgres password helps ensure the security of your data.  Note where each parameter below is used in the container commands file above:</p> <pre><code>#!python\n\n# This file provides parameters for the container_cmdsPostgresDocker.sh file.\n# Edit this file to reflect your configuration\n\n# Port that is mapped to the docker postgres 5432 port.  If running on a Cornell CBSU machine,\n# the mapped port must be between 8009-8019.  Those are the only exposed ports.\nDOCKER_DB_PORT=\"5433\"\n\n# Name for the docker container running against the postgres image\nCONTAINER_NAME=\"phg_postgres_container\"\n\n# When running on Cornell CBSU machines, a prefix is appended to any docker container created.\n# This prefix consists of the user id followed by \"__biohpc_\", e.g. \"lcj34__biohpc-\"\n# This prefix can remain blank if not running on Cornell CBSU machine\nCONTAINER_PREFIX=\"\"\nDOCKER_CONTAINER_NAME=${CONTAINER_PREFIX}${CONTAINER_NAME}\n\n# Name of the docker image against which your container will be run\nDOCKER_IMAGE_NAME=\"postgres:11\"\n\n# password for postgres db user.  It is recommended this be changed from the default \"postgres\"\n# The  POSTGRES_PASSWORD must remain consistent when attaching to the same persisted database\n# This password should appear in the config file used with other PHG scripts for loading data\n# to the phg database.\nPOSTGRES_PASSWORD=\"yoursecretpassword\"\n\n# Place to mount the data.  Data will be mapped to internal postgres /var/lib/postgresql/data directory\n# This data will persist to the volume specified in POSTGRES_DATA_MOUNT.\nPOSTGRES_DATA_MOUNT=\"/Users/lcj34/postgres_docker/test_postgres_data/data\"\n</code></pre> <p>To setup the postgres docker execute the container commands files (container_cmdsPostgres.sh above) from the command line.  The db is now ready for connections.  To connect to this postgres instance from the PHG pipeline follow the rules below for defining the host/port in the config file.</p>"},{"location":"Pipeline_version1/PHG_PostgresDocker/#rules-of-thumb-for-connecting-to-the-postgresql-docker-image-for-phg","title":"Rules of Thumb for Connecting to the PostgreSQL docker image for PHG","text":"<p>In all cases you must ensure the user name/password you used when creating the database initially remains the same across all containers that connect to the PHG postgres docker image, and in all config files used by scripts that access the database.  </p>"},{"location":"Pipeline_version1/PHG_PostgresDocker/#connecting-from-an-external-machine","title":"Connecting from an external machine:","text":"<ul> <li>your config file must have the internet address of the host where the docker container resides</li> <li>the port must be the mapped port (value of $DOCKER_DB_PORT) from the \"docker run --detach ... command\" (see -p = $DOCKER_DB_PORT:5432 in the container and parameters files above)</li> <li>example:  If the remote machine's ip address is 128.32.5.269 with a mapped port of 5433, the config file would have a host line of \"host=128.32.5.269:5433\"</li> </ul>"},{"location":"Pipeline_version1/PHG_PostgresDocker/#connecting-from-a-phg-docker-script-on-the-same-host","title":"Connecting from a PHG docker script on the same host:","text":"<ul> <li>your config file should have host ip addr of 0.0.0.0: <li>if your machine is using a Docker virtual network, the port would instead be the regular postgres port of 5432</li> <li>CAVEAT:  This does not always work.  If you are having problems, try using the external machine address, ie host=128.32.5.269:5433 (replacing the IP address and mapped port with your machine's address and mapped port).</li>"},{"location":"Pipeline_version1/PHG_PostgresDocker/#connecting-from-a-non-docker-program-on-the-same-host","title":"Connecting from a non-docker program on the same host:","text":"<ul> <li>your config file should have host ip addr of 0.0.0.0:"},{"location":"Pipeline_version1/PHG_PostgresDocker/#connecting-docker-docker-on-a-cornell-cbsu-machine-both-dockers-on-the-same-machine","title":"Connecting docker-docker on a Cornell CBSU machine (both dockers on the same machine):","text":"<ul> <li>your config file should have host ip addr of 0.0.0.0:5432</li> <li>If the above doesn't work, use the full external address with mapped port.  This takes you out and back in the network which shouldn't be required, but it works if nothing else does.</li> <li>When a postgres docker run has finished creating and loading a new db, \"root\" will own all the directories.  In order to run subsequent scripts that mount to this data directory you must \"claim\" and change permissions on your postgres data directory as below:</li> </ul> <pre><code>#!python\n\n&gt; docker1 claim\n&gt; chmod -R 777 data  \n</code></pre>"},{"location":"Pipeline_version1/ParallelAssemblyAnchorsLoad/","title":"ParallelAssemblyAnchorsLoad","text":"<p>The ParallelAssemblyAnchorsLoad.sh script takes 3 parameters:  a configuration file containing database connection information, a keyfile containing specifics on reference and assembly fastas files to be aligned and turned into haplotypes for loading into a PHG database, and an output directory.  </p> <p>This script calls the AssemblyHaplotypesMultiThreadPlugin plugin, which takes reference and assembly fastas, split by chromosome, and aligns them with mummer4 scripts.  It performs the same functions as are executed when the LoadAssemblyAnchors.sh script is called, but will run the mummer4 alignment programs in parallel.  The number of processes run concurrently is dependent on a user supplied thread count.</p> <p>When determining the number of threads to run, please keep in mind the alignment of each ref-assembly fasta pair will take RAM commensurate with the size of the genomes.  Consider the RAM limitations of your machine as well as the number of cores available.</p> <p>The required input to this script is a configuration file with database connection information, a keyfile with fasta information and an output directory.  Both the configuration file and the keyfile must live in a local machine directory that is mounted to the docker's /tempFileDir/data directory.</p> <p>When ParallelAssemblyAnchorsLoad.sh is called as part of a Docker script, the Docker script expects these mount points:</p> <ul> <li>Mount localMachine:/pathToDataDir/ to docker:/tempFileDir/data/</li> <li>Mount localMachine:/pathToOutputDir/ to docker:/tempFileDir/outputDir/</li> </ul> <p>The script will create a subdirectory called \"align\" under the mounted output directory where mummer4 alignment files will be stored.</p> <p>Description of required parameters:</p> <ul> <li>keyfile: The keyfile must be a tab-delimited file with 1 row for each per-chromosome assembly fasta you wish to align and load to the database.  The required columns for the keyfile are RefDir, RefFasta,  AssemblyDir, AssemblyFasta, AssemblyDBName and Chromosome.  The oolumn descriptions are below.<ul> <li>RefDir: full path to the directory containing the reference fasta file.</li> <li>RefFasta: name of the reference fasta file to be aligned to the assembly fasta file in this row.  This should be a fasta file at the chromosome level.</li> <li>AssemblyDir: full path to the directory containing the assembly fasta files.</li> <li>AssemblyFasta:  name of the assembly fasta file to be aligned to the reference fasta file in this row.  This should be a fasta file at the chromosome level.</li> <li>AssemblyDBName: the name for this assembly to add as the \"line_name\" to the database's genotype table.</li> <li>Chromosome: the chromosome name that is being processed.  This name must match a reference chromosome that was loaded with the LoadGenomeIntervals.sh script.</li> </ul> </li> <li>configFile:  DB Config File containing properties host,user,password,DB and DBtype where DBtype is either \"sqlite\" or \"postgres\". This file is expected to live in the folder mounted to Docker tempFileDir/data/. This file may also contain values for optional parameters.</li> </ul> <p>The optional parameters that may be defined in the configuration file are  mummer4Path, clusterSize, entryPoint, minInversionLen, loadDB, assemblyMethod and numThreads. If you wish to set a value other than the default for these parameters, they should be included in the configuration file in the format shown below (with your own values replacing the default values shown here):</p> <ul> <li>AssemblyHaplotypesMultiThreadPlugin.mummer4Path=/mummer/bin</li> <li>AssemblyHaplotypesMultiThreadPlugin.clusterSize=250</li> <li>AssemblyHaplotypesMultiThreadPlugin.minInversionLen=7500</li> <li>AssemblyHaplotypesMultiThreadPlugin.loadDB=true</li> <li>AssemblyHaplotypesMultiThreadPlugin.assemblyMethod=mummer4</li> <li>AssemblyHaplotypesMultiThreadPlugin.numThreads=3</li> <li>AssemblyHaplotypesMultiThreadPlugin.entryPoint=all</li> </ul> <p>The \"entryPoint\" parameter is provided for users who wish To speed up processing by running the mummer4 alignments on multiple machines. You may do this, then gather all the mummer files to a single machine.  If you do this, set the loadDB flag to FALSE, run all your alignments, then gather them to a single machine for loading into the database.</p> <p>Description of optional parameters:</p> <ul> <li>mummer4Path: If the mummer4 executables exist in a path other than /mummer/bin, then provide that path via this parameter.  If you are running in the PHG docker, the path will be /mummer/bin and this parameter is not necessary.</li> <li>clusterSize:  This is a parameter to mummer4's nucmer alignment program.  It sets the minimum length for a cluster of matches.  We have found the value of 250 provides good coverage with acceptable speed.If you wish a different value, set this parameter.</li> <li>minInversionLen: Minimum length of inversion for it to be kept as part of the alignment. Default is 7500</li> <li>Boolean: true means load haplotypes to db, false means do not populate the database.  Useful if the user is running mummer4 alignments on multiple machines and plans later to put all mummer4 files on 1 machine to load to a single database.  Defaults to true</li> <li>numThreads: Number of threads used to process assembly chromosomes.  The code will subtract 2 from this number to get the number of worker threads.  It leaves 1 thread for IO to the DB and 1 thread for the Operating System.</li> <li>entryPoint:  This parameter indicates at which point assembly processing should begin.  If a step other than \"all\" is chosen, files normally created from the previous steps must be present in a sub-directory named \"align\" in your output directory.  They must be named using the format shown below for the software to recognize them:<ul> <li>all: Run the entire pipeline.  The software creates all necessary files.</li> <li>deltafilter:  Assumes the nucmer aligning step has already been performed. Processing starts with mummer4 delta-filter step includes running mummer4 show-coords on both the original alignment and the filtered alignment as well as mummer4 show-snps.  The delta file must be available in a sub-directory named \"align\" in the directory mounted to docker's /tempFileDir/outputDir/.  The file name must be in the format:<ul> <li>ref___c.delta <li>ex:  ref_W22_1_c250.delta</li> <li>refilter: Assumes delta-filter and show-coords for both the delta and the delta-filter steps have been run. This step runs additional filtering to process overlapping alignments, remove embedded alignments and find a longest-path solution.Your output/align directory  must have the files for deltafilter plus:<ul> <li>ref___c.delta_filtered <li>ref___c.coords_orig <li>ref___c.coords_filtered haplotypes: This step takes the mummer4 output files and from them creates haplotype sequence fro the database.  If starting from this step, you must have the files from above plus: <li>ref___c.coords_filteredPlus_noEmbedded <li>ref___c.coords_final <li>ref___c.snps_prefiltered <li>ref___c.snps_final <p>An example Docker script that would call this shell script is below.  Note: you need to change the mounted user directories to match your own directory configuration.  The docker directories (right side of the count pair beginning with \"tempFileDir\") need to remain as shown here.  The file names, e.g. \"config.txt\" should match the names of your files.  And the \"config.txt\" and \"parallelKeyFile_B204.txt\" must live in the directory mounted to /tempFileDir/data/.</p> <pre><code>echo \"Starting taxa B104\"\n\ndocker1 run --name phg_assembly_container_B104 --rm \\\n        -v /workdir/${USER}/phg_assemblyParallel_test/DockerOutput/:/tempFileDir/outputDir/ \\\n        -v /workdir/${USER}/phg_assemblyParallel_test/DataFolders/:/tempFileDir/data/ \\\n        -v /workdir/${USER}/phg_nam_assemblies/B73/:/tempFileDir/referenceFastas/ \\\n        -v /workdir/${USER}/phg_nam_assemblies/B104/:/tempFileDir/assemblyFastas/ \\\n        -t phgrepository_test:latest \\\n        /ParallelAssemblyAnchorsLoad.sh config.txt \\\n                parallelKeyFile_B104.txt\n\n\necho \"Finished taxa 104 \"\n\n\n</code></pre> <p>An example key file the has information for a single assembly genome is shown below.  Note all fields are tab-delimited and because this is run in a docker, the reference and assembly directories are docker paths.</p> <pre><code>RefDir  RefFasta        AssemblyDir     AssemblyFasta   AssemblyDBName  Chromosome\n/tempFileDir/referenceFastas/   B73chr1.fa      /tempFileDir/assemblyFastas/    B104chr1.fa     B104_Assembly   1\n/tempFileDir/referenceFastas/   B73chr2.fa      /tempFileDir/assemblyFastas/    B104chr2.fa     B104_Assembly   2\n/tempFileDir/referenceFastas/   B73chr3.fa      /tempFileDir/assemblyFastas/    B104chr3.fa     B104_Assembly   3\n\n</code></pre> <p>An example key file that has per-chromosome pastas for multiple species is shown in the next example.  Note that the assembly chromosome fasta files must exist in the same directory.</p> <pre><code>RefDir  RefFasta        AssemblyDir     AssemblyFasta   AssemblyDBName  Chromosome\n/tempFileDir/referenceFastas/   B73chr1.fa      /tempFileDir/assemblyFastas/    B104chr1.fa     B104_Assembly   1\n/tempFileDir/referenceFastas/   B73chr2.fa      /tempFileDir/assemblyFastas/    B104chr2.fa     B104_Assembly   2\n/tempFileDir/referenceFastas/   B73chr3.fa      /tempFileDir/assemblyFastas/    B104chr3.fa     B104_Assembly   3\n/tempFileDir/referenceFastas/   B73chr1.fa      /tempFileDir/assemblyFastas/    B97chr1.fa      B97_Assembly    1\n/tempFileDir/referenceFastas/   B73chr2.fa      /tempFileDir/assemblyFastas/    B97chr2.fa      B97_Assembly    2\n/tempFileDir/referenceFastas/   B73chr3.fa      /tempFileDir/assemblyFastas/    B97chr3.fa      B97_Assembly    3\n</code></pre> <p>The ParalleleAssemblyAnchorsLoad.sh script uses the parameter cache option to load the configuration file into memory.  This is the -configParameters  directive given to the tassel run_pipeline.pl script used in the Docker container. if you run this plugin manually at the command line, you will need to include this yourself with a command similar to the one below: <pre><code>/tassel-5-standalone/run_pipeline.pl -Xmx100G -debug -configParameters myConfigFile.txt -AssemblyHaplotypesMultiThreadPlugin  -keyFile myKeyFile.txt  -outputDir /workdir/user\u0010/phg_assemblies/DockerOutput/ -endPlugin\n</code></pre>"},{"location":"Pipeline_version1/PathsToVCFPlugin/","title":"PathsToVCFPlugin","text":"<p>The PathsToVCFPlugin is used to create VCF files from haplotype paths.  The user may specific specific reference ranges to be included in the exported file.  This plugin would likely be chained with the HaplotypeGraphBuilderPlugin and the ImportHaplotypePathFilePlugin.  It takes input from both and uses this to create the requested VCF files.</p> <p>The parameters to this plugin are:</p> <ul> <li>-outputFile  The file name for storing the VCF data. (Default=null) (REQUIRED) <li>-RefRangeFileVCF  Reference Range file used to further subset the paths for only specified regions of the genome.  It not provided, all references ranges are used.  (Default=null) (OPTIONAL)"},{"location":"Pipeline_version1/RunHapCollapsePlugin/","title":"RunHapCollapsePlugin","text":"<p>The RunHapCollapsePlugin method was written to consolidate calls to the collapse pipeline.  Invocation of this plugin results in the following functionality performed:</p> <p>Loop through each reference range in the graph:</p> <ul> <li>Extract the HaplotypeNodes with the VariantContexts (we assume that the user has not pulled these yet for memory reasons)</li> <li>Merge all the VariantContext records for each base pair (bp) of the reference range</li> <li>Export a GenotypeTable containing each bp</li> <li>Run HapCollapse Finding algorithm on this genotype table</li> </ul> <p>For each VCF file exported, upload them to the DB as a consensus</p> <p>The parameters to this plugin are:</p> <ul> <li>-mergedOutputDir  Directory to store the ouput VCFs from the merge process. (Default=null)(REQUIRED) <li>-ref  Input reference fasta to be passed to FindHaplotypeClusters for extracting reference sequence. <li>-haplotypeMethod  Haplotype calling method used to retrieve the correct haplotypes from the graph. (Default=null) (REQUIRED) <li>-dbConfigFile  File holding the database configuration information. <li>-consensusVCFOutputDir  Directory in which to store the output VCFs from the consensus process. (Default=null) (REQUIRED) <li>-consensusFastaOutputDir  Directory in which to store the output fastas from the consensus process. (Default=null) (REQUIRED) <li>-refVersion  Version name of the reference intervals.  Must match a version name that is stored in the genome_interval_versions DB table. (Default=null) (REQUIRED) <li>-collapseMethod  Name to be stored in the DB for this method used to collapse the haplotypes into consensus sequences. (Default=null) (REQUIRED) <li>collapseMethodDetails  Details describing the collapse method. (Default=null) (REQUIRED) <p>This method calls FindHaplotypeClustersPlugin/MergeGVCFPlugin in parallel for each reference range to create the haplotype clusters.  The resulting consensus sequences are loaded to the haplotypes table via a call to LoadConsensusAnchorSequencesPlugin.</p>"},{"location":"Pipeline_version1/SAMToMappingPlugin/","title":"SAMToMappingPlugin","text":""},{"location":"Pipeline_version1/SAMToMappingPlugin/#overview","title":"Overview","text":"<p>This plugin allows you to create ReadMappings from an existing SAM/BAM file and upload them to the DB.  This uses a KeyFile similarly to FastqToMappingPlugin, but will skip the Minimap2 Alignment step.</p>"},{"location":"Pipeline_version1/SAMToMappingPlugin/#steps","title":"Steps:","text":"<ol> <li>Loop through each SAM file in the key file</li> <li>Apply a set of filters to the mappings</li> <li>Push the readMapping to the DB</li> <li>Generate a Path KeyFile for use with Path finding.</li> </ol>"},{"location":"Pipeline_version1/SAMToMappingPlugin/#example-run-command","title":"Example Run Command","text":"<pre><code>#!bash\n\ntime /tassel-5-standalone/run_pipeline.pl -debug -Xmx100G -configParameters [CONFIG_FILE] -HaplotypeGraphBuilderPlugin -configFile [CONFIG_FILE] -methods [HAPLOTYPE_METHOD] -includeVariantContexts false -includeSequences false  -endPlugin -SAMToMappingPlugin -endPlugin\n\n</code></pre>"},{"location":"Pipeline_version1/SAMToMappingPlugin/#config-parameters","title":"Config Parameters","text":"<pre><code>#!bash\n\nSAMToMappingPlugin.keyFile = Name of the Keyfile to process.  Must have columns cultivar, flowcell_lane, filename, and PlateID.\n\nSAMToMappingPlugin.samDir = Name of the SAM/BAM dir to process.\n\nSAMToMappingPlugin.maxRefRangeErr = Maximum allowed error when choosing best reference range to count.  Error is computed 1 - (mostHitRefCount/totalHits)\n\nSAMToMappingPlugin.lowMemMode = true(default) or false.  If true, will run in a memory efficient manner with minimal performance hit.  If the reads are not ordered in the SAM file, you must use false and the whole file must fit in RAM\n\nSAMToMappingPlugin.methodName = Name of the ReadMappings to be stored in the DB.\n\nSAMToMappingPlugin.methodDescription = Description of the method\n\nSAMToMappingPlugin.debugDir = (Optional)Directory on the file system that will write the ReadMapping files to disk.\n\nSAMToMappingPlugin.outputSecondaryStats = True or false(default).  If true this will out put some additional mapping statistics which can be used to debug.  The files will be written to the current working directory.\n\n</code></pre> <p>All of these parameters can also be set on the command line as well.  A Path keyfile will be exported which can then be used to find paths.</p>"},{"location":"Pipeline_version1/help_biostars/","title":"How to ask questions about the PHG","text":"<p>Step 0: Create a userID on Biostars (skip to step 1 if you already have a user ID)</p> <ul> <li> <p>Click on the Login tab near the top of the home page or on this link: https://www.biostars.org/accounts/signup/</p> </li> <li> <p>On the login page, sign in with either your Google credentials or use your own custom ID and password.</p> </li> </ul> <p>Step 1: Ask your question </p> <ul> <li> <p>Log in and find the new posts through the \u201cNew Post\u201d button on the top right-hand side of the screen</p> </li> <li> <p>On the New Post page, provide the following:</p> <ul> <li> <p>Post Title: A clearly defined title for your question</p> </li> <li> <p>Post Type: Question</p> </li> <li> <p>Post Tags: Since Biostars is a larger online bioinformatics community, this will ensure your question is found and answered by the Buckler Lab staff. Please use one of the following tags:</p> <p>phg - questions relating to the practical haplotype graph (PHG) framework</p> <p>rphg - questions relating to the R frontend for the PHG</p> </li> <li> <p>Enter your post below: your clearly stated, unambiguous question in proper formatting.</p> </li> </ul> </li> <li> <p>Click submit </p> </li> </ul> <p>Step 2: Wait for potential remedies and follow up questions </p> <p>To follow the phg or rphg tags or receive email notifications for PHG or rPHG questions:</p> <ul> <li> <p>Click on your userid on the main biostars page</p> </li> <li> <p>Click edit profile. You can enter tags to show up on myTags or email there</p> </li> </ul>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/","title":"THIS SCRIPT IS DEPRECATED","text":"<p>Use the Groovy Scripts instead.</p> <p>The CreateHaplotypes.sh script will create haplotype sequences for each anchor and load them into the database.  The script can either start with raw WGS fastq files and a reference fasta or can operate on pre-aligned BAM files created with BWA mem.  Either fastq files or BAM files must be passed into the Docker container using volume mounting.  If neither are supplied, no haplotypes will be created.</p>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#mount-points-for-use-with-the-phg-docker","title":"Mount Points For use with the PHG Docker","text":"<ul> <li>Mount localMachine:/pathToInputs/refFolder to docker://tempFileDir/data/reference/ (required)</li> <li>Mount localMaching:/pathToInputs/config.txt to docker://tempFileDir/data/config.txt (required)</li> <li>Mount localMachine:/pathToInputs/fastq to docker://tempFileDir/data/fastq/ (optional if WGS sequence needs to be aligned, if not declared, BAM files must be input)</li> <li>Mount localMachine:/pathToInputs/bam to docker://tempFileDir/data/bam/externalBams/ (optional to skip alignment, if not declared, Fastq files must be input.  BAMs must be in format TAXON_otherInformation.bam. We split on the \"_\" and extract the first token as the taxon/sample name)</li> <li>Mount localMachine:/pathToGVCFOutput/ to docker://tempFileDir/data/outputs/gvcfs/ (optional to get the output GVCF file created by HaplotypeCaller)</li> <li>Mount localMachine:/pathToGVCFFilteredOutput/ to docker://tempFileDir/data/filteredGvcfsAndFasta/Filtered/ (optional to get out the filtered gvcf, useful to see if your filtering parameters are too stringent)</li> </ul>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#createhaplotypessh-parameters","title":"CreateHaplotypes.sh parameters","text":"<ul> <li>configFile: Path to config file containing DB parameters host, user, password, DB, type.  Used for making the database connection.  Type must be wither \"sqlite\" or \"postgres\" to identify db type for connection.  This file will also contain filtering criteria for both the BAM file filtering and the GVCF filtering.  A sample config file can be found here:Config File Wiki</li> <li>currentTaxon :  Name of taxon currently being processed.</li> <li>operationMode:  paired or single (for aligning via BWA)</li> <li>methodName: Name used to store this run of haplotypes into the database.  Later on it makes it easier to pull a specific set of haplotypes from the database when making the graph.</li> <li>fastqList: Optional list of fastq file names separated by commas to be aligned with BWA.  If paired, the length of this list must be even.  If this is blank, the script will assume you have BAM files to import.</li> </ul>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#pipeline-steps","title":"Pipeline Steps","text":"<ol> <li>Check /tempFileDir/data/reference/ for bwa and .fai indices.  If they do not exist create new ones</li> <li>Pull the Anchor Bed file from the Database</li> <li>If fastq files are stored in /tempFileDir/data/fastq/ and they are specified as an argument, execute BWA MEM in either single or paired end mode(also specified by command line argument) 3a. Dedup the resulting BAM files</li> <li>Apply a MapQ filter to remove any reads which map to multiple places on the genome(Default: MapQ&gt;48)</li> <li>Create GVCF files. If sentieon_license is specified in config.txt, the script will run Sentieon.  Else run GATK 4s implementation.</li> <li>Apply a GVCF filter to the gvcf based on parameters in config.txt.  This is generally used to filter out abnormally high depth regions(repeats) or heterozygous regions. This step uses the FilterGVCFPlugin.</li> <li>Load the final GVCF file to the DB.  This process will split the GVCF by anchor regions and will store both the VariantContext records(think VCF records) and the fasta sequence in the db.  This step uses the LoadHapSequencesToDBPlugin.</li> </ol>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#relevant-config-file-parameters-must-be-in-the-form-paramvalue","title":"Relevant Config File Parameters. Must be in the form param=value","text":"<p>A sample config file can be found here:Config File Wiki</p>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#db-parameters","title":"DB Parameters","text":"<ul> <li>host - host name of the db</li> <li>user - db user name</li> <li>password - password for db</li> <li>DB - name of the db</li> <li>DBtype - sqlite or postgres</li> </ul>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#java-arguments","title":"Java Arguments","text":"<ul> <li>Xmx - max heap space for the pipeline.  This is similar to Java's -Xmx argument.</li> </ul>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#bam-filtering-arguments","title":"BAM Filtering Arguments","text":"<ul> <li>mapQ - minimum mapQ filtering default it 48</li> </ul>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#gvcf-filtering-arguments","title":"GVCF Filtering Arguments","text":""},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#these-are-optional-and-can-be-used-in-combination-with-one-another","title":"These are optional and can be used in combination with one another","text":"<ul> <li>DP_poisson_min - between 0 and 1. If used the pipeline will create a poisson distribution of the depths and filter out any depths below this poisson bound.</li> <li>DP_poisson_max - between 0 and 1. If used the pipeline will create a poisson distribution of the depths and filter out any depths above this poisson bound.</li> <li>DP_min - minimum Depth threshold.  Cannot be used in conjunction with DP_poisson_min or DP_poisson_max.</li> <li>DP_max - maximum Depth threshold.  Cannot be used in conjunction with DP_poisson_min or DP_poisson_max.</li> <li>GQ_min - minimum Genotype Quality(GQ annotation in VCF) threshold.  </li> <li>GQ_max - maximum Genotype Quality(GQ annotation in VCF) threshold.</li> <li>QUAL_min - minimum base pair quality(QUAL column in VCF) threshold. WARNING This parameter is heavily dependent on Depth.  We recommend to not use this option.</li> <li>QUAL_max - maximum base pair quality(QUAL column in VCF) threshold. WARNING This parameter is heavily dependent on Depth.  We recommend to not use this option.</li> <li>filterHets - true or false.  This will remove any VCF records which suggest a het based on Allele Depth(AD).  For simplicity it will remove any records where two or more alleles have greater than 0 depth.  In other words if the AD values look like this: 0,10,0 or 1,0,0 the record will stay.  If the values are 1,2,0 or 1,1,0 or 1,10,0 it will be filtered out.</li> <li>exclusionString - this will override all previous parameters with a bcftools valid exclusion string.  This option is not recommended unless you have a very complicated or specific filter which needs to be applied to the gvcf.</li> </ul>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#other-optional-parameters","title":"Other Optional Parameters","text":"<ul> <li>numThreads - number of threads which are available for HaplotypeCaller to run. (Default: 10).</li> <li>extendedWindowSize - When filtering BAM files, we will keep some flanking regions around the anchor in case HaplotypeCaller requires some additional information for the active window.  Default is 1000 bp on both sides of the anchor.</li> <li>sentieon_license - location of Sentieon Licensing server.  If this is not defined, the Pipeline will use GATK's implementation of HaplotypeCaller.  If it is, it will attempt to use Sentieon.</li> </ul>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#example-run-scripts","title":"Example Run Scripts","text":"<p>An example shell script that runs a Docker container in a loop to process a list of taxon is here: The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The \"-t\" directive indicates the Docker image of which this container will be an instance.  The last line tells the Docker container to run the CreateHaplotypes.sh script which is found in the root directory.  The items following are the parameters to the CreateHaplotypes.sh script.</p> <p>If you are running on Cornell CBSU you need to change docker to docker1.</p>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#this-script-is-running-with-just-bams","title":"This script is running with just BAMs","text":"<pre><code>#!bash\n\n#Set these properties\nTAXON=B73\nREF_DIR=/workdir/user/DockerTuningTests/InputFiles/Reference/\nBAM_DIR=/workdir/user/DockerTuningTests/InputFiles/WGSBams/${TAXON}/\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nCONFIG_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/config.txt\n#Optional debug parameters\nGVCF_OUTPUT_DIR=/workdir/user/DockerTuningTests/DockerOutput/gvcfOut/${TAXON}/\nGVCF_FILTERED_DIR=/workdir/user/DockerTuningTests/DockerOutput/gvcfOutFilter/${TAXON}/\n\ndocker run --name cbsu_phg_container_${TAXON} --rm \\\n        -v ${REF_DIR}:/tempFileDir/data/reference/ \\\n        -v ${BAM_DIR}:/tempFileDir/data/bam/${TAXON}/DedupBAMs/ \\\n        -v ${DB}:/tempFileDir/outputDir/phgTestMaizeDB.db \\\n        -v ${GVCF_OUTPUT_DIR}:/tempFileDir/data/outputs/gvcfs/ \\\n        -v ${GVCF_FILTERED_DIR}:/tempFileDir/data/filteredGvcfsAndFasta/Filtered/ \\\n        -v ${CONFIG_FILE}:/tempFileDir/data/config.txt \\\n        -t phgrepository_test:latest \\\n        /CreateHaplotypes.sh /tempFileDir/data/config.txt \\\n                          ${TAXON} \\\n                          single \\\n                          GATK_PIPELINE\n</code></pre>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#this-script-is-running-with-fastq-files-in-single-end-mode","title":"This script is running with fastq files in Single End Mode","text":"<pre><code>#!bash\n\n#Set these properties\nTAXON=B73\nREF_DIR=/workdir/user/DockerTuningTests/InputFiles/Reference/\nFASTQ_DIR=/workdir/user/DockerTuningTests/InputFiles/Fastas/${TAXON}/\nFASTQ_LIST=B73_batch1.fastq,B73_batch2.fastq,B73_batch3.fastq\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nCONFIG_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/config.txt\n#Optional debug parameters\nGVCF_OUTPUT_DIR=/workdir/user/DockerTuningTests/DockerOutput/gvcfOut/${TAXON}/\nGVCF_FILTERED_DIR=/workdir/user/DockerTuningTests/DockerOutput/gvcfOutFilter/${TAXON}/\n\ndocker run --name cbsu_phg_container_${TAXON} --rm \\\n        -v ${REF_DIR}:/tempFileDir/data/reference/ \\\n        -v ${FASTQ_DIR}:/tempFileDir/data/fastq/ \\\n        -v ${DB}:/tempFileDir/outputDir/phgTestMaizeDB.db \\\n        -v ${GVCF_OUTPUT_DIR}:/tempFileDir/data/outputs/gvcfs/ \\\n        -v ${GVCF_FILTERED_DIR}:/tempFileDir/data/filteredGvcfsAndFasta/Filtered/ \\\n        -v ${CONFIG_FILE}:/tempFileDir/data/config.txt \\\n        -t phgrepository_test:latest \\\n        /CreateHaplotypes.sh /tempFileDir/data/config.txt \\\n                          ${TAXON} \\\n                          single \\\n                          GATK_PIPELINE \\\n                          $FASTQ_LIST\n</code></pre>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#this-script-is-running-with-fastq-files-in-paired-end-mode","title":"This script is running with fastq files in Paired End Mode","text":"<pre><code>#!bash\n\n#Set these properties\nTAXON=B73\nREF_DIR=/workdir/user/DockerTuningTests/InputFiles/Reference/\nFASTQ_DIR=/workdir/user/DockerTuningTests/InputFiles/Fastas/${TAXON}/\nFASTQ_LIST=B73_batch1_1.fastq,B73_batch1_2.fastq,B73_batch2_1.fastq,B73_batch2_2.fastq,B73_batch3_1.fastq,B73_batch3_2.fastq\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nCONFIG_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/config.txt\n#Optional debug parameters\nGVCF_OUTPUT_DIR=/workdir/user/DockerTuningTests/DockerOutput/gvcfOut/${TAXON}/\nGVCF_FILTERED_DIR=/workdir/user/DockerTuningTests/DockerOutput/gvcfOutFilter/${TAXON}/\n\ndocker run --name cbsu_phg_container_${TAXON} --rm \\\n        -v ${REF_DIR}:/tempFileDir/data/reference/ \\\n        -v ${FASTQ_DIR}:/tempFileDir/data/fastq/ \\\n        -v ${DB}:/tempFileDir/outputDir/phgTestMaizeDB.db \\\n        -v ${GVCF_OUTPUT_DIR}:/tempFileDir/data/outputs/gvcfs/ \\\n        -v ${GVCF_FILTERED_DIR}:/tempFileDir/data/filteredGvcfsAndFasta/Filtered/ \\\n        -v ${CONFIG_FILE}:/tempFileDir/data/config.txt \\\n        -t phgrepository_test:latest \\\n        /CreateHaplotypes.sh /tempFileDir/data/config.txt \\\n                          ${TAXON} \\\n                          paired \\\n                          GATK_PIPELINE \\\n                          $FASTQ_LIST\n</code></pre>"},{"location":"Pipeline_version1/DeprecatedScripts/CreateHaplotypes_sh/#this-script-is-running-running-over-a-list-of-bam-files-for-multiple-taxon","title":"This script is running running over a list of BAM files for multiple taxon","text":"<pre><code>#!bash\n\ntaxonList=(B73 A632 B14 B37 B97 CO125 LH74 Ms71 Oh43 OH7B W22)\n\nREF_DIR=/workdir/user/DockerTuningTests/InputFiles/Reference/\nBAM_DIR=/workdir/user/DockerTuningTests/InputFiles/WGSBams/\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nCONFIG_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/config.txt\n#Optional debug parameters\nGVCF_OUTPUT_DIR=/workdir/user/DockerTuningTests/DockerOutput/gvcfOut/\nGVCF_FILTERED_DIR=/workdir/user/DockerTuningTests/DockerOutput/gvcfOutFilter/\n\nfor TAXON in \"${taxonList[@]}\"\ndo\nmkdir -p /workdir/user/DockerTuningTests/DockerOutput/gvcfOut/${TAXON}/\nmkdir -p /workdir/user/DockerTuningTests/DockerOutput/gvcfOutFilter/${TAXON}/\n\n\ndocker run --name cbsu_phg_container_${TAXON} --rm \\\n        -v ${REF_DIR}:/tempFileDir/data/reference/ \\\n        -v ${BAM_DIR}/${TAXON}/:/tempFileDir/data/bam/${TAXON}/DedupBAMs/ \\\n        -v ${DB}:/tempFileDir/outputDir/phgTestMaizeDB.db \\\n        -v ${GVCF_OUTPUT_DIR}/{TAXON}/:/tempFileDir/data/outputs/gvcfs/ \\\n        -v ${GVCF_FILTERED_DIR}/${TAXON}/:/tempFileDir/data/filteredGvcfsAndFasta/Filtered/ \\\n        -v ${CONFIG_FILE}:/tempFileDir/data/config.txt \\\n        -t phgrepository_test:latest \\\n        /CreateHaplotypes.sh /tempFileDir/data/config.txt \\\n                          ${TAXON} \\\n                          single \\\n                          GATK_PIPELINE\n\ndone\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step0_conda/","title":"Use Conda to run PHG","text":"<p>This option is not yet implemented</p>"},{"location":"UserInstructions/CreatePHG_step0_conda/#quick-start","title":"Quick Start","text":""},{"location":"UserInstructions/CreatePHG_step0_conda/#process-details","title":"Process Details","text":""},{"location":"UserInstructions/CreatePHG_step0_conda/#kitchen-sink","title":"Kitchen Sink","text":""},{"location":"UserInstructions/CreatePHG_step0_conda/#troubleshooting","title":"Troubleshooting","text":"<p>Return to Step 0 pipeline</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step0_docker/","title":"Use Docker to run PHG","text":""},{"location":"UserInstructions/CreatePHG_step0_docker/#quick-start","title":"Quick Start","text":"<ol> <li>Download and install Docker following instructions on the docker website.</li> <li>Pull the PHG docker images from Docker Hub.  It is recommended you pull an image based on a specific tag. Knowing which specific PHG image was used can aid with debugging, and make your pipeline more reproducible.  Change the \"0.0.40\" tag below to match the PHG tag you wish to pull.</li> </ol> <pre><code>docker pull maizegenetics/phg:0.0.40\n</code></pre> <p>If on a Cornell CBSU machine, use docker1 as below:</p> <pre><code>docker1 pull maizegenetics/phg:0.0.40\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step0_docker/#details","title":"Details","text":"<p>The PHG is deployed as a Docker image. Docker is an open source platform that wraps applications into an image to be deployed on a host system. Docker containers are instances of a Docker image that may run simultaneously on the same host. See the Docker documentation here for details.</p> <p>The PHG Docker image is built on the GATK image from docker hub. To this image several genomic analysis tools are added, including: VIM, BWA, minimap2, jbwa, htslib, bcftools, SAMTools and TASSEL-5 with the PHG jar. In addition, scripts for loading and processing the PHG are added.</p> <p>To pull the PHG image from docker hub, run the following commands (replace  with the tagged version you wish to run): <pre><code>docker pull maizegenetics/phg:&lt;version tag&gt;\n</code></pre> <p>If on a Cornell CBSU machine, use docker1 as below (Replace  with the tagged version you wish to run): <pre><code>docker1 pull maizegenetics/phg:&lt;version tag&gt;\n</code></pre> <p>Access to PHG databases in all cases is through a config file that specifies the host:port, user, password, db name, and the database type (sqlite or postgres). An example of the relevant portions of an SQLite config file is below. NOTE: The user and password parameters are not used for SQLite. The sqlite db file must include a path that is relative to its location within the PHG docker scripts. This will be discussed when each script is discussed.</p> <pre><code>host=localHost\nuser=sqlite\npassword=sqlite\nDB=/tempFileDir/outputDir/phgTestDB_mapq48.db\nDBtype=sqlite\n</code></pre> <p>An example of a postgres config file is below:</p> <pre><code>host=172.17.0.2:5432\\n\nuser=postgres\\n\npassword=postgres_pwd\\n\nDB=phgdb\\n\nDBtype=postgres\n</code></pre> <p>On CBSU NOTE: If unable to pull a docker image from docker hub on a Cornell CBSU machine, check if you are on an enhanced security machine. cbsumm01, 03, 11 and others are. You can see this if you go to basic biohpc reservations pages, and look at the machines you're able to reserve. They will have a note in the description about enhanced security.</p>"},{"location":"UserInstructions/CreatePHG_step0_docker/#kitchen-sink","title":"Kitchen Sink","text":""},{"location":"UserInstructions/CreatePHG_step0_docker/#why-postgresql","title":"Why PostgreSQL?","text":"<p>PostgreSQL is an industry standard object-relational database system providing reliability and data integrity.  It provides good performance, supports many concurrent users, and allows for user extensions.</p>"},{"location":"UserInstructions/CreatePHG_step0_docker/#why-postgresql-in-a-docker","title":"Why PostgreSQL in a docker?","text":"<p>If you do not have access to Postgres on your server, you can download a containerized version of postgres. To do this execute the following comand, replacing  with the version of your choice. <pre><code>\ndocker pull postgres:&lt;version&gt;\n</code></pre> <p>Providing a containerized version of PostgreSQL removes the need for the user to intall and maintain their own postgres image.  The database file created and manipulated in the postgres docker can be persisted to a shared volume for processing by multiple applications.</p>"},{"location":"UserInstructions/CreatePHG_step0_docker/#creating-the-phg-postgresql-docker-image","title":"Creating the PHG PostgreSQL docker image","text":"<p>The postgres docker image may be downloaded from docker hub via the following command.  Replace  with the postgres version you wish to run. <pre><code>\n  docker pull postgres:&lt;version&gt;\n</code></pre> <p>If running on a Cornell CBSU machine, replace \"docker\" with \"docker1\".</p> <p>If the postgres docker has already been installed and you wish to upgrade, stop and remove any containers associated with the image.  Then remove the existing image using the docker \"rmi\" command.</p>"},{"location":"UserInstructions/CreatePHG_step0_docker/#connecting-to-a-postgresql-docker-image","title":"Connecting to a PostgreSQL docker image","text":"<p>Connecting to a PHG postgres docker image requires two files:</p> <ol> <li>a parameters file</li> <li>a container commands file.</li> </ol> <p>The container commands file reads a user supplied parameters file, then creates a container against the postgres docker image using sourced parameters.  The container is created in detached mode, which allows for \"docker exec\" commands to be run against it.</p> <p>Here is an example of a container commands file (container_cmdsPostgres.sh):</p> <pre><code>#!python\n\n#!/usr/bin/env bash\nset -u\nset -e\nset -x\n\n# read in parameters\nsource $1\n\n# run the docker in detach mode\ndocker run -i --detach --name $DOCKER_CONTAINER_NAME -e POSTGRES_PASSWORD=$POSTGRES_PASSWORD -v $POSTGRES_DATA_MOUNT:/var/lib/postgresql/data -p $DOCKER_DB_PORT:5432 $DOCKER_IMAGE_NAME\n\n# start the docker\ndocker start $DOCKER_CONTAINER_NAME\n\n# PHG scripts may now be run using the postgres db with the host ip address and DOCKER_DB_PORT assigned above\n</code></pre> <p>A sample parameters file, with a description of each parameter, is below.  Among other parameters, the defining of a value for the postgres password helps ensure the security of your data.  Note where each parameter below is used in the container commands file above:</p> <pre><code>#!python\n\n# This file provides parameters for the container_cmdsPostgresDocker.sh file.\n# Edit this file to reflect your configuration\n\n# Port that is mapped to the docker postgres 5432 port.  If running on a Cornell CBSU machine,\n# the mapped port must be between 8009-8019.  Those are the only exposed ports.\nDOCKER_DB_PORT=\"5433\"\n\n# Name for the docker container running against the postgres image\nCONTAINER_NAME=\"phg_postgres_container\"\n\n# When running on Cornell CBSU machines, a prefix is appended to any docker container created.\n# This prefix consists of the user id followed by \"__biohpc_\", e.g. \"lcj34__biohpc-\"\n# This prefix can remain blank if not running on Cornell CBSU machine\nCONTAINER_PREFIX=\"\"\nDOCKER_CONTAINER_NAME=${CONTAINER_PREFIX}${CONTAINER_NAME}\n\n# Name of the docker image against which your container will be run\nDOCKER_IMAGE_NAME=\"postgres:11\"\n\n# password for postgres db user.  It is recommended this be changed from the default \"postgres\"\n# The  POSTGRES_PASSWORD must remain consistent when attaching to the same persisted database\n# This password should appear in the config file used with other PHG scripts for loading data\n# to the phg database.\nPOSTGRES_PASSWORD=\"yoursecretpassword\"\n\n# Place to mount the data.  Data will be mapped to internal postgres /var/lib/postgresql/data directory\n# This data will persist to the volume specified in POSTGRES_DATA_MOUNT.\nPOSTGRES_DATA_MOUNT=\"/Users/lcj34/postgres_docker/test_postgres_data/data\"\n</code></pre> <p>To set up the postgres docker, execute the container commands files (container_cmdsPostgres.sh above) from the command line. The db is now ready for connections. Follow the rules below for defining the host/port in the config file if you would like to connect to this postgres instance from the PHG pipeline.</p>"},{"location":"UserInstructions/CreatePHG_step0_docker/#rules-of-thumb-for-connecting-to-the-postgresql-docker-image-for-phg","title":"Rules of Thumb for Connecting to the PostgreSQL docker image for PHG","text":"<p>In all cases you must ensure the user name/password you used when creating the database initially remains the same across all containers that connect to the PHG postgres docker image, and in all config files used by scripts that access the database.  </p>"},{"location":"UserInstructions/CreatePHG_step0_docker/#connecting-from-an-external-machine","title":"Connecting from an external machine:","text":"<ul> <li>your config file must have the internet address of the host where the docker container resides</li> <li>the port must be the mapped port (value of $DOCKER_DB_PORT) from the \"docker run --detach ... command\" (see -p = $DOCKER_DB_PORT:5432 in the container and parameters files above)</li> <li>example:  If the remote machine's ip address is 128.32.5.269 with a mapped port of 5433, the config file would have a host line of \"host=128.32.5.269:5433\"</li> </ul>"},{"location":"UserInstructions/CreatePHG_step0_docker/#connecting-from-a-phg-docker-script-on-the-same-host","title":"Connecting from a PHG docker script on the same host:","text":"<ul> <li>your config file should have host ip addr of 0.0.0.0: <li>if your machine is using a Docker virtual network, the port would instead be the regular postgres port of 5432</li> <li>CAVEAT:  This does not always work.  If you are having problems, try using the external machine address, ie host=128.32.5.269:5433 (replacing the IP address and mapped port with your machine's address and mapped port).</li>"},{"location":"UserInstructions/CreatePHG_step0_docker/#connecting-from-a-non-docker-program-on-the-same-host","title":"Connecting from a non-docker program on the same host:","text":"<ul> <li>your config file should have host ip addr of 0.0.0.0:"},{"location":"UserInstructions/CreatePHG_step0_docker/#connecting-docker-docker-on-a-cornell-cbsu-machine-both-dockers-on-the-same-machine","title":"Connecting docker-docker on a Cornell CBSU machine (both dockers on the same machine):","text":"<ul> <li>your config file should have host ip addr of 0.0.0.0:5432</li> <li>If the above doesn't work, use the full external address with mapped port.  This takes you out and back in the network which shouldn't be required, but it works if nothing else does.</li> <li>When a postgres docker run has finished creating and loading a new db, \"root\" will own all the directories.  In order to run subsequent scripts that mount to this data directory you must \"claim\" and change permissions on your postgres data directory as below:</li> </ul> <pre><code>#!python\n\n&gt; docker1 claim\n&gt; chmod -R 777 data  \n</code></pre>"},{"location":"UserInstructions/CreatePHG_step0_docker/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Docker can be tricky to get running because it requires root access. If you are having problems, double-check that docker has permissions to access root on your machine.</li> <li>On CBSU NOTE: If unable to pull a docker image from docker hub on a Cornell CBSU machine, check if you are on an enhanced security machine. cbsumm01, 03, 11 and others are. You can see this if you go to basic biohpc reservations pages, and look at the machines you're able to reserve. They will have a note in the description about enhanced security.</li> </ol> <p>Return to Step 0 pipeline</p> <p>Return to PHG Version 1 instructions</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step0_main/","title":"CreatePHG step0 main","text":""},{"location":"UserInstructions/CreatePHG_step0_main/#use-the-flow-chart-and-links-below-to-download-the-phg-code","title":"Use the flow chart and links below to download the PHG code","text":"<p>A. Run PHG with Docker</p> <p>B. Run PHG with Singularity</p> <p>C. Run PHG with Conda</p> <p>D. Test the Docker by creating and running an example or </p> <p>E. Proceed to Step 1</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step0_singularity/","title":"Use Singularity to run PHG","text":""},{"location":"UserInstructions/CreatePHG_step0_singularity/#important","title":"IMPORTANT","text":"<p>Because Singularity loads the user $HOME folder, it also picks up any environment variables the user has set.   When running any scripts that load liquibase, this can be a problem if the JAVA_HOME variable is set and it is different from what is in the container.  To avoid errors it is best to unset this environment variable.  To do this on a linux machine, please type the following before running your Singularity containers:</p> <p>unset JAVA_HOME</p> <p>Also, a note on Singularity binding.  Singularity allows the user to bind using 2 different syntaxes.  The first specifies just a source destination with the assumption there exists a folder with the same name in the singularity container.  The second option is to bind using the source:destination syntax.  This syntax explicitly specifies the container-specific destination folder where the source folder will be mounted.  When no colon is present Singularity assumes the source and destination are the same.  </p> <p>While the no-colon method works for plugins, it doesn't play well with Liquibase, which expects folders to be relative to /Libuibase.  Because of this, we recommend users always bind with the source:destination method as seen in the example below:</p> <p> singularity exec -B ${WORKING_DIR}:/phg </p>"},{"location":"UserInstructions/CreatePHG_step0_singularity/#quick-start","title":"Quick Start","text":"<ol> <li>Follow the Quick Start installation steps to install Singularity on your machine.</li> <li>Run the following commands to create a singularity image based on the PHG docker images. The command below downloads the version 0.0.22 phg image from docker hub and wraps it in a singularity image named \"phg_22.simg\".  You should replace the phg version tag with the tag you wish to download.</li> </ol> <pre><code>singularity build phg_22.simg docker://maizegenetics/phg:0.0.22\n\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step0_singularity/#details","title":"Details","text":"<p>You can use Singularity containers to run docker containers without installing Docker. The benefit of using Singularity is that it does not require root access on your machine.</p> <p>Singularity and Docker work well together, and you can run the PHG Docker container through singularity without needing Docker installed. </p> <p>Singularity communicates with Docker Hub to pull a Docker image into the Singularity Docker Registry.</p> <p>As with Docker, Singularity containers give users the ability to work in a uniform and consistent computing environment which facilitates reproducible science.</p> <p>Here are example Singularity PHG commands that work on a TACC machine.  Note the \"export PATH=${PATH}:/sbin\" command may not be needed.  At the time of this writing, there was a singularity issue on TACC machines that required this step.</p> <p>Also note: \"/scratch1/07005/lynnjo\"  should be replaced by your own scratch directory path, which can be found by doing a \"cds\" followed by \"pwd\".</p> <p>For long singularity runs (e.g. loading haplotypes to the db, or imputing values) a slurm job should be executed.  See TACC instructions for further details.</p> <pre><code>#!python\n\n&gt; cds\n&gt; mkdir -p phg_scratch/phg_run1\n&gt; export PATH=${PATH}:/sbin\n&gt; module load tacc-singularity\n&gt; cd phg_scratch\n&gt; singularity build phg_22.simg docker://maizegenetics/phg:0.0.22\n&gt; \n&gt; singularity exec -B /scratch1/07005/lynnjo/phg_scratch/phg_run1/:/phg/ /scratch1/07005/lynnjo/phg_scratch/phg_22.simg /tassel-5-standalone/run_pipeline.pl -debug -Xmx1G -MakeDefaultDirectoryPlugin -workingDir /phg/ -endPlugin\n\n\n\n\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step0_singularity/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>If you are having trouble downloading the Docker image with Singularity, check the Singularity troubleshooting guide.</li> </ol> <p>Return to Step 0 pipeline</p> <p>Return to PHG version 1 instructions</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step1-2_main/","title":"Create and Populate a PHG database with haplotypes","text":"<p>Before creating the db, you need to setup the PHG directory structure.  We recommend using the default directory structure, which can be created as described in step 1A below.  When the default directory is created, two of the files created are config.txt and load_genome_data.txt. Those files need to have values added or modified by the user before other steps can be run.</p> <p>A sample config file can be viewed here</p> <p>There are two steps to building and populating a PHG database. The first step creates the PHG database and  populates it with the reference genome. The second step populates the database with additional haplotypes that have been aligned to the reference genome reference intervals.</p>"},{"location":"UserInstructions/CreatePHG_step1-2_main/#step-1-create-database-and-load-the-reference-genome-and-genome-intervals","title":"Step 1: Create database and load the reference genome and genome intervals","text":"<p>This part of the pipeline populates the database with the reference genome, broken into \"reference ranges\" based on user-defined genome intervals. These same reference ranges are used to create haplotype blocks in later steps. The user can define different groups of reference ranges for different analyses.</p>"},{"location":"UserInstructions/CreatePHG_step1-2_main/#run-step-1","title":"Run Step 1","text":""},{"location":"UserInstructions/CreatePHG_step1-2_main/#use-the-links-below-to-work-through-each-step-in-the-step-1-flow-chart","title":"Use the links below to work through each step in the Step 1 flow chart","text":"<p>A. Create the default directory structure</p> <p>B. Create a PHG database</p> <p>C. Create bed file to define genome intervals</p> <p>D. Optionally, set up additional groups of PHG intervals</p> <p>Step 1D can be done at any time after the initial genome intervals have been loaded.</p>"},{"location":"UserInstructions/CreatePHG_step1-2_main/#step-2-add-haplotypes-to-a-database","title":"Step 2: Add haplotypes to a database","text":""},{"location":"UserInstructions/CreatePHG_step1-2_main/#to-run-the-step-2-pipeline-with-default-values","title":"To run the Step 2 pipeline with default values","text":"<p>After replacing \"/path/to\" with the correct path on your computer and my_container_name with a meaningful name,     run <code>docker run --name small_example_container --rm -v /path/to/dockerBaseDir/:/phg/ -t maizegenetics/phg /tassel-5-standalone/run_pipeline.pl -Xmx2G -debug -configParameters /phg/configSQLiteDocker.txt -PopulatePHGDBPipelinePlugin -endPlugin</code></p> <p>For information on setting config parameters, see details for running PopulatePHGDBPipelinePlugin.</p>"},{"location":"UserInstructions/CreatePHG_step1-2_main/#individual-parts-of-the-step-2-pipeline-can-be-run-separately","title":"Individual parts of the step 2 pipeline can be run separately","text":"<p>A. Align assemblies to reference genome, add to DB, use one of these options. The alignment with mummer4 will take less time to run and is only available with PHG versions 0.0.40 and older.  Anchorwave alignment can be very slow.  But the quality of alignment when using anchorwave has been shown to be superior to other methods.  Please check out this paper for more information on the anchorwave alignment process:</p> <ol> <li>Align assemblies using anchorwave aligner</li> <li>Align assemblies using mummer4 aligner</li> </ol> <p>B. Align WGS fastq files to reference genome</p> <p>C. Call variants from BAM file</p> <p>D. Filter GVCF, add to database</p> <p>Important: A GVCF file contains more information than a regular VCF file. We use the GATK GVCF format for the PHG.</p> <p>E. Create consensus haplotypes</p> <p>F. Optionally, set up groups of PHG taxa</p> <p>Return to PHG version 0.0.40 Home </p> <p>Return to PHG version 1.0 Home</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step1CreateDB_loadRef/","title":"Create and Populate a PHG database with haplotypes","text":"<p>Before creating the db, you need to setup the PHG directory structure.  We recommend using the default directory structure, which can be created as described in step 1A below.  When the default directory is created, two of the files created are config.txt and load_genome_data.txt. Those files need to have values added or modified by the user before other steps can be run.</p> <p>A sample config file can be viewed here</p> <p>There are two steps to building and populating a PHG database. The first step creates the PHG database and  populates it with the reference genome. The second step populates the database with additional haplotypes that have been aligned to the reference genome reference intervals.</p>"},{"location":"UserInstructions/CreatePHG_step1CreateDB_loadRef/#step-1-create-database-and-load-the-reference-genome-and-genome-intervals","title":"Step 1: Create database and load the reference genome and genome intervals","text":"<p>This part of the pipeline populates the database with the reference genome, broken into \"reference ranges\" based on user-defined genome intervals. These same reference ranges are used to create haplotype blocks in later steps. The user can define different groups of reference ranges for different analyses.</p>"},{"location":"UserInstructions/CreatePHG_step1CreateDB_loadRef/#run-step-1","title":"Run Step 1","text":""},{"location":"UserInstructions/CreatePHG_step1CreateDB_loadRef/#use-the-links-below-to-work-through-each-step-in-the-step-1-flow-chart","title":"Use the links below to work through each step in the Step 1 flow chart","text":"<p>A. Create the default directory structure</p> <p>B. Create a PHG database</p> <p>C. Create bed file to define genome intervals</p> <p>D. Optionally, set up additional groups of PHG intervals</p> <p>Step 1D can be done at any time after the initial genome intervals have been loaded.</p> <p>Return to PHG version 1 Home </p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step1_bedfile/","title":"Create bedfile of intervals for PHG reference ranges","text":""},{"location":"UserInstructions/CreatePHG_step1_bedfile/#intervals-file-documentation","title":"Intervals file documentation","text":"<p>The input to the loadGenomes pipeline is a tab-delimited file in bedfile format (positions are 0-based, start position is inclusive, end position is exclusive). The file must contain at least four columns. The first column should indicate chromosome, the second column indicates start position,  the third column indicates end position and the fourth column indicates the group name to which the interval belongs. The group name column should be a single word text.  The database will create reference range groups containing the intervals associated with each group from this fourth column.</p> <p>Other columns may be present but will be ignored. Header lines must begin with # and will be skipped. Documentation for the bed file format can be found here.</p> <p>If column headers are present, the first four must be named \"chrom\",\"chromStart\", \"chromEnd\", and \"name\" as per bed file format.</p> <p>You can break the reference genome into any set of intervals you want. In later steps these intervals can also be organized into different groups. For example, you can use a bed file created based on gff gene regions and organize these regions into genic and intergenic intervals in your database. If you have an existing bed file that includes only the regions on which you want to focus, but would like the non-represented regions of the genome to be included as well, you may run this bed file through the CreateValidIntervalsFilePlugin TASSEL plugin either within or outside of that docker.</p> <p>In later steps, the PHG loadGenomeIntervals step will load reference genome and assembly sequence for both the intervals defined in the provided intervals file. WGS data is only added for the intervals defined in the bed file. If you would like WGS data added for the entire genome, you must create a bed file with no gaps between the intervals.</p>"},{"location":"UserInstructions/CreatePHG_step1_bedfile/#things-to-consider-when-creating-a-phg-bed-file","title":"Things to consider when creating a PHG bed file","text":"<ul> <li>How large is the genome you're working with?</li> <li>What genome intervals are you most interested in studying?</li> <li>How diverse is the species you're working with? Do you expect large IBD regions?</li> </ul>"},{"location":"UserInstructions/CreatePHG_step1_bedfile/#createvalidintervalsfileplugin","title":"CreateValidIntervalsFilePlugin","text":"<p>The CreateValidIntervalsFilePlugin allows the user a convenient way to both check their intervals file for overlaps, and to create intervals for regions of the reference genome not covered by the initial intervals file.  The output from this script is a properly formatted intervals file that can be used as input when loading reference ranges to the PHG database. </p> <p>The CreateValidIntervalsFilePlugin has 6 parameters: * referenceFasta: path to reference fasta file (required) * IntervalsFile: original bed file with intervals (required) * generatedFile: name with path for intervals file created by the plugin (required) * mergeOverlaps: true or false indicating whether to merge overlapping intervals (optional - defaults to false) * userRegionsGroupName: name to give group of reference intervals provided in user file (optional - defaults to FocusRegion) * otherRegionsGroupName: name to give for any intervals that are created representing regions not in the original intervals file (optional - defaults to FocusComplement)</p> <p>An example of a script to run this plugin in a docker is below.  Note the paths assume the user has run the MakeDefaultDirectoryPlugin to create the directory structure:</p> <pre><code>#!java\nWORKING_DIR=/workdir/user/phg_new\nDOCKER_CONFIG_FILE=/phg/config.txt\n\ndocker1 run --name test_assemblies --rm  \\\n    -v ${WORKING_DIR}/:/phg/ \\\n    -t maizegenetics/phg:0.0.24 \\\n    /tassel-5-standalone/run_pipeline.pl -Xmx100G -debug -configParameters ${DOCKER_CONFIG_FILE} \\\n    -CreateValidIntervalsFilePlugin -intervalsFile /phg/inputDir/reference/original_intervals.bed \\\n    -referenceFasta /phg/inputDir/reference/ref.fa \\\n    -mergeOverlaps true \\\n    -generatedFile /phg/validBedFile.bed -endPlugin\n</code></pre> <p>If not running in docker, the command line plugin CreateValidIntervalsFilePlugin may be invoked from TASSEL in a similar manner. If the user chooses not to have overlapping intervals merged and overlapping intervals are found, the program will end with an error message.  A list of the overlapping intervals will be printed.</p> <p>Here is example command to call the CreateValidIntervalsFilePlugin from the command line outside a docker. You may, in addition, provide the optional parameters \"-userRegionsGroupName\" and \"-otherRegionsGroupName\".  The default values for these groups are \"FocusRegion\" and \"FocusComplement\".  By adding these parameters you may change the group names.</p> <pre><code>#!python\n\ntassel-5-standalone/run_pipeline.pl -Xmx50G -debug -CreateValidIntervalsFilePlugin -intervalsFile myInitialBedFile.bed -referenceFasta ref.fa -mergeOverlaps true -generatedFile validBedFile.bed -mergeOverlaps true -endPlugin &gt; plugin_output.txt\n</code></pre> <p>On a successful run, the output file will contain columns for chrom, chromStart, chromEnd and name.  Any overlapping intervals will have been merged, and intervals will have been created for areas of the reference genome not covered by the original intervals file.  All created intervals by this script will have the group name as provided in the group name parameter.</p> <p>The user may then edit the provided file, removing any intervals they do not wish to be included.  It is strongly recommended that the whole genome be represented by these intervals.  When regions of the genome are missing, you risk losing assembly sequence that may be conserved, but did not align to ranges specified in your intervals file.</p>"},{"location":"UserInstructions/CreatePHG_step1_bedfile/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Bed file formatting convention dictates that the bed file is 0-based, with an inclusive start position and exclusive end position. This differs from VCF file format, which is 1-based. Make sure to keep these coordinates in mind when creating your bed file.</li> <li>The PHG cannot deal with overlapping reference range intervals. Gene models from gff files often overlap; we recommend merging all overlapping intervals into a single interval when creating the PHG reference range interval bed file. It is recommended the CreateValidIntervalsFile.sh script or CreateValidIntervalsFilePlugin (TASSEL command line) be used to create a valid file from your original intervals file.  Other option: Bedtools offers one set of useful command line tools to create and manipulate bed files.</li> </ol> <p>Return to Step 1 pipeline version 0.0.40 or earlier</p> <p>Return to Step 1 pipeline version 1.0 or later</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step1_groupGenomeIntervals/","title":"Separate genome intervals into different groups","text":""},{"location":"UserInstructions/CreatePHG_step1_groupGenomeIntervals/#details","title":"Details","text":"<p>Once you have created the initial set of genome intervals, you can group these intervals in various ways. This step lets you add multiple reference range groups after the original reference range loadings. It does not add different reference ranges; only combines the existing reference ranges in different groups. To add different reference range coordinates you need to create an entirely new database.</p>"},{"location":"UserInstructions/CreatePHG_step1_groupGenomeIntervals/#kitchen-sink","title":"Kitchen Sink","text":"<p>The AddRefRangeGroupPlugin method creates groupings for the reference ranges defined in your database. You can create as many sets of groupings in the database as you would like. </p> <p>There are 4 parameters used by this step:</p> <ul> <li>configFile: Path to file containing database access information. The config file contains separate lines for host=X, user=X, password=X, DB=X, and DBtype=X where X is defined by the user, and DBtype is either sqlite or postgres. (required)</li> <li>methodName: Method name for this reference range group.  (required)</li> <li>methodDetails: Description for this group of reference ranges. (required)</li> <li>ranges: This should be a tab-delimited bed formatted file with a subset of reference ranges currently existing in the DB. The file should contain chromosome, reference range start position, and reference range end position for the intervals you want to include in this group. There should not be a header line. (required)</li> </ul>"},{"location":"UserInstructions/CreatePHG_step1_groupGenomeIntervals/#details-on-running-this-step-through-docker","title":"Details on running this step through docker","text":"<p>When AddRefRangeGroupPlugin is run as part of a Docker container script, the Docker script expects the following directory mount points.  It assumes the directory structure used matches that created from the MakeDefaultDirectoryPlugin, and that both the config.txt and ranges.txt files live at the top level in the folder mounted to /phg:</p> <ul> <li>mount point 1: folder that contains the configFile and ranges file.</li> </ul> <p>An example Docker script to run the AddRefRangeGroupPlugin plugin is:</p> <pre><code>docker run --name group_intervals --rm \\\n    -v /localDir:/phg\n    -t maizegenetics/phg:latest \\\n    /tassel-5-standalone/run_pipeline.pl -debug -AddRefRangeGroupPlugin -configFile /phg/config.txt -methodName ${methodName} -methodDetails ${methodDetails} -ranges ${rangesFile}\n</code></pre> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The -t directive indicates the Docker image of which this container will be an instance.  </p> <p>The last line tells the Docker container to run the GroupGenomeIntervals.sh script which is found in the root directory.  The items following are the parameters to the GroupGenomeIntervals.sh script.</p>"},{"location":"UserInstructions/CreatePHG_step1_groupGenomeIntervals/#files","title":"Files","text":"<p>Config file</p> <p>An example can be found here: Master config file</p> <p>Ranges file</p> <p>This is a tab-delimited bed file with a subset of the reference ranges used to create the database. The file should contain chromosome, reference range start position, and reference range end position for the intervals you want to include in this group. There should not be a header line. There is no limit to the number of ranges that can be included in this file, but reference ranges in this file must match ranges already in the database (added in the LoadGenomeIntervals step). A reference range can be added to multiple groups by running this step multiple times with different method names.</p>"},{"location":"UserInstructions/CreatePHG_step1_groupGenomeIntervals/#addrefrangegroupplugin","title":"AddRefRangeGroupPlugin","text":"<p>The plugin takes a bed file with a list of reference ranges in the database, a method name for that group of ranges, a description of that group of ranges, and the configuration file indicating database name and type. The plugin reads the bed file into an object and all reference ranges from the database. It verifies that all entries from the bed file exist in the reference_ranges table (it will throw an exception if they are not all present). Next, the plugin verifies that the user-defined method name does not already exist in the methods table. If it does not, the plugin will add that method name to the database and add the specified reference ranges to the ref_range_ref_range_methods table with a new method ID.</p> <p>Database tables updated via this method are:</p> <ul> <li>methods</li> <li>ref_range_ref_range_method</li> </ul> <p>The parameters to this plugin are:</p> <ul> <li>-methodName  Method name for this reference range group. (REQUIRED) <li>-methodDetails  Description for this group of reference ranges. (REQUIRED) <li>-ranges  Tab-delimited, BED Formatted file containing chrom, ref range start position, ref range end position. No header line. (REQUIRED) <li>-configFile  Path to file containing database access information, separate lines for host=X, user=X, password=X, DB=X, DBtype=X where X is user defined, and DBtype is either sqlite or postgres. (REQUIRED) <p>This plugin expects a database connection as input. This can be obtained by chaining the GetDBConnectionPlugin with the AddRefRangeGroupPlugin when called from the command line with the TASSEL run_pipeline.pl script.</p> <p>An example of chaining these plugins is below:</p> <pre><code>#!python\n\n/tassel-5-standalone/run_pipeline.pl -Xmx10G -debug \\\n-GetDBConnectionPlugin \\\n    -config ${dbConfigFile} \\\n    -create true  \\\n    -endPlugin \\\n-AddRefRangeGroupPlugin \\\n    -config ${dbConfigFile} \\\n    -ranges ${rangesFile} \\\n    -methodName ${method_name}  \\\n    -methodDetails ${method_details} \\\n    -endPlugin\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step1_groupGenomeIntervals/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Check file paths and names for mistakes.</li> <li>Double check that all reference ranges in the ranges file are present in the database (an easy way to check this is by verifying these ranges are in your original reference range intervals file). </li> <li>Make sure the methodName you are trying to use is unique and not already in the database methods table.</li> </ol> <p>Return to Step 1 pipeline version 0.0.40 or earlier</p> <p>Return to Step 1 pipeline version 1.0 or later</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step1_loadGenomeIntervals/","title":"Load genome intervals to PHG","text":""},{"location":"UserInstructions/CreatePHG_step1_loadGenomeIntervals/#details","title":"Details","text":"<p>This step populates the PHG database with a reference genome and creates reference intervals to be used in later steps of the pipeline. These tend to be conserved regions identified from your reference genome, but can be identified and defined any way you choose. Some options include defining gene regions from a gff file, exon regions, or arbitrary 1000 bps regions. Any region not specified in the reference intervals file will not be included in as reference range intervals in the database. Haplotype data is not added for any region that is not represented in the intervals file. After running this step you will have a database containing the reference genome and any reference range groups you have defined.</p>"},{"location":"UserInstructions/CreatePHG_step1_loadGenomeIntervals/#kitchen-sink","title":"Kitchen Sink","text":"<p>The LoadGenomeIntervals.sh script reads reference genome sequence, genome intervals (anchor bed file) information and populates the initial database tables with reference genome data.  The database can be either postgres or SQLite.  The type of database and the means to access it are provided in the config file.</p> <p>There are 6 parameters used by this step:</p> <ul> <li>configFile: DB Config File containing properties host,user,password,DB and DBtype where DBtype is either sqlite or postgres. The shell script assumes the configuration file lives in the mounted /tempFileDir/data/ directory. If using sqlite, the sqlite database should be written to the directory mounted to /tempFileDir/outputDir/. The master config file can be found here: Config File. Only the first 5 entries in the config file deal with database connection. The example config file contains additional parameters that are used in other steps of the pipeline.</li> <li>referenceFastaFile: The reference genome fasta file. The path to this file should be included in the config file. </li> <li>anchorBedFile: A BED-formatted, tab-delimited file containing the chromosome, interval start position, interval end position and \"type\" for the genome intervals that will comprise the reference ranges. The \"type\" column indicates the group to which the interval belongs.  The default groups, when creating this file from the CreateValidIntervalsFilePlugin,  are \"FocusRegion\" and \"FocusComplement\" which can be thought of as \"genic\" and \"inter-genic\" if the user were separating their regions by gene space.  The user may use any group names, but this field must be populated for each reference range in the anchors file.  The path to this file should be included in the config file.   NOTE: only ranges represented in the anchorBedFile will be included in the DB. This script will no longer automatically created inter-regions for those regions not in the anchor bed file. You may use the CreateValidIntervalsFilePlugin plugin to create a bedfile containing inter-regions.</li> <li>genomeDataFile: This is a tab-delimited file containing specific data related to the reference genome. It must have the following columns:  \"Genotype\" \"Hapnumber\" \"Dataline\" \"Ploidy\" \"Reference\" \"GenesPhased\" \"ChromsPhased\" \"Confidence\" \"Method\" \"MethodDetails\".  If running with PHG version 1.0 or greater, it additionally needs the column \"gvcfServerPath\".  The location of this file should be stored in the config file.  See this example genome data file: Sample Genome Data File.  Note this example genome data file contains the \"gvcfServerPath\" column needed when running PHG versions 1.0 or greater.</li> <li>refServerPath: This parameter provides information on where, in the real world, the genome fasta used to populate the reference haplotypes is stored.  This reflects a more permanent location, e.g. an irods directory.  Or could be a directory on AWS, or at some institution. It is just a string. </li> <li>create_new boolean: either \"true\" or \"false\", indicating whether a new database instance should be created. If the value is \"true\", a new db as identified in the config file will be created to hold the reference anchor data. If \"false\", the data created will be added to an existing database specified by the config file. If the boolean is \"false\" and there is no database of the specified name, an error is thrown and processing stops.  The \"false\" value allows users to add multiple reference versions to a single database. We recommend against this, and recommend this value be \"true\" to facilitate creation of a new database. WARNING:  \"true\" will delete any existing database of the specified name before recreating it.</li> </ul>"},{"location":"UserInstructions/CreatePHG_step1_loadGenomeIntervals/#details-on-running-this-step-through-docker","title":"Details on running this step through docker","text":"<p>When LoadGenomeIntervals.sh is run as part of a Docker container script, the Docker script expects the following directory mount points:</p> <ul> <li>Mount localMachine:/pathToOutputDir/ to docker:/phg/outputDir/. If using sqlite, the database mentioned in the config file should be in this directory.</li> <li>Mount localMachine:/pathToReferenceDir/ to docker:/phg/data/reference. This directory holds the reference genome fasta file</li> <li>Mount localMachine:/pathToDataDir/ to docker:/phg/data/. This contains the config file and genome data file</li> <li>Mount localMachine:/pathToAnswerDir/ to docker:/tempFileDir/answer/. This directory much contain the anchor bed file.</li> </ul> <p>An example Docker script to run the LoadGenomesIntervals.sh script is:</p> <pre><code>#!python\n\n# This script assumes the user is using the default directory structure created\n# when the PHG MakeDefaultDirectoryPlugin TASSEL plugin has been run.\n\n# It assumes the config.txt file lives in the top level working directory\n# It assumes the reference fasta (in this case, Zea_mays.AGPv4.dna.toplevelMtPtv3.fa) lives\n# in workingDir/inputDir/reference/\n# It assumes the genome intervals file (in the example below, maizeRefAnchor_intervals_bed) lives\n# in workingDir/inputDir/\n# It assumes the genomeData file describing the reference genome lies in workingDir/inputDir/reference/ \n# It assumes your sqlite database lives in the workingDir/outputDir/  This in only relevant when running an SQLite database.  This path shows up in the config file, parameter \"db\".\n\n# You must change \"/workdir/user/DockerTuningTests/...\" to match your own directory paths\ndocker run --name load_phg_container --rm \\\n        -v /workdir/user/MyGenomePHG/:/phg/ \\\n        -t maizegenetics/phg:latest \\\n        /LoadGenomeIntervals.sh config.txt Zea_mays.AGPv4.dna.toplevelMtPtv3.fa maizeRefAnchor_intervals.bed B73Ref_load_data.txt  \"irods:/ibl/assemblies\" true\n\n</code></pre> <p>Or run using the MakeInitialPHGDBPipelinePlugin  from the docker:</p> <pre><code>#!python\ndocker run --name small_seq_test_container --rm \\\n    -v /Users/lcj34/temp/phgSmallSeq/dockerBaseDir/:/phg/ \\\n    -t phgrepository_test:latest \\ \n    /tassel-5-standalone/run_pipeline.pl -Xmx10G -debug -configParameters /phg/configSQLiteDocker.txt -MakeInitialPHGDBPipelinePlugin -endPlugin\n\n</code></pre> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The -t directive indicates the Docker image of which this container will be an instance.  </p> <p>The last line tells the Docker container which script to run, either LoadGenomeIntervals.sh, or tassel with plugin MakeInitialPHGDBPipelinePlugin. Both tassel-t-standalone and LoadGenomeIntervals.sh script are found in the root directory.  The items following are the parameters to the LoadGenomeIntervals.sh script.</p>"},{"location":"UserInstructions/CreatePHG_step1_loadGenomeIntervals/#files","title":"Files","text":"<p>Config file</p> <p>An example can be found here: Master config file</p> <p>Reference Fasta file</p> <p>Fasta file for the reference genome you want to use in the database</p> <p>Anchor bed file</p> <p>Bed file with selected reference range intervals. Information on how to create this file can be found here: Create reference ranges</p> <p>Genome Data File</p> <p>An example of the genome data file can be found here: Sample Genome Data File</p> <p>The contents of the genome data file are described below:</p> <ul> <li>Genotype: The name of the line as you want it to appear in the database genotypes table \"name\" column (e.g. \"B104\" or \"B104_haplotype_caller\"). These names must be unique for each line. If your reference genome is B73 and you also have a B73 haplotype that you want to process from other data, then you must make the names distinct (e.g. B73Ref and B73).</li> <li>Hapnumber: The 0-based chromosome number. For inbreds, this is always 0. </li> <li>Dataline: Text description defining the taxon. This data will be stored in the \"description\" field of the genotypes table.</li> <li>Ploidy: The number of chromosomes for this genome, the value to be stored in the \"ploidy\" field of the genome_intervals table. Should be 1 for the reference genome.</li> <li>GenesPhased: A boolean field indicating if the genes are phased. This should be false for the reference genome.</li> <li>ChromsPhased: A boolean field indicating if the chromosomes are phased. This should be false for the reference genome.</li> <li>Confidence: A float field indicating confidence in phasing. Leave this value as 1 when there is no phasing.</li> <li>Method: A method name by which this version of the reference data can be identified.</li> <li>MethodDetails: Text description defining the method used to create the reference (or indicating the AGP version)</li> <li>gvcfServerPath: The server and path where the gvcf files will be stored outside of the database.  This should be the remove server path, not the local path where these file are downloaded for processing, unless the remote and local server are the same.</li> </ul>"},{"location":"UserInstructions/CreatePHG_step1_loadGenomeIntervals/#plugins","title":"Plugins","text":""},{"location":"UserInstructions/CreatePHG_step1_loadGenomeIntervals/#loadallintervalstophgdbplugin","title":"LoadAllIntervalsToPHGdbPlugin","text":"<p>The database loading functionality is performed by a call to the LoadAllIntervalsToPHGdbPlugin in TASSEL, which is described below. </p> <p>This plugin takes a reference genome fasta, a genome data file and an anchors file as described above. The plugin code grabs sequence from the reference genome fasta based on the coordinates from the anchors file and loads the genome interval sequence to the specified PHG database. The genome data file contains genome specific details used when adding to the genotypes, gametes and method tables.</p> <p>When finished loading the user defined regions, the intervals will be grouped based on the \"name\" field from the user provided intervals file. </p> <p>Database tables populated via this method are:</p> <ul> <li>genotypes</li> <li>gametes</li> <li>gamete_groups</li> <li>methods</li> <li>genome_interval_versions</li> <li>genome_intervals</li> <li>haplotypes</li> <li>gamete_haplotypes</li> <li>ref_range_ref_range_method</li> </ul> <p>The parameters to this plugin are:</p> <ul> <li>-ref   Reference Genome File for aligning against. (Default is null) (REQUIRED) <li>-anchors  CSV file containing chrom, anchor start positions, anchor end position, gene start, gene end. The positions are physical positions (1-based, inclusive/inclusive).  (Default is null) (REQUIRED) <li>-genomeData  A tab-delimited file containing genome specific data with columns as described above. (Default is null) (REQUIRED) <li>-refServerPath  The external server/path where users may find the genome fasta used as the reference for this PHG db instance. <p>This plugin expects a database connection as input. This can be obtained by chaining the GetDBConnectionPlugin with the LoadGenomeIntervalsToPHGdbPlugin when called from the command line with the TASSEL run_pipeline.pl script.</p> <p>An example of chaining these plugins is below:</p> <pre><code>#!python\n\n/tassel-5-standalone/run_pipeline.pl -Xmx10G -debug \\\n-GetDBConnectionPlugin \\\n    -config dbConfigFile \\\n    -create true  \\\n    -endPlugin \\\n-LoadAllIntervalsToPHGdbPlugin \\\n    -ref reference \\\n    -anchors rangesFile \\\n    -genomeData genomeDataFile  \\\n        -refServerPath refServerString \\\n    -outputDir &lt;pathToOutPut&gt; \\\n    -endPlugin\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step1_loadGenomeIntervals/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>The reference genome name must be unique - if you have duplicate taxa, make sure they have unique labels when added to the database. </li> <li>Check that the genomeDataFile file is tab-delimited and that all fields are present. Check that there is not a trailing tab after the last field and that there are no extra columns in the file.</li> </ol> <p>Return to Step 1 pipeline version 0.0.40 or earlier</p> <p>Return to Step 1 pipeline version 1.0 or later</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step2AssemblyAndWGSHaplotypes/","title":"Step 2: Add haplotypes to a database","text":""},{"location":"UserInstructions/CreatePHG_step2AssemblyAndWGSHaplotypes/#step-2-pipeline-includes-several-individual-steps-some-of-which-may-be-run-in-parallel","title":"Step 2 pipeline includes several individual steps, some of which may be run in parallel.","text":"<p>All PHG haplotypes are created from GVCF files.  The recommended method of creating these GVCF files is by aligning assemblies to a reference genome and converting the output to bgzipped GVCF files along with a tabix'd version of the bgzipped file.  Using assemblies provides for more complete haplotypes and better calling of long indels.  However, not every species has multiple assembled genomes. For those species, GVCF files created by aligning WGS fastq files to a reference genome may be used to create PHG haplotypes. In both cases, GVCF files are the vehicle for creating the PHG haplotypes.</p> <p> Important:  A GVCF file contains more information than a regular VCF file. We use the GATK GVCF format for the PHG.</p> <p></p> <p>The major architectural difference between PHG version 1 and the versions that preceded it is in the storage of haplotype variant information.  In previous releases, these variants were stored in the database, originally in the haplotypes table, and in later versions, in their own table.  While it was convenient to have all data stored in the database, these tables became very large and adversely affected database performance. As the number of variants increased, the processing time involving these variants also increased, often to levels where processing hung.</p> <p>To counter-act this behavior, we made the decision to store all variant information in the original gvcf files external to the database, but optionally on the same server.  There will be 2 files stored for each genome:  a bgzipped version of the gvcf, and a tabix'd version of the bgzipped file.  While this requires a bit more work on the part of the user, it greatly increases processing speed in the PHG while allowing variants to be stored in a standard format compatible for processing with other bioinformatics applications.</p> <p>When loading the haplotypes, the user specifies via parameters the location of the external gvcf folder.  This data is then stored to the PHG database table \"genome_file_data\".  Once haplotypes have been created and stored from the gvcfs files, the user must arrange for these files to be stored to an accessible, central location as specified by the gvcfServerDir parameter.  Prior to running any PHG code that requires haplotype variant information, the user must download the gvcf files (.gz and .gz.tbi files) to the local server where data processing will occur.  </p> <p>A new \"localGVCFFolder\" parameter that holds the local location of these files is now required in the following plugins.  This parameter is used for accessing the files when variant information is requested:</p> <ul> <li>HaplotypeGraphBuilderPlugin</li> <li>HaplotypeGraphStreamBuilderPlugin</li> <li>ImputePipelinePlugin</li> <li>MakeInitialPHGDBPipelinePlugin</li> <li>PopulatePHGDBPlugin (we do not recommend the use of this plugin)</li> </ul> <p>The \"gvcfServerPath\" column must be in the keyfiles that are passed as parameters to the following plugins/functions.  This value is stored to the PHG genome_file_data table where it records the server/path where these files are stored externally. This is expected to be the more \"permanent\" home for these files, and is the link from which a user will download the files to his local server:</p> <ul> <li>LoadAllIntervalsToPHGdbPlugin (new column in the file sent as the \"genomeData\" parameter)</li> <li>LaodHaplotypesFromGVCFPlugin (new column in the file sent as the \"wgsKeyFile\" parameter)</li> <li>PopulatePHGDBPipelinePlugin (parameter used when creating the gvcf key file - note, we do not recommend using this plugin)</li> </ul> <p> A. Align assemblies to reference genome, add to DB. </p> <p>Any alignment method may be used to create the GVCF files needed for loading haplotypes to the PHG database.  We recommend using the anchorwave program for genome alignment.  Anchorwave alignment can be memory and processing intensive but the quality of alignment has been shown to be superior to other alignment methods. Anchorwave can take up to 50G per thread when aligning a single genome.  We recommend you run your alignments separately on one or more machines, convert the resulting MAF files to bgzipped/tabix'd GVCF files, then copy these files (both the bgzipped and tabix'd versions) to the machine from which you will load your database. </p> <p>Processing alignments via mummer4 is available in previous editions of the PHG software, but not with the new versions that process GVCF files into haplotypes.  Should you wish to continue to use mummer4, you can align outside of the pipeline software, and use a tool e.g. mummer-2-vcf for conversion.</p> <p>Specifics on running anchorwave through the PHG pipeline are described below</p> <ol> <li>Align assemblies using anchorwave aligner</li> </ol> <p> B. Create haplotypes from WGS fastq files </p> <ol> <li> <p>Align WGS fastq files to reference genome</p> </li> <li> <p>Call variants from BAM file</p> </li> <li> <p>Filter GVCF, add to database</p> </li> </ol> <p> C. Creating consensus </p> <ol> <li>Create consensus haplotypes</li> </ol> <p> D.  Optional: group your PHG taxa </p> <ol> <li>Optionally, set up groups of PHG taxa</li> </ol> <p>Return to PHG version 1.0 Home </p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step2_FilterGVCFPlugin/","title":"Filter GVCF Plugin","text":"<p>This plugin filters a GVCF file and creates fasta sequence for loading to the PHG database.  BCFTools is used with a poissonProbability tuple to filter based on depth.</p> <p>Tables populated via this method are:</p> <ul> <li>genotypes</li> <li>gametes</li> <li>gamete_groups</li> <li>methods</li> <li>haplotypes</li> <li>gamete_haplotypes</li> </ul> <p>The parameters to this plugin are:</p> <ul> <li>-refFile  Reference File used to create the GVCFs. (Default is null) (REQUIRED) <li>-bedFile  Bed file containing the genome interval information. (Default is null) (REQUIRED) <li>-gvcfDir  Directory holding the GVCF files to be filtered. (Default is null) (REQUIRED) <li>-outputDir  Directory to hold the output files. (Default is null) (REQUIRED) <li>-configFile  Config file containing the filtering parameters. If not present, defaults will be used. (Default is null) (OPTIONAL) <li>-lowerPoissonBound  Lower Poisson Bound used for filtering.  (Default: 0.0) (OPTIONAL) <li>-upperPoissonBound  Lower Poisson Bound used for filtering.  (Default: 1.0) (OPTIONAL) <p>The configFile parameter should contain key-value pairs.  If this file is present, the gvcf files will be filtered based on the file's parameters. To read more about the strucure of variant call records and the meaning of the fields below, please see GATK Forum.  The supported key-value pairs for the config file are below.  Any fields left out will be ignored.</p> <ul> <li>exclusionString = String used for bcftools filtering.  If this is specified, no additional terms are added to exclude</li> <li>DP_poisson_min = minimum poisson bound used for filtering(absolute minimum is 0.0)</li> <li>DP_poisson_max = maximum poisson bound used for filtering(absolute maximum is 1.0)</li> <li>DP_min = minimum DP bound: the filtered depth at the sample level. Minimum number of filtered reads that support each of the reported alleles.</li> <li>DP_max = maximum DP bound: maximum number of filtered reads that support each of the reported alleles.</li> <li>GQ_min = minimum GQ bound: minimum confidence level that the genotype assigned to a particular sample is correct.</li> <li>GQ_max = maximum GQ bound: maximum confidence level that the genotype assigned to a particular sample is correct.</li> <li>QUAL_min = base quality minimum bound: confidence there exists variation at a given site.</li> <li>QUAL_max = base quality maximum bound: confidence there exists variation at a given site.</li> <li>filterHets = true/false or t/f, if true will filter sites where 2 or more alleles have above 0 </li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_FilterGVCFSingleFilePlugin/","title":"FilterGVCFSingleFilePlugin","text":"<p>This plugin filters a GVCF file and creates fasta sequence for loading to the PHG database.  BCFTools is used with a poissonProbability tuple to filter based on depth.</p> <p>Tables populated via this method are:</p> <ul> <li>genotypes</li> <li>gametes</li> <li>gamete_groups</li> <li>methods</li> <li>haplotypes</li> <li>gamete_haplotypes</li> </ul> <p>The parameters to this plugin are:</p> <ul> <li>-inputGVCFFile  GVCF File to be filtered. (Default is null) (REQUIRED)</li> <li>-outputGVCFFile  Output GVCF File Path and Name. (Default is null) (REQUIRED) <li>-configFile  Config file containing the filtering parameters. If not present, defaults will be used. (Default is null) (OPTIONAL) <p>The configFile parameter should contain key-value pairs.  If this file is present, the gvcf files will be filtered based on the file's parameters. To read more about the strucure of variant call records and the meaning of the fields below, please see GATK Forum.  The supported key-value pairs for the config file are below.  Any fields left out will be ignored.</p> <ul> <li>exclusionString = String used for bcftools filtering.  If this is specified, no additional terms are added to exclude</li> <li>DP_poisson_min = minimum poisson bound used for filtering(absolute minimum is 0.0)</li> <li>DP_poisson_max = maximum poisson bound used for filtering(absolute maximum is 1.0)</li> <li>DP_min = minimum DP bound: the filtered depth at the sample level. Minimum number of filtered reads that support each of the reported alleles.</li> <li>DP_max = maximum DP bound: maximum number of filtered reads that support each of the reported alleles.</li> <li>GQ_min = minimum GQ bound: minimum confidence level that the genotype assigned to a particular sample is correct.</li> <li>GQ_max = maximum GQ bound: maximum confidence level that the genotype assigned to a particular sample is correct.</li> <li>QUAL_min = base quality minimum bound: confidence there exists variation at a given site.</li> <li>QUAL_max = base quality maximum bound: confidence there exists variation at a given site.</li> <li>filterHets = true/false or t/f, if true will filter sites where 2 or more alleles have above 0 </li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_LoadHaplotypesFromGVCFPluginDetails/","title":"LoadHaplotypesFromGVCFPlugin Details","text":"<p>When you have a GVCF file (created from the MAFToGVCFPlugin or other means), the next step is to load the database with the information from this GVCF.  The LoadHapltoypesFromGVCFPlugin takes a keyfile containing a list of GVCF files for uploading.  It parses and formats the data, then loads this data as haplotypes to a PHG database defined by the user configuration file.</p> <p>This step can be run using a shell script similar to the one detailed below:</p> <pre><code>WORKING_DIR=/workdir/lcj34/anchorwaveTesting/\nDOCKER_CONFIG_FILE=/phg/config.txt\n\n# The LoadHaplotypesFromGVCFPlugin parameters will be obtained via the config file\ndocker1 run --name load_haplotypes_container --rm \\\n    -v ${WORKING_DIR}/:/phg/ \\\n    -t maizegenetics/phg:latest \\\n    /tassel-5-standalone/run_pipeline.pl -Xmx100G -debug -configParameters ${DOCKER_CONFIG_FILE} \\\n    -LoadHaplotypesFromGVCFPlugin -endPlugin\n</code></pre> <p>It may also be run directly from tassel-5-standalone as per this command.  Parameters are explicitly defined in this example, but they may alternately be pulled from a config file as with the example above.</p> <pre><code>/workdir/lcj34/tassel-5-standalone/run_pipeline.pl -Xmx100G -debug -LoadHaplotypesFromGVCFPlugin -wgsKeyFile /pathToDir/keyFile.txt -gvcfDir /workdir/lcj34/GVCF_files/ -referenceFasta ref.fasta -bedFile /pathToDir/mybedfile.bed -hapltoypteMethodName gvcfHaplotypes &gt; output_loadHaplotypeFromGVCF.txt\n</code></pre> <p>If running with a config file, the parameters listed below should be defined(replace values with appropriate values for your own run).  Optional parameters are not shown in the example below, but may be included as desired.</p> <pre><code>LoadHaplotypesFromGVCFPlugin.referenceFasta=/phg/inputDir/reference/Zm-B73-REFERENCE-NAM-5.0.fa\nLoadHaplotypesFromGVCFPlugin.wgsKeyFile=/phg/gvcfKeyFile.txt\nLoadHaplotypesFromGVCFPlugin.gvcfDir=/phg/inputDir/loadDB/gvcfs/\nLoadHaplotypesFromGVCFPlugin.bedFile=myBedFile.bed\nLoadHaplotypesFromGVCFPlugin.haplotypeMethodName=assembly_by_anchorwave\nLoadHaplotypesFromGVCFPlugin.haplotypeMethodDescription=\"files aligned with anchorwave, then turned to gvcf with plugin\"\n\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_LoadHaplotypesFromGVCFPluginDetails/#details","title":"Details","text":"<p>The GVCF files that will be loaded are defined in a keyfile that provides the location of the gvcfs, the sample names to use with each file, and information relating to gene phasing for the sample.  All data will be processed into haplotypes to be stored in the PHG database haplotypes table.  These haplotypes will be associated with the method defined by the plugin parameters.</p>"},{"location":"UserInstructions/CreatePHG_step2_LoadHaplotypesFromGVCFPluginDetails/#parameters","title":"Parameters","text":"<p>Run the following command to print a current list of parameters for this plugin:</p> <pre><code>docker run --rm  maizegenetics/phg /tassel-5-standalone/run_pipeline.pl -LoadHaplotypesFromGVCFPlugin\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_LoadHaplotypesFromGVCFPluginDetails/#parameter-descriptions","title":"Parameter Descriptions","text":"<ul> <li>wgsKeyFile: (required) Name of the keyfile to process.  Details on the keyfile are in a separate section below.</li> <li>gvcfDir: (required) Full path to the directory holding all the gvcf files.  If running with PHG Version 0.0.40 or lower, these are files with .gvcf extension.  If running with PHG version 1.0 or higher, these are bgzipped and tabix'd versions of the gvcf files.  For each genome, there should be a .gz and corresponding *.gz.tbi file.</li> <li>referenceFasta: (required) Full path to the reference fasta file</li> <li>sampleName: (required ) Sample name to write to the GVCF file.  This is also the name that will later be written to the database.</li> <li>bedFile: (required) Path to a bed formatted file containing the database reference range information.</li> <li>haplotypeMethodName: (required) The method name to use for the haplotypes being uploaded. This is a user defined method name that should be descriptive of the method used to create the haplotypes.  Examples would be \"anchorwave_NAM_assemblies\" or \"anchorwave_released_assemblies\" or \"WGS_haplotypes\".  The name may be as long as you wish but should be meaningful to you.</li> <li>haplotypeMethodDescription: (optional: no default) Description of the method used for the haplotypes being uploaded. This may describe methods of alignment or any other information specific to the haplotype gvcf creation.</li> <li>numThreads: (optional: default=3) The number of CPU threads to use when processing the data.  The GVCF upload will subtrace 2 from this number, elaving 1 thread for IO to the DB and 1 thread for the operating system.</li> <li>maxNumHapsStaged: (optional: default=1000) Number of haplotype instances to stage per chromosome before a DB write is triggered.  Lower this number if you are running into RAM issues.  It will take longer to process, but should help balance the load.</li> <li>mergeRefBlocks: (optional: default=false)  Merge consecutive GVCF ReferenceBlocks together.  If there is at least 1 bp between two gvcf refBlock records, the records will not be merged</li> <li>queueSize: (optional: default=30) Size of Queue used to pass information to the DB writing thread.  Increase this number to have better thread utilization at the expense of RAM. If you are running into Java heap Space/RAM issues and cannot use a bigger machine, decrease this parameter.</li> <li>isTestMethod: (optional: default=false) Indication if the data is to be loaded against a test method. Data loaded with test methods are not cached with the PHG ktor server</li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_LoadHaplotypesFromGVCFPluginDetails/#keyfile","title":"Keyfile","text":"<p>The LoadHaplotypesFromGVCFPlugin keyfile is used to specify the inputs to the database.  It must have columns sample_name, files, type, chrPhased, genePhased, and phasingConf. If running with PHG version 1.0 or greater, it must also have a column named gvcfServerPath.  Optionally you can have sample_description, assemblyServerPath, assemblyLocalPath. Ordering of the columns does not matter.</p> <p>If present, the \"sample_description\" column  becomes the description stored in the genotypes table \"description\" field associated with the sample taxon.  The \"assemblyServerPath\" and \"assemblyLocalPath\" columns are used when the gvcf files represent data from assemblies. The information in these 2 columns is used to populate the genome_file_data table.  The \"assemblyServerPath\" value is stored in this PHG table to preserve information on where this assembly may be found in the public domain.  The \"assemblyLocalPath\" value is used to access the genome fasta locally for creating a hash value to store with this file data.</p> <p>The keyfile should be in tab-delimited format.  The headers should look as below: (this example includes the optional columns, as well as the gvcfServerPath required for PHG version 1.0 or greater)</p> <pre><code>type    sample_name     sample_description      mafFile files   chrPhased       genePhased      phasingConf     assemblyServerPath      assemblyLocalPath  gvcfServerPath\n</code></pre> <p>An example keyfile for this plugin is below. Note all fields are tab-delimited and because this is run in a docker, the assemblyLocalPath value is a docker path.  The assemblyServerPath should be an accessible location outside the docker, as should be the gvcfServerPath.  The gvcfServerPath values must be of the format ;, with a semi-colon separating the server and path values <pre><code>type    sample_name     sample_description      mafFile files   chrPhased       genePhased      phasingConf     assemblyServerPath      assemblyLocalPath       gvcfServerPath\nGVCF    LineA_Assembly  smallSeq description for assembly LineA /phg/outputDir/align//LineA.maf LineA.gvcf.gz   true    true    0.9     irods:AssemblyFilePath/LineA.fa /phg/inputDir/assemblies//LineA.fa      198.42.78.6;/mypath/remoteGvcfs/\nGVCF    LineB_Assembly  smallSeq description for assembly LineB /phg/outputDir/align//LineB.maf LineB.gvcf.gz   true    true    0.9     irods:AssemblyFilePath/LineB.fa /phg/inputDir/assemblies//LineB.fa      198.42.78.6;/mypath/remoteGvcfs/\n</code></pre> <p>Column details:</p> <ul> <li>sample_name: name of the taxon you are uploading</li> <li>files: a comma-separated list of file names(without path) providing the names of the GVCF files to be uploaded.  If there are multiple files, gamete_ids will be assigned in order.</li> <li>type: the type of files.   This plugin will only process keyfile lines with type = \"GVCF\".</li> <li>chrPhased: must be either 'true' or 'false' and represent whether or not the chromosome is phased.</li> <li>genePhased: must be either 'true' or 'false' and represent whether or not the genes are phased.</li> <li>phasingConf: must be a number between 0.0 and 1.0 representing the confidence in the phasing.</li> <li>sample_description:  a short description of the sample name to be uploaded.  If this column is not specified an empty description will be used.</li> <li>assemblyServerPath (optional column): an external path where the genome fasta file for this assembly may be found. It should be directory and file.  This column is optional and generally included when the GVCF files are the result of assembly aligning with the anchorwave program.</li> <li>assemblyLocalPath (optional column): the path to the assembly genome fasta file that can be accessed from this program.  This file is used to create am MD5 hash for the genome_file_data table.  This column is optional and generally included when the GVCF files are the result of assembly aligning with the anchorwave program.</li> <li>gvcfServerPath (required when running PHG versions 1.0 or greater) an external server and path to which this gvcf file will be uploaded.</li> </ul> <p>Return to Step 2 pipeline, version 0.0.40 or earlier</p> <p>Return to Step 2 pipeline, version 1.0 or later</p>"},{"location":"UserInstructions/CreatePHG_step2_MAFToGVCFPluginDetails/","title":"MAFToGVCFPlugin Details","text":"<p>After aligning your assemblies and obtaining a MAF file from anchorwave, the next step is to convert the MAF file to a GVCF file.  It is this latter format which will be used for loading the alignment data as haplotypes to the PHG database.</p> <p>This step  can be run using a shell script similar to the one detailed below:</p> <pre><code>WORKING_DIR=/workdir/lcj34/anchorwaveTesting/\nDOCKER_CONFIG_FILE=/phg/config.txt\n\n# The the config file must contain the necessary MAFToGVCFPLugin parameters\ndocker run --name anchorwave_assemblies_maf --rm \\\n    -v ${WORKING_DIR}/:/phg/ \\\n    -t maizegenetics/phg:latest \\\n    /tassel-5-standalone/run_pipeline.pl -Xmx100G -debug -configParameters ${DOCKER_CONFIG_FILE} \\\n    -MAFToGVCFPlugin -endPlugin\n</code></pre> <p>The plugin may also be run directly from tassel-5-standalone as per the command below.  In this example, parameters are explicitly defined, but they may alternately be pulled from a config file as with the example above.</p> <pre><code>/workdir/lcj34/tassel-5-standalone/run_pipeline.pl -Xmx100G -debug -MAFToGVCFPlugin -referenceFasta /workdir/lcj34/phg_nam_assemblies/B73/Zm-B73-REFERENCE-NAM-5.0.fa -mafFile /workdir/lcj34/MAF_files/-CML108_B73.maf -sampleName CML108_anchorwave -gvcfOutput /workdir/lcj34/GVCF_output/CML108ToB73.gvcf -fillGaps false &gt; CML108_outputMafToGVCF.txt\n</code></pre> <p>If running with a config file, the parameters listed below should be defined (replace values with appropriate values for your own run).  Note the MAFToGVCFPlugin does not require database access.  This plugin neither reads not writes to a database.</p> <pre><code>MAFToGVCFPlugin.referenceFasta=/phg/inputDir/reference/Zm-B73-REFERENCE-NAM-5.0.fa\nMAFToGVCFPlugin.mafFile=/phg/outputDir/align/CML108_B73.maf\nMAFToGVCFPlugin.gvcfOutput=/phg/inputDir/loadDB/gvcfs/CML108ToB73.gvcf\nMAFToGVCFPlugin.sampleName=CML108_anchorwave\nMAFToGVCFPlugin.fillGaps=false\nMAFToGVCFPlugin.twoGvcfs=false\n\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_MAFToGVCFPluginDetails/#details","title":"Details","text":"<p>This plugin takes as input a UCSC Multiple Alignment Format (MAF) file and creates a GVCF file from the MAF data.  It keeps track of SNP, indel and reference positions.  When available, it also keeps track of the assembly chromosome, the assembly start and end positions, and the assembly strand from the alignment.  For MAF files created from diploid alignments, it will create 2 gvcfs.  This is indicated when the user sets the \"twoGvcfs\" flag to \"true\".  When creating 2 gvcf files, _1 and _2 will be appended to the filename.</p> <p>While this plugin should work to create a GVCF file from any correctly formatted MAF file, its intended use is for creating GVCF files from anchorwave created MAF files.  These GVCF files would then be loaded into a PHG database.  Because of this, the code requires that only a reference and a single sample are included in the MAF file.  It will ignore any MAF block lines except the \"a\" denoting the start of a MAF block, and the first 2 \"s\" lines, denoting the reference and single sample sequence lines.  Any MAF block \"i\", \"e\" and \"q\" lines are ignored.</p>"},{"location":"UserInstructions/CreatePHG_step2_MAFToGVCFPluginDetails/#parameters","title":"Parameters","text":"<p>Run the following command to print a current list of parameters for this plugin:</p> <pre><code>docker run --rm  maizegenetics/phg /tassel-5-standalone/run_pipeline.pl -MAFToGVCFPlugin\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_MAFToGVCFPluginDetails/#parameter-descriptions","title":"Parameter Descriptions","text":"<ul> <li>gvcfOutput: (required) Output GVCF file. If generating two output files, _1 and _2 will be appended to the filename.</li> <li>mafFile: (required) Input MAF file.  Please note that this needs to be a MAF file with 2 samples. The first will be assumed to be the Reference and the second will be the assembly.</li> <li>referenceFasta: (required) Full path to the reference fasta file</li> <li>sampleName: (required ) Sample name to write to the GVCF file.  This is also the name that will later be written to the database.</li> <li>fillGaps: (optional: default=false) When true, if the maf file does not fully cover the reference genome any gaps in coverage will be filled in with reference blocks. This is necessary if the resulting GVCFs are to be combined.</li> <li>twoGvcfs: (optional: default=false) The input maf was created from a diploid alignment and should be used to create two separate gVCF files.</li> <li>outputJustGT (optional: default=false) Output just the GT flag.  If set to false(default) will output DP, AD and PL fields.</li> <li>outputType (optional: default=gvcf) Output GVCF typed files. If set to gvcf(default) it will send all REFBlocks, SNPs and Indels.  If set to vcf, it will only output SNPs, Indels and missing values(if fillGaps = true.</li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_addHaplotypesFromWGS/","title":"Quick Start","text":"<p>The simplest way to load haplotypes from whole genome sequence (WGS : fastq, BAM, or GVCF) is to use the PopulatePHGDBPipelinePlugin.  The parameter descriptions in the details section provide information about how the parameters are used and which ones you will need to set. If your life is not complicated enough or you need more control over the process, you can run the scripts used by this plugin on your own as described in Details. </p>"},{"location":"UserInstructions/CreatePHG_step2_addHaplotypesFromWGS/#details","title":"Details","text":"<p>The scripts used by the plugin and distributed as part of the PHG Docker are CreateHaplotypesFromFastq.groovy,  CreateHaplotypesFromBAM.groovy, and CreateHaplotypesFromGVCF.groovy. These scripts can also be found in the PHG source code repository.</p> <p>To align sequence from fastq files to the reference genome and load the resulting haplotypes to the database, use the script CreateHaplotypesFromFastq.groovy. This script aligns the reads to created BAM files then calls CreateHaplotypesFromBAM.groovy, which in turn calls CreateHaplotypesFromGVCF.groovy. To start with BAM files run CreateHaplotypesFromBAM.groovy. To start with GVCF files, run CreateHaplotypesFromGVCF.groovy</p> <p>Because the BAM files can be quite large, you may wish to delete them after the GVCFs are created. Saving the GVCFs is worthwhile should it be necessary to rebuild the database.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHaplotypesFromWGS/#sentieon","title":"Sentieon","text":"<p>Sentieon is a commercial version of GATK that runs much faster. By default, GATK HaplotypeCaller is used to call variants from BAM files. If a Sentieon license is provided in the config file, Sentieon will be used.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHaplotypesFromWGS/#parameters","title":"Parameters","text":"<p>The following are parameters that can or be assigned in the config file. Some of the parameters are required and do not have default values.  The default paths will work with the Docker PHG when the MakeDefaultDirectory plugin is used to create a directory structure. Running CreateHaplotypesFromFastq.groovy uses all of the parameters. Running CreateHaplotypesFromBAM.groovy uses parameters for that script plus CreateHaplotypesFromGVCF.groovy. Running CreateHaplotypesFromGVCF.groovy uses only CreateHaplotypesFromGVCF parameters.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHaplotypesFromWGS/#createhaplotypesfromfastqgroovy","title":"CreateHaplotypesFromFastq.groovy","text":"Name Default Description referenceFasta required full path to the reference fasta file wgsKeyFile required the key file listing the fastq's to be processed numThreads 10 the maximum number of threads to be used picardPath /picard.jar The path to picard.jar fastqFileDir /phg/inputDir/loadDB/fastq/ the directory holding the fastq files tempFileDir /phg/inputDir/loadDB/temp/ the storage location for temporary files created by the script dedupedBamDir /phg/inputDir/loadDB/bam/dedup/ the stroace location for the deduped BAM files created by the script"},{"location":"UserInstructions/CreatePHG_step2_addHaplotypesFromWGS/#createhaplotypesfrombamgroovy","title":"CreateHaplotypesFromBAM.groovy","text":"Name Default Description referenceFasta required full path to the reference fasta file numThreads 10 the maximum number of threads to be used gvcfFileDir /phg/inputDir/loadDB/gvcf/ directory where gvcf files will be written tempFileDir /phg/inputDir/loadDB/bam/temp/ directory where temporary files will be written filteredBamDir /phg/inputDir/loadDB/bam/mapqFiltered/ directory where filtered BAMs will be written dedupedBamDir /phg/inputDir/loadDB/bam/dedup/ directory where deduped BAMs will be written Xmx 10G Max Java Heap Space used when running TASSEL code extendedWindowSize 1000 adds flanking regions (base pairs) to reference ranges for BAM filtering tasselLocation /tassel-5-standalone/run_pipeline.pl location of TASSEL, default works for docker mapQ 48 minimum MapQ value used in the BAM file filtering sentieon_license optional the sentieon license if you have one sentieonPath /sentieon/bin/sentieon the path to the Sentieon executable gatkPath /gatk/gatk the path to the gatk executable"},{"location":"UserInstructions/CreatePHG_step2_addHaplotypesFromWGS/#createhaplotypesfromgvcfgroovy","title":"CreateHaplotypesFromGVCF.groovy","text":"Name Default Description refRangeMethods refRegionGroup tempFileDir /phg/inputDir/loadDB/bam/temp/ tasselLocation /tassel-5-standalone/run_pipeline.pl extendedWindowSize 1000 Xmx 10G"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/","title":"Separate genome intervals into groups","text":""},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#quick-start","title":"Quick Start","text":"<ol> <li>Change <code>param1</code>, <code>param2</code>, <code>param3</code>, and <code>param4</code> to match file paths on your computer.</li> <li>Run </li> </ol> <pre><code>#!bash\n\nCreateHaplotypesFromBAM.groovy -[hd] -config [dbConfigFile]\n</code></pre> <p>Command line flags</p> <pre><code>#!bash\n\n[-h, -help] 'Show usage information'\n[-d, -description] 'Show information about this Pipeline'\n[-config] 'DB Config File(required)'\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#details","title":"Details","text":"<p>This step is part of a pipeline that allows you to load Haplotypes into the Database. Currently we support inputs from WGS fastq files, BAM files of WGS reads aligned to a reference and a GVCF file. This step adds data from a BAM file.</p> <p>Steps to the full pipeline:</p> <ol> <li>Check to see if the reference is BWA and fai indexed</li> <li>If not, index using bwa, samtools, and picard</li> <li>Align WGS fastq files to the reference using bwa mem</li> <li>Sort and MarkDuplicates in the output BAM file</li> <li>Filter the BAM file based on Minimum MapQ</li> <li>Run HaplotypeCaller using GATK or Sentieon</li> <li>Filter the GVCF file</li> <li>Extract out the reference ranges and load the haplotypes to the database</li> </ol> <p>This page describes the CreateHaplotypesFromBAM.groovy script, which runs steps 4-7.</p> <p>If you have data from multiple formats, we suggest running CreateHaplotypesFromFastq.groovy first on all fastq files, then CreateHaplotypesFromBAM.groovy on all BAM files, and finally CreateHaplotypesFromGVCF.groovy on all GVCF files.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#kitchen-sink","title":"Kitchen Sink","text":"<p>This Groovy Script will create haplotypes originating from BAM Files specified within a keyFile.  This groovy script can be run by itself, but it is also called from CreateHaplotypesFromFastq.groovy.  This expects BAM files where WGS short reads have been aligned to a reference using BWA MEM.</p> <p>Note only records with type BAM and GVCF will be processed by this script.  BAM records will be run though HaplotypeCaller and filtered.  Then the GVCFs coming out of HaplotypeCaller will be added to the DB along with the GVCF records in the keyfile.</p> <p>This Groovy Script will open up the keyFile(location is specified as the LoadHaplotypesFromGVCFPlugin.keyFile entry in the config file) and will loop over all FASTQ records and run BWA MEM to create BAM files. Then all BAM records in the keyfile and those output by BWA MEM are run through GATK/Sention's HaplotypeCaller. If multiple BAM files are stored for a given taxon, they will all be input for that taxon's HaplotypeCaller run. The resulting GVCFs will then be filtered and uploaded along with GVCF keyfile records to the DB. If multiple GVCFs are stored for a given taxon, each GVCF will be treated as an independent haplotype.</p> <p>This CreateHaplotypes Script will need some configuration parameters set in order to work correctly. You must set referenceFasta, LoadHaplotypesFromGVCFPlugin.keyFile, LoadHaplotypesFromGVCFPlugin.gvcfDir, LoadHaplotypesFromGVCFPlugin.referenceFasta, LoadHaplotypesFromGVCFPlugin.haplotypeMethodName.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#file-directories","title":"File Directories.","text":"<ul> <li>gvcfFileDir=/tempFileDir/data/gvcfs/<ul> <li>The Optional Output GVCF file Directory.  If you need to keep these files, mount a local drive to this location</li> </ul> </li> <li>tempFileDir=/tempFileDir/data/bam/temp/<ul> <li>The Temp file directory.  This Location is used to make any intermediate files when processing.</li> </ul> </li> <li>filteredBamDir=/tempFileDir/data/bam/filteredBAMs/<ul> <li>The Optional Output filtered file Directory. After Filtering the BAM files by MapQ, the files are written here.  Mount a local drive to this location if you need the bams.</li> </ul> </li> <li>dedupedBamDir=/tempFileDir/data/bam/DedupBAMs/<ul> <li>After filtering and Deduplication, the BAM files are stored here. This will be where the script is expecting the input BAM files.</li> </ul> </li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#tassel-parameters","title":"TASSEL parameters","text":"<ul> <li>Xmx=10G<ul> <li>Max Java Heap Space used when running TASSEL code.</li> </ul> </li> <li>tasselLocation=/tassel-5-standalone/run_pipeline.pl<ul> <li>Location of TASSEL on machine.  If using PHG docker this is the correct location</li> </ul> </li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#phg-createhaplotypes-parameters","title":"PHG CreateHaplotypes Parameters","text":"<ul> <li>referenceFasta<ul> <li>Reference fasta file location.  This is Required. Note, if using Docker, this needs to be the Docker specific path to the reference file.  You will need to mount these files to the Docker in order for it to work.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.keyFile<ul> <li>Location of the keyfile. This is Required. Note, if using Docker, this needs to be the Docker specific path to the keyfile.  You will need to mount this file to the Docker in order for it to work.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.gvcfDir<ul> <li>Directory of the GVCF files you wish to upload. This is Required. Note, if using Docker, this needs to be the Docker specific path to the gvcfs.  You will need to mount these files to the Docker in order for it to work. Note: This needs to match gvcfFileDir in the config file.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.referenceFasta<ul> <li>This should be set to the same location as referenceFasta. This is Required.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.haplotypeMethodName=GATK_PIPELINE<ul> <li>This is the method name which you are going to write to the DB.  This is Required. If you attempt to upload the same gvcfs and do not change this an error will be thrown.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.haplotypeMethodDescription<ul> <li>This is the description for the haplotypeMethodName.  If this is not set and empty description will be written to the DB.  It is strongly suggested that you put something in this field.</li> </ul> </li> <li>extendedWindowSize=1000<ul> <li>This Script will extract out the Focused reference ranges from the Database into a BED file.  When doing the BAM file filtering, we allow the user to add flanking regions around each of the regions.</li> </ul> </li> <li>mapQ=48<ul> <li>Minimum MapQ value allowed in the BAM file filtering.  This is to reduce the number of reads which map to multiple locations across the genome.</li> </ul> </li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#gatk-and-sentieon-parameters","title":"GATK and Sentieon Parameters","text":"<ul> <li>gatkPath=/gatk/gatk<ul> <li>Location of GATK on this system.  This is the location within the PHG docker</li> </ul> </li> <li>numThreads=10<ul> <li>Number of threads requested to run HaplotypeCaller.  </li> </ul> </li> <li>sentieon_license<ul> <li>If you have access to Sentieon and wish to use it, provide the sentieon license here.  The Script will automatically attempt to use Sentieon if this parameter is specified.</li> </ul> </li> <li>sentieonPath=/sentieon/bin/sentieon<ul> <li>This is the Docker specific path to sentieon.  If sentieon is installed somewhere else on your system, change this parameter.</li> </ul> </li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#details-on-running-this-step-with-wrapper-scripts","title":"Details on running this step with wrapper scripts","text":"<p>When running this step on the command line, all file paths and parameters are set in the config file. If you would like to overwrite the parameters set in the config file, you can do that by setting the parameters on the command line directly.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#details-on-running-this-step-through-docker","title":"Details on running this step through docker","text":"<p>An example Docker script to run the CreateHaplotypesFromFastq.groovy script is:</p> <pre><code>#!bash\nREF_DIR=/workdir/user/DockerTuningTests/InputFiles/Reference/\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nCONFIG_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/config.txt\nCONFIG_FILE_IN_DOCKER=/tempFileDir/data/config.txt\nDEDUP_BAM_DIR=/workdir/user/DockerTuningTests/InputFiles/WGSBams/\nFILTERED_BAM_DIR=/workdir/user/DockerTuningTests/InputFiles/WGSBams_Filtered/\nGVCF_OUTPUT_DIR=/workdir/user/DockerTuningTests/DockerOutput/gvcfOut/\nKEY_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/keyfile.txt\nKEY_FILE_IN_DOCKER=/tempFileDir/data/keyFile.txt\n\ndocker run --name small_seq_test_container --rm \\\n        -w / \\\n        -v ${REF_DIR}:/tempFileDir/data/reference/ \\\n        -v ${DB}:/tempFileDir/outputDir/phgSmallSeq.db \\\n        -v ${CONFIG_FILE}:${CONFIG_FILE_IN_DOCKER} \\\n        -v ${GVCF_OUTPUT_DIR}:/tempFileDir/data/gvcfs/ \\\n        -v ${DEDUP_BAM_DIR}:/tempFileDir/data/bam/DedupBAMs/ \\\n        -v ${FILTERED_BAM_DIR}:/tempFileDir/data/bam/filteredBAMs/ \\\n        -v ${KEY_FILE}:${KEY_FILE_IN_DOCKER} \\\n        -t maizegenetics/phg /CreateHaplotypesFromBAM.groovy -config ${CONFIG_FILE_IN_DOCKER}\n\n</code></pre> <p>Note that /tempFileDir/data/gvcfs/ and /tempFileDir/data/bam/filteredBAMs/ are locations for intermediate files to be stored.  These are not required, but if you wish to keep the GVCFs and/or the filtered BAM files you should mount these locations otherwise they will not persist.  If you would like to change the docker locations for these files, simply provide a gvcfFileDir and filteredBamDir config parameter in the config file and the script will write the files to that location.</p> <p>Also note the -w / parameter. This is needed to guarantee that the script will run correctly. When running a normal docker, this is likely not needed, but if running on a system like cbsu, the working directory needs to be set to the root directory.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#files","title":"Files","text":"<p>Haplotype keyfile</p> <p>Information on the keyfile format is here: Haplotype keyfile </p> <p>Reference fasta</p> <p>The reference genome in fasta file format</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#steps","title":"Steps","text":""},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#filter-bams","title":"Filter BAMs","text":"<p>Extract the BED file from the database for filtering and use with HaplotypeCaller. A BED file with extended anchors will be used to filter the BAM file.</p> <p>Filter BAM files based on user-defined MapQ score and intervals contained in BED file. Re-index the filtered file.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#run-gatk-haplotypecaller-or-sentieon-haplotypecaller","title":"Run GATK HaplotypeCaller or Sentieon HaplotypeCaller","text":"<p>Run HaplotypeCaller with Sentieon if Sentieon license is present in config file, otherwise with GATK. The BED file without extended anchors is used with this step.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#run-gvcf-filtering","title":"Run GVCF filtering","text":"<p>Reheader the GVCF to make sure it matches the name of the taxon, then filter.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#pass-on-to-createhaplotypesfromgvcfgroovy","title":"Pass on to CreateHaplotypesFromGVCF.groovy","text":""},{"location":"UserInstructions/CreatePHG_step2_addHapsFromBAM/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Make sure you have write permissions for all directories.</li> </ol> <p>Return to Step 2 pipeline, version 0.0.40-</p> <p>Return to Step 2 pipeline, version 1.0+</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/","title":"Align WGS fastq files to reference genome","text":""},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#quick-start","title":"Quick Start","text":"<p>After making sure that config file parameters match paths on your computer, run</p> <pre><code>#!bash\n\nCreateHaplotypesFromFastq.groovy -[hd] -config [dbConfigFile]\n</code></pre> <p>Command line flags</p> flag description -h, -help Show usage information -d, -description Show information about this pipeline -config  DB config file (required)"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#details","title":"Details","text":"<p>This step allows you to load Haplotypes into the Database. Currently we support inputs from WGS fastq files, BAM files of WGS reads aligned to a reference, or a GVCF file.</p> <p>Steps to the full pipeline:</p> <ol> <li>Check to see if the reference is BWA and fai indexed</li> <li>If not, index using bwa, samtools, and picard</li> <li>Align WGS fastq files to the reference using bwa mem</li> <li>Sort and MarkDuplicates in the output BAM file</li> <li>Filter the BAM file based on Minimum MapQ</li> <li>Run HaplotypeCaller using GATK or Sentieon</li> <li>Filter the GVCF file</li> <li>Extract out the reference ranges and load the haplotypes to the database</li> </ol> <p>This page describes the CreateHaplotypesFromFastq.groovy script, which runs all 7 steps.</p> <p>If you have data from multiple formats, we suggest running CreateHaplotypesFromFastq.groovy first on all fastq files, then CreateHaplotypesFromBAM.groovy on all BAM files, and finally CreateHaplotypesFromGVCF.groovy on all GVCF files.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#kitchen-sink","title":"Kitchen Sink","text":"<p>This Groovy Script will create haplotypes originating from a FastqFile specified in a keyfile. This groovy script can be run by itself outside of the docker, but you will need to be sure to set up the output directories correctly. This script's steps can be viewed here: CreateHaplotypesFromFastq.groovy If you also have some BAM files aligned for the same taxon you can include these as well.</p> <p>Note only records with type FASTQ, BAM and GVCF will be processed by this script. FASTQ records will be aligned to the the reference using BWA MEM.  Then all output BAM files along with the keyfile BAM records will be run though HaplotypeCaller and filtered. Then the GVCFs coming out of HaplotypeCaller will be added to the DB along with the GVCF records in the keyfile.</p> <p>This Groovy Script will open up the keyFile(location is specified as the LoadHaplotypesFromGVCFPlugin.keyFile entry in the config file) and will loop over all FASTQ records and run BWA MEM to create BAM files. Then all BAM records in the keyfile and those output by BWA MEM are run through GATK/Sention's HaplotypeCaller. If multiple BAM files are stored for a given taxon, they will all be input for that taxon's HaplotypeCaller run. The resulting GVCFs will then be filtered and uploaded along with GVCF keyfile records to the DB. If multiple GVCFs are stored for a given taxon, each GVCF will be treated as an independent haplotype.</p> <p>There are many parameters used by this step, all of which can be set in the config file.  The paths here assume the user has setup directory via MakeDefaultDirectoryPlugin:</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#file-directories","title":"File Directories.","text":"<ul> <li>gvcfFileDir=/phg/inputDir/loadDB/gvcf/<ul> <li>The Output GVCF file Directory.  If you need to keep these files, mount a local drive to this location</li> </ul> </li> <li>tempFileDir=/phg/inputDir/loadDB/temp/<ul> <li>The Temp file directory.  This Location is used to make any intermediate files when processing.</li> </ul> </li> <li>filteredBamDir=/phg/inputDir/loadDB/bam/filteredBAMs/<ul> <li>After Filtering the BAM files by MapQ, the files are written here.  Mount a local drive to this location if you need the bams.</li> </ul> </li> <li>dedupedBamDir=/phg/inputDir//loadDB/bam/DedupBAMs/<ul> <li>After filtering and Deduplication, the BAM files are stored here.  It is strongly suggested to mount to this location as BWA alignment does take some time.</li> </ul> </li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#tassel-parameters","title":"TASSEL parameters","text":"<ul> <li>Xmx=10G<ul> <li>Max Java Heap Space used when running TASSEL code.</li> </ul> </li> <li>tasselLocation=/tassel-5-standalone/run_pipeline.pl<ul> <li>Location of TASSEL on machine.  If using PHG docker this is the correct location</li> </ul> </li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#phg-createhaplotypes-parameters","title":"PHG CreateHaplotypes Parameters","text":"<ul> <li>referenceFasta<ul> <li>Reference fasta file location.  This is Required. Note, if using Docker, this needs to be the Docker specific path to the reference file.  You will need to mount these files to the Docker in order for it to work.</li> </ul> </li> <li>wgsKeyFile<ul> <li>Location of the keyfile. This is Required. Note, if using Docker, this needs to be the Docker specific path to the keyfile.  You will need to mount this file to the Docker in order for it to work.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.gvcfDir<ul> <li>Directory of the GVCF files you wish to upload. This is Required. Note, if using Docker, this needs to be the Docker specific path to the gvcfs.  You will need to mount these files to the Docker in order for it to work. Note: This needs to match gvcfFileDir in the config file.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.referenceFasta<ul> <li>This should be set to the same location as referenceFasta. This is Required.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.haplotypeMethodName=GATK_PIPELINE<ul> <li>This is the method name which you are going to write to the DB.  This is Required. If you attempt to upload the same gvcfs and do not change this an error will be thrown.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.haplotypeMethodDescription<ul> <li>This is the description for the haplotypeMethodName.  If this is not set and empty description will be written to the DB.  It is strongly suggested that you put something in this field.</li> </ul> </li> <li>extendedWindowSize=1000<ul> <li>This Script will extract out the Focused reference ranges from the Database into a BED file.  When doing the BAM file filtering, we allow the user to add flanking regions around each of the regions.</li> </ul> </li> <li>mapQ=48<ul> <li>Minimum MapQ value allowed in the BAM file filtering.  This is to reduce the number of reads which map to multiple locations across the genome.</li> </ul> </li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#gatk-and-sentieon-parameters","title":"GATK and Sentieon Parameters","text":"<ul> <li>gatkPath=/gatk/gatk<ul> <li>Location of GATK on this system.  This is the location within the PHG docker</li> </ul> </li> <li>numThreads=10<ul> <li>Number of threads requested to run HaplotypeCaller.  </li> </ul> </li> <li>sentieon_license<ul> <li>If you have access to Sentieon and wish to use it, provide the sentieon license here.  The Script will automatically attempt to use Sentieon if this parameter is specified.</li> </ul> </li> <li>sentieonPath=/sentieon/bin/sentieon<ul> <li>This is the Docker specific path to sentieon.  If sentieon is installed somewhere else on your system, change this parameter.</li> </ul> </li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#details-on-running-this-step-with-wrapper-scripts","title":"Details on running this step with wrapper scripts","text":"<p>When running this step on the command line, all file paths and parameters are set in the config file. If you would like to overwrite the parameters set in the config file, you can do that by setting the parameters on the command line directly.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#details-on-running-this-step-through-docker","title":"Details on running this step through docker","text":"<p>An example Docker script to run the CreateHaplotypesFromFastq.groovy script is:</p> <pre><code>#!bash\nREF_DIR=/workdir/user/DockerTuningTests/InputFiles/Reference/\nFASTQ_DIR=/workdir/user/DockerTuningTests/InputFiles/WGSFastq/\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nCONFIG_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/config.txt\nCONFIG_FILE_IN_DOCKER=/tempFileDir/data/config.txt\nGVCF_OUTPUT_DIR=/workdir/user/DockerTuningTests/DockerOutput/gvcfOut/\nKEY_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/keyfile.txt\nKEY_FILE_IN_DOCKER=/tempFileDir/data/keyFile.txt\n\n\ndocker run --name small_seq_test_container --rm \\\n        -w / \\\n        -v ${REF_DIR}:/tempFileDir/data/reference/ \\\n        -v ${FASTQ_DIR}:/tempFileDir/data/fastq/ \\\n        -v ${DB}:/tempFileDir/outputDir/phgSmallSeq.db \\\n        -v ${CONFIG_FILE}:${CONFIG_FILE_IN_DOCKER} \\\n        -v ${GVCF_OUTPUT_DIR}:/tempFileDir/data/gvcfs/ \\\n        -v ${KEY_FILE}:${KEY_FILE_IN_DOCKER} \\\n        -t maizegenetics/phg /CreateHaplotypesFromFastq.groovy -config ${CONFIG_FILE_IN_DOCKER}\n\n\n</code></pre> <p>Note the -w / parameter.  This is needed to guarantee that the script will run correctly.  When running a normal docker, this is likely not needed, but if running on a system like cbsu, the working directory needs to be set to the root directory.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#files","title":"Files","text":"<p>Haplotype keyfile</p> <p>Information on the keyfile format is here: Haplotype keyfile </p> <p>Reference fasta</p> <p>The reference genome in fasta file format</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#steps","title":"Steps","text":""},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#index-reference-fasta","title":"Index reference fasta","text":"<p>BWA, samtools, and picard are used to index the reference fasta if index files are not already present.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#align-fastq-files-using-bwa-mem","title":"Align fastq files using bwa mem","text":"<p>Fastq files are read from the fastq directory are aligned to the reference genome. After all are aligned the script verifies that BAM files for all taxa exist. The BAM is indexed and Picard is used run to MarkDuplicates.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#call-the-createhaplotypesfrombamgroovy-script","title":"Call the CreateHaplotypesFromBAM.groovy script","text":""},{"location":"UserInstructions/CreatePHG_step2_addHapsFromFastq/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Check that paths and filenames are spelled correctly in the config file.</li> <li>Make sure the key file is formatted correctly and included in the config file.</li> </ol> <p>Return to Step 2 pipeline, version 1.0</p> <p>Return to Step 2 pipeline, version 0.0.40</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/","title":"Filter GVCF and add variants to database","text":""},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/#quick-start","title":"Quick Start","text":"<ol> <li>Change <code>param1</code>, <code>param2</code>, <code>param3</code>, and <code>param4</code> to match file paths on your computer.</li> <li>Run </li> </ol> <pre><code>#!bash\n\nCreateHaplotypesFromGVCF.groovy -[hd] -config [dbConfigFile]\n</code></pre> <p>Command line flags</p> <pre><code>#!bash\n\n[-h, -help] 'Show usage information'\n[-d, -description] 'Show information about this Pipeline'\n[-config] 'DB Config File(required)'\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/#details","title":"Details","text":"<p>This step is part of a pipeline that allows you to load Haplotypes into the Database. Currently we support inputs from WGS fastq files, BAM files of WGS reads aligned to a reference and a GVCF file. This step adds data from a GVCF file.</p> <p>Steps to the full pipeline:</p> <ol> <li>Check to see if the reference is BWA and fai indexed</li> <li>If not, index using bwa, samtools, and picard</li> <li>Align WGS fastq files to the reference using bwa mem</li> <li>Sort and MarkDuplicates in the output BAM file</li> <li>Filter the BAM file based on Minimum MapQ</li> <li>Run HaplotypeCaller using GATK or Sentieon</li> <li>Filter the GVCF file</li> <li>Extract out the reference ranges and load the haplotypes to the database</li> </ol> <p>This page describes the CreateHaplotypesFromBAM.groovy script, which runs step 7.</p> <p>If you have data from multiple formats, we suggest running CreateHaplotypesFromFastq.groovy first on all fastq files, then CreateHaplotypesFromBAM.groovy on all BAM files, and finally CreateHaplotypesFromGVCF.groovy on all GVCF files.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/#kitchen-sink","title":"Kitchen Sink","text":"<p>This Groovy Script will create haplotypes originating from a set of filtered GVCF files specified in a keyfile. This groovy script can be run by itself, but it is also called from CreateHaplotypesFromBAM.groovy (source).  Basically if you already have GVCFs that are filtered to your liking and are ready to be uploaded, you should use this script.</p> <p>A GVCF file contains more information than a regular VCF file. We use the GATK GVCF format for the PHG.</p> <p>Note only records with type GVCF will be processed by this plugin.</p> <p>If you need to filter, either use an external tool like bcftools or vcftools.  Alternatively you can use the FilterGVCFSingleFilePlugin in the PHG. The documentation for FilterGVCFPlugin describes the possible filters applied to the GVCF file.  </p> <p>For Maize, we set mapQ=48, DP_poisson_min=0.01, DP_poisson_max=0.99, GQ_min=50 and filterHets=t. NOTE THESE ARE FOR MAIZE WGS WITH GOOD COVERAGE &gt;5X.  These can be used as starting points, but you should not follow these values blindly.  </p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/#we-strongly-suggest-you-use-an-external-tool-so-you-know-exactly-what-filters-are-applied-to-your-gvcf-file","title":"We strongly suggest you use an external tool so you know exactly what filters are applied to your GVCF file.","text":"<p>This Groovy Script will open up the keyFile(location is specified as the LoadHaplotypesFromGVCFPlugin.keyFile entry in the config file) and will loop over all GVCF records and upload to the DB. If multiple GVCFs are stored for a given taxon, each GVCF will be treated as an independent haplotype.</p> <p>This CreateHaplotypes Script will need some configuration parameters set in order to work correctly.  These must go in you config.txt file. You must set referenceFasta, LoadHaplotypesFromGVCFPlugin.keyFile, LoadHaplotypesFromGVCFPlugin.gvcfDir, LoadHaplotypesFromGVCFPlugin.referenceFasta, LoadHaplotypesFromGVCFPlugin.haplotypeMethodName.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/#file-directories","title":"File Directories.","text":"<ul> <li>tempFileDir=/tempFileDir/data/bam/temp/<ul> <li>The Temp file directory.  This Location is used to make any intermediate files when processing.  For this script, only the BED File will be extracted containing the Focused Reference Ranges setup in a previous step.</li> </ul> </li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/#tassel-parameters","title":"TASSEL parameters","text":"<ul> <li>Xmx=10G<ul> <li>Max Java Heap Space used when running TASSEL code.</li> </ul> </li> <li>tasselLocation=/tassel-5-standalone/run_pipeline.pl<ul> <li>Location of TASSEL on machine.  If using PHG docker this is the correct location</li> </ul> </li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/#phg-createhaplotypes-parameters","title":"PHG CreateHaplotypes Parameters","text":"<ul> <li>referenceFasta<ul> <li>Reference fasta file location.  This is Required. Note, if using Docker, this needs to be the Docker specific path to the reference file.  You will need to mount these files to the Docker in order for it to work.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.keyFile<ul> <li>Location of the keyfile. This is Required. Note, if using Docker, this needs to be the Docker specific path to the keyfile.  You will need to mount this file to the Docker in order for it to work.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.gvcfDir<ul> <li>Directory of the GVCF files you wish to upload. This is Required. Note, if using Docker, this needs to be the Docker specific path to the gvcfs.  You will need to mount these files to the Docker in order for it to work. </li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.referenceFasta<ul> <li>This should be set to the same location as referenceFasta. This is Required.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.haplotypeMethodName=GATK_PIPELINE<ul> <li>This is the method name which you are going to write to the DB.  This is Required. If you attempt to upload the same gvcfs and do not change this an error will be thrown.</li> </ul> </li> <li>LoadHaplotypesFromGVCFPlugin.haplotypeMethodDescription<ul> <li>This is the description for the haplotypeMethodName.  This is Required.</li> </ul> </li> <li>extendedWindowSize=1000<ul> <li>This Script will extract out the Focused reference ranges from the Database into a BED file.  When doing the BAM file filtering, we allow the user to add flanking regions around each of the regions.</li> </ul> </li> <li>numThreads=10<ul> <li>Number of threads requested to run HaplotypeCaller.  </li> </ul> </li> <li>refRangeMethods=refRegionGroup<ul> <li>This is used to extract a BED file out of the DB before the GVCF file is processed.  The BED file is then used to extract out regions of the GVCF used to become the haplotypes.  Typically, refRegionGroup refers to the anchor Reference ranges.  If \"refRegionGroup,refInterRegionGroup\" is used it will create a BED file representing both anchors and inter anchors.  We strongly suggest not setting this parameter in the Config File</li> </ul> </li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/#details-on-running-this-step-with-wrapper-scripts","title":"Details on running this step with wrapper scripts","text":"<p>When running this step on the command line, all file paths and parameters are set in the config file. If you would like to overwrite the parameters set in the config file, you can do that by setting the parameters on the command line directly.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/#details-on-running-this-step-through-docker","title":"Details on running this step through docker","text":"<p>An example Docker script to run the CreateHaplotypesFromFastq.groovy script is:</p> <pre><code>#!bash\n#Set these properties\nREF_DIR=/workdir/user/DockerTuningTests/InputFiles/Reference/\nGVCF_DIR=/workdir/user/DockerTuningTests/InputFiles/\nGVCF_DIR_IN_DOCKER=/tempFileDir/data/outputs/gvcfs/\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nCONFIG_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/config.txt\nCONFIG_FILE_IN_DOCKER=/tempFileDir/data/config.txt\nKEY_FILE=/workdir/user/DockerTuningTests/DataFolders/LoadRefDataDocker/keyfile.txt\nKEY_FILE_IN_DOCKER=/tempFileDir/data/keyFile.txt\n\ndocker run --name small_seq_test_container --rm \\\n        -v ${REF_DIR}:/tempFileDir/data/reference/ \\\n        -v ${GVCF_DIR}:${GVCF_DIR_IN_DOCKER} \\\n        -v ${DB}:/tempFileDir/outputDir/phgTestMaizeDB.db \\\n        -v ${CONFIG_FILE}:${CONFIG_FILE_IN_DOCKER} \\\n        -v ${KEY_FILE}:${KEY_FILE_IN_DOCKER} \\\n        -t maizegenetics/phg \\\n        /CreateHaplotypesFromGVCF.groovy \\\n            -config ${CONFIG_FILE_IN_DOCKER}\n\n</code></pre> <p>Note the GVCF_DIR_IN_DOCKER needs to also be specified in the config file under the Parameter:LoadHaplotypesFromGVCFPlugin.gvcfDir</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/#files","title":"Files","text":"<p>Haplotype keyfile</p> <p>Information on the keyfile format is here: Haplotype keyfile </p> <p>Reference fasta</p> <p>The reference genome in fasta file format</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/#steps","title":"Steps","text":""},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/#upload-gvcfs-to-database","title":"Upload GVCFs to Database","text":"<p>Pull out the BED file from the database if this script is not called from CreateHaplotypesFromBAM.groovy.</p> <p>Upload variants to the database for all intervals within the BAM file.</p>"},{"location":"UserInstructions/CreatePHG_step2_addHapsFromGVCF/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>If you are having problems with this step, first verify that your GVCF files are formatted correctly and that you have only a single sample per GVCF file.</li> </ol> <p>Return to Step 2 pipeline, version 0.0.40</p> <p>Return to Step 2 pipeline, version 0.1.0+</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/","title":"Align assemblies to reference genome","text":""},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/#instructions","title":"Instructions","text":""},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/#details","title":"Details","text":"<p>The load assembly step uses mummer4 scripts to align a reference fasta file to the assembly fasta file. This is done on a chromosome-by-chromosome basis, and the Quick Start pipeline automatically splits the reference fasta into single chromosomes for the alignment. The fasta ID line for these files should contain the chromosome name with a space between the chromosome name and any additional information on the ID line and chromosome names between the reference and assembly genomes must be consistent.</p> <p>The pipeline runs mummer4 nucmer, delta-filter, show-coords, and show-snps scripts and then processeses and reformats data for the PHG. </p>"},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/#parameters","title":"Parameters","text":"<p>Required parameters:</p> <ul> <li>configFile: DB Config File containing properties host,user,password,DB and DBtype where DBtype is either \"sqlite\" or \"postgres\". This file is expected to live in the folder mounted to Docker tempFileDir/data/.</li> <li>referenceFasta: The reference fasta file. This file is used when running Mummer4 alignment scripts. If running this step on the command line, this parameter is set in the config file directly. If running through the docker, this file is expected to live in the folder mounted to Docker /tempFileDir/data/reference. The SplitFastaByChromPlugin automatically separates the reference and assembly fastas into chromosomes, so the user does not need to do that.</li> <li>assemblyFastaDir: The directory containing all assembly fasta files to be added to the database. The SpliteFastaByChromPlugin automatically separates the reference and assembly fastas into chromosomes, so the user does not need to do that. The individual chromosome files are used to align against the respective reference chromosome when the mummer4 scripts are run. If running this step on the command line, this parameter is set in the config file. If running through the coker, this file is expected to be in the folder mounted to Docker path /tempFileDir/data/assemblyFasta.</li> <li>assemblyName: name to append to the output files that identifies the assembly processed. This is also the name that will be stored in the database genotypes table. To differentiate the assembly from other instances of taxa with this name, you may want to name it _Assembly. There cannot be duplicate assembly names in the database. <li>chromList: A list of chromosome names that will be processed. This must match the chromosome names as stored on the ID line in the fasta file, or can be just the chromosome number if the chromosome is listed as \"chr1\" or \"chromosome1\". Chromosome names must match between the assembly and reference genomes. Note if leading 0's are added to the chromosome name, they must be consistently added in both assembly fasta, reference fasta, and this shell parameter.</li> <li>clusterSize: The minimum length for a cluster of matches used when running mummer4 nucmer program. Nucmer default is 65 but we have found when running maize assemblies 250 is faster and provides good results. That is the default we show below.</li> <li>assemblyQuality: Either 0 or 1 indicating assembly quality. If working with an assembly that has been scaffolded into chromosomes that approximately match chromsomes in the reference genome, set this parameter as 1. If working with an assembly that has been scaffolded but not built to chromosome-level, set this parameter to 0. There are two pipelines for adding assembly data to the PHG. The 1 flag for this parameter runs the highQualityAssemblyPlugin pipeline, while the 0 flag for this parameter runs the lowQualityAssemblyPlugin pipeline. </li> <li>addAssemblyMethod: Either \"parallel\" or \"sequential\". When there are multiple assemblies to add to the database, the PHG code can add them one by one or in parallel. The outcome is identical regardless of which step is used, but memory usage and timings will differ.</li> <li>keyfile: The keyfile must be a tab-delimited file with 1 row for each per-chromosome assembly fasta you wish to align and load to the database. The required columns for the keyfile are RefDir, RefFasta AssemblyDir, AssemblyFasta, AssemblyDBName and Chromosome. Example assembly keyfiles for a single taxon or for multiple taxa. The column descriptions are below.<ul> <li>RefDir: full path to the directory containing the reference fasta file.</li> <li>RefFasta: name of the reference fasta file to be aligned to the assembly fasta file in this row. This should be a fasta file at the chromosome level.</li> <li>AssemblyDir: full path to the directory containing the assembly fasta files.</li> <li>AssemblyFasta: name of the assembly fasta file to be aligned to the reference fasta file in this row. This should be a fasta file at the chromosome level.</li> <li>AssemblyDBName: the name for this assembly to add as the \"line_name\" to the database's genotype table.</li> <li>Chromosome: the chromosome name that is being processed. This name must match a reference chromosome that was loaded with the LoadGenomeIntervals.sh script.</li> </ul> </li> <p>Optional parameters:</p> <ul> <li>mummer4Path: If the mummer4 executables exist in a path other than /mummer/bin, then provide that path via this parameter. If you are running in the PHG docker, the path will be /mummer/bin and this parameter is not necessary.</li> <li>clusterSize: This is a parameter to mummer4's nucmer alignment program. It sets the minimum length for a cluster of matches. We have found the value of 250 provides good coverage with acceptable speed.If you wish a different value, set this parameter.</li> <li>minInversionLen: Minimum length of inversion for it to be kept as part of the alignment. Default is 7500</li> <li>Boolean: true means load haplotypes to db, false means do not populate the database. Useful if the user is running mummer4 alignments on multiple machines and plans later to put all mummer4 files on 1 machine to load to a single database. Defaults to true</li> <li>numThreads: Number of threads used to process assembly chromosomes. The code will subtract 2 from this number to get the number of worker threads. It leaves 1 thread for IO to the DB and 1 thread for the Operating System.</li> <li>entryPoint: The \"entryPoint\" parameter is provided for users who wish to speed up processing by running the mummer4 alignments on multiple machines. You may do this, then gather all the mummer files to a single machine. If you do this, set the loadDB flag to FALSE, run all your alignments, then gather them to a single machine for loading into the database. This parameter indicates at which point assembly processing should begin. If a step other than \"all\" is chosen, files normally created from the previous steps must be present in a sub-directory named \"align\" in your output directory. They must be named using the format shown below for the software to recognize them:<ul> <li>all: Run the entire pipeline.  The software creates all necessary files.</li> <li>deltafilter:  Assumes the nucmer aligning step has already been performed. Processing starts with mummer4 delta-filter step includes running mummer4 show-coords on both the original alignment and the filtered alignment as well as mummer4 show-snps.  The delta file must be available in a sub-directory named \"align\" in the directory mounted to docker's /tempFileDir/outputDir/.  The file name must be in the format:<ul> <li>ref___c.delta <li>ex:  ref_W22_1_c250.delta</li> <li>refilter: Assumes delta-filter and show-coords for both the delta and the delta-filter steps have been run. This step runs additional filtering to process overlapping alignments, remove embedded alignments and find a longest-path solution.Your output/align directory  must have the files for deltafilter plus:<ul> <li>ref___c.delta_filtered <li>ref___c.coords_orig <li>ref___c.coords_filtered haplotypes: This step takes the mummer4 output files and from them creates haplotype sequence fro the database.  If starting from this step, you must have the files from above plus: <li>ref___c.coords_filteredPlus_noEmbedded <li>ref___c.coords_final <li>ref___c.snps_prefiltered <li>ref___c.snps_final"},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/#details-on-running-this-step-with-wrapper-scripts","title":"Details on running this step with wrapper scripts","text":"<p>When running this step on the command line, all file paths and parameters are set in the config file. The only call that needs to be run in the terminal is <code>phg loadAssembly /path/to/config.txt</code>. If you would like to overwrite the parameters set in the config file, you can do that by setting the parameters on the command line directly.</p> <p>For example, to ignore the config file assembly name and set one directly, you could run:</p> <pre><code>phg loadAssembly -configFile /path/to/config.txt -assemblyName newAssembly1\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/#details-on-running-this-step-through-docker","title":"Details on running this step through docker","text":"<p>When LoadAssemblyAnchors is called as part of a Docker script, the Docker script expects these mount points:</p> <ul> <li>Mount localMachine:/pathToAssemblyFastas/ to docker:/tempFileDir/data/assemblyFasta</li> <li>Mount localMachine:/pathToDataDir/ to docker:/tempFileDir/data/</li> <li>Mount localMachine:/pathToOutputDir/ to docker:/tempFileDir/outputDir/</li> <li>Mount localMachine:/pathToReferenceFastaDir/ to docker:/tempFileDir/data/reference</li> </ul> <p>Note the script will create a subdirectory named \"align\" under the /tempFileDir/outputDir/ directory. This is where the mummer4 programs output will be written.</p> <p>All output from running this script will be written to files in the directory mounted to Docker /tempFileDir/outputDir/align.</p> <p>An example Docker script that would call this shell script is below.  Note: you need to change the mounted user directories to match your own directory configuration.  The docker directories (right side of the count pair beginning with \"tempFileDir\" need to remain as shown here.  The file names, e.g. \"config.txt\" should match the names of your files.</p> <p>This example runs through all 10 chromosomes for a particular assembly.  In this example, the assembly files are broken into fasta files name ph207chr1.fasta, ph297chr2.fasta, etc.  The reference fastas are broken by chromosome and are named b73chr1.fasta, b73chr2.fasta, etc.  You need to change those names to match your fasta file names.</p> <pre><code>#!python\n\n#!/bin/bash\nchromosomeList=${chromList}\n\nfor chrom in \"${chromosomeList[@]}\"\ndo\n\necho \"Starting chrom ${chrom} \"\n\ndocker run --name phg_assembly_container_chr${chrom} --rm \\\n        -v /workdir/user/mummer4_testing/output:/tempFileDir/outputDir/ \\\n        -v /workdir/user/mummer4_testing/data:/tempFileDir/data/ \\\n        -v /workdir/user/mummer4_testing/refFastas/:/tempFileDir/data/reference/ \\\n        -v /workdir/user/mummer4_testing/assemblyFastas/:/tempFileDir/data/assemblyFasta/ \\\n        -v /workdir/user/mummer4_testing/mummer_from_shellDocker/:/tempFileDir/outputDir/align/ \\\n        -t maizegenetics/phg:latest \\\n        /LoadAssemblyAnchors.sh configSQLite_docker.txt \\\n                b73chr${chrom}.fasta \\\n                ph207chr${chrom}.fasta \\\n                PH207_Assembly \\\n                ${chrom} \\\n                250\n\n\necho \"Finished chrom  ${chrom} \"\ndone\n</code></pre> <p>ParallelAssemblyAnchorsLoad runs similarly to LoadAssemblyAnchors. When ParallelAssemblyAnchorsLoad is called as part of a Docker script, the Docker script expects these mount points:</p> <ul> <li>Mount localMachine:/pathToDataDir/ to docker:/tempFileDir/data/</li> <li>Mount localMachine:/pathToOutputDir/ to docker:/tempFileDir/outputDir/</li> </ul> <p>The script will create a subdirectory called \"align\" under the mounted output directory where mummer4 alignment files will be stored.</p> <p>An example Docker script that would call this shell script is below.  Note: you need to change the mounted user directories to match your own directory configuration. The docker directories (right side of the count pair beginning with \"tempFileDir\") need to remain as shown here. The file names, e.g. \"config.txt\" should match the names of your files. And the \"config.txt\" and \"parallelKeyFile_B204.txt\" must live in the directory mounted to /tempFileDir/data/.</p> <pre><code>echo \"Starting taxa B104\"\n\ndocker1 run --name phg_assembly_container_B104 --rm \\\n        -v /workdir/${USER}/phg_assemblyParallel_test/DockerOutput/:/tempFileDir/outputDir/ \\\n        -v /workdir/${USER}/phg_assemblyParallel_test/DataFolders/:/tempFileDir/data/ \\\n        -v /workdir/${USER}/phg_nam_assemblies/B73/:/tempFileDir/referenceFastas/ \\\n        -v /workdir/${USER}/phg_nam_assemblies/B104/:/tempFileDir/assemblyFastas/ \\\n        -t phgrepository_test:latest \\\n        /ParallelAssemblyAnchorsLoad.sh config.txt \\\n                parallelKeyFile_B104.txt\n\n\necho \"Finished taxa 104 \"\n</code></pre> <p>Note: use docker1 if running on cbsu.</p> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The -t directive indicates the Docker image of which this container will be an instance.  The last line tells the Docker container to run the LoadAssemblyAnchors.sh script which is found in the root directory.  The items following are the parameters to the LoadAssemblyAnchors.sh script.</p>"},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/#files","title":"Files","text":"<p>Config file</p> <p>An example can be found here: Master config file</p> <p>Reference Fasta file</p> <p>Fasta file for the reference genome you want to use in the database</p> <p>Assembly Fasta file(s)</p> <p>Fasta files with genome assemblies you want to add to the PHG. These are read from the directory passed in the assemblyFastaDir parameter.</p> <p>Mummer4 output files</p> <p>Mummer3 output files are written to a directory called \"align\" in the output directory specified by the user. The output directory should be mounted to the Docker /tempFileDir/outputDir/align directory.</p> <p>Assembly key file</p> <p>The assembly key file must be a tab-delimited file with 1 row for each per-chromosome assembly fasta you wish to align and load to the database. The required columns for the keyfile are RefDir, RefFasta AssemblyDir, AssemblyFasta, AssemblyDBName and Chromosome. An example assembly keyfile for a single genome assembly can be found here. Note all fields are tab-delimited and because this is run in a docker, the reference and assembly directories are docker paths.</p> <p>An example key file that has per-chromosome pastas for multiple species is shown here. Note that the assembly chromosome fasta files must exist in the same directory.</p>"},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/#plugins","title":"Plugins","text":"<p>This step calls multiple plugins to add assemblies to the PHG. The plugins are called in the order SplitFastaByChromPlugin --&gt; CreateAssemblyAnchorsPlugin --&gt; AssemblyHaplotypesPlugin/AssemblyHaplotypesMultiThreadPlugin. AddRefRangeAssemblyPlugin is an optional step.</p> <p></p> <p>The assembly entries in the genotypes table will be defaulted to the following values:</p> <ul> <li>line_name:  the assembly name</li> <li>line_data: the mummer script parameters used with this version of PHG</li> <li>Ploidy:  1</li> <li>is_reference:  false</li> <li>isPhasedAcrossGenes: true</li> <li>isPhasesAcrossChromosomes: true</li> </ul> <p>The assembly entries in the gametes table will be defaulted to the following values:</p> <ul> <li>hapNumber: 0</li> <li>Confidence:  1</li> </ul> <p>The assembly entries in the methods table will be defaulted to the following values:</p> <ul> <li>method_type: DBLoadingUtils.MethodTYpe.ASSEMBLY_HAPLOTYPES</li> <li>name:  mummer4</li> <li>description:  the mummer script parameters used with this version of PHG ( same as genotypes:line_data</li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/#splitfastabychromplugin","title":"SplitFastaByChromPlugin","text":"<p>This plugin splits fasta by chromosome. It assumes the id line has a chromosome of the form &gt;1, &gt;chr1 or &gt;chromosome1. All of the above forms (case insensitive) will be written with an id line of just the number, e.g. &gt;1 or &gt;2.</p> <p>The \"name\" parameter is used as the basis of the name.  To this name will be appended the \"chr\" and chrom number and .fa. For example:  If the user gives \"w22\" as the name, the code will write files: w22chr1.fa, w22chr2.fa, etc.</p> <p>The isGca parameter: The assemblies starting with GCA have long text in the idLIne, with the chromosome stuck in the middle. This plugin will correctly parse these lines. Other weird id lines are not supported and may need to be corrected manually before running through this plugin.</p> <p>Seems each set of assemblies that arrives has a different signature for the idline. So I keep customizing. Consider this code to the \"base\" plugin.  On each run, if the idlines don't adhere to chr/chromosome/X or GCA, then user should run an altered version of this, or fix the idlines first.</p> <p>The parameters to this plugin are:</p> <ul> <li>-fasta  Fasta File to split by chromosome. (REQUIRED) <li>-name  Name to give each file, e.g w22.  To this name will be appended 'chr' plus the chrom number plus .fa (REQUIRED) <li>-outputDir  ath to write the split files (REQUIRED) <li>-isGca  GCA fastas have long text as idLines.  These fasta will have their id lines specially parsed to extract the chromosome number."},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/#createassemblyanchorsplugin","title":"CreateAssemblyAnchorsPlugin","text":"<p>This plugin takes an assembly genome fasta file, splits the sequence into contigs based on the presence of N's. Each contig begins at a defined allele (base of A,C,G or T) and ends when an N is encountered. N's are skipped, a new contig begins at the next non-N allele. The contig fasta is aligned against the reference genome using minimap2. Samtools mpileup command is used to generate a pileup of read bases aligning to the reference, limiting the pileup to the conserved region intervals. Bcftools are then used to create a gvcf file. Fastas created from the gvcf are loaded to the db via the LoadHapSequencesToDBPlugin.</p> <p>Assembly inter-anchor regions are not identified at this time.</p> <p>Tables populated via this method are:</p> <ul> <li>genotypes</li> <li>gametes</li> <li>gamete_groups</li> <li>methods</li> <li>haplotypes</li> <li>gamete_haplotypes</li> </ul> <p>The parameters to this plugin are:</p> <ul> <li>-ref   Reference Genome File for aligning against. (Default is null) (REQUIRED) <li>-genomeFile  Path to assembly genome fasta from which to pull sequence for aligning. <li>-genomeData  A tab-delimited file containing genome specific data with columns: Genotype Hapnumber Dataline Ploidy Reference GenePhased ChromPhased Confidence Method MethodDetails RefVersion.   <li>-outputDir   Path to output directory including trailing / that will be used when creating temporary output files. <li>-intervals  Path to BED formatted file containing the intervals used when creating the reference genome intervals. The positions are 0-based, inclusive/exclusive. This is used in mpileup. (Default is null) (REQUIRED) <li>-configFile  Path to config file containing DB parameters host, user, password, DB, type.  Used for making the database connection.  Type must be either \"sqlite\" or \"postgres\" to identify db type for connection. <p>The genome data file contents are described below:</p> <ul> <li>Genotype:  the nmae of the line as you want it to appear in the db genotypes table \"name\" column, e.g. \"B104\" or \"B104_haplotype_caller\"</li> <li>Hapnumber:  The 0-based chromosome number.  For inbreds, this is always 0.</li> <li>Dataline: Text description defining the line. This data will be stored in the \"description\" field of the genotypes table.</li> <li>Ploidy:  Number of chromosomes for this genome, the value to be stored in the \"ploidy\" field of the genome_intervals table.  Should be 1 for the reference.</li> <li>Reference:  a boolean field indicating if this genome is the reference (should be true for this plugin)</li> <li>GenePhased: a boolean field indicating if the genes are phased.  Should be false for the reference.</li> <li>ChromPhased:  a boolean field indicating if the chromosomes are phased.  Should be false for the reference.</li> <li>Confidence:  a float field indicating confidence in phasing.  Generally 1 when no phasing.</li> <li>Method:  a mathod name by which this version of the reference data can be identified.</li> <li>MethodDetails:  Text description defining the method used to create the reference (or indicating the AGP version)</li> <li>RefVersion: a version name used to identify the version of this reference data.</li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/#assemblyhaplotypesplugin","title":"AssemblyHaplotypesPlugin","text":"<p>This plugin takes a reference fasta file containing sequence for a single chromosome and an assembly fasta file with sequence for a single corresponding chromosome. The fastas are aligned using mummer4 nucmer script with a cluster size of 250 and the --mum parameter (anchor matches are unique in both reference and query).  </p> <p>The nucmer results are filtered using mummer4 delta-filter script with the -g option. The -g option provides a 1-1 global alignment not allowing rearrangments. Mummer script show-coords is run on both the original delta file created by nucmer, and the filtered delta created via delta-filter. Post-processing is done to add back inversions.</p> <p>Mummer script show-snps is run using the post-processed coords file. The list of final-snps is stored in htsjdk VariantContext records. The haplotype sequence is pulled from the VariantContext records. For alignments that map to reference anchor regions, haplotype sequence and variant lists are created and loaded to the PHG database.</p> <p>Both assembly anchor and inter-anchor regions are processed in this method.</p> <p>Tables populated via this method are:</p> <ul> <li>genotypes</li> <li>gametes</li> <li>gamete_groups</li> <li>methods</li> <li>haplotypes</li> <li>gamete_haplotypes</li> </ul> <p>The parameters to this plugin are:</p> <ul> <li>-ref   Reference Genome File for a single chromosome.  (Default is null) (REQUIRED) <li>-assembly  Path to assembly fasta for a single chromosome from which to pull sequence for aligning. <li>-outputDir   Path to output directory including trailing / that will be used when creating temporary output files. <li>-dbConfigFile  Path to config file containing DB parameters host, user, password, DB, type.  Used for making the database connection.  Type must be wither \"sqlite\" or \"postgres\" to identify db type for connection. <li>-version  Version name for the set of DB anchor/inter-anchor coorcinates as stored in the anchor_versions table (Default is null) (REQUIRED) <li>-assemblyName  Name of assembly taxon, to be stored as taxon name in the DB. (Default is null) (REQUIRED) <li>-chrom  Name of chromosome as it appears both for the reference in the db genome_intervals table and in the fasta file idline for the assembly.  The chromosome name can be just a number, a number preceded by \"chr\" or a number preceded by \"chromsome\".  Reference and assembly need to be consistent."},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/#assemblyhaplotypesmultithreadplugin-optional","title":"AssemblyHaplotypesMultiThreadPlugin (optional)","text":"<p>The AssemblyHaplotypesMultiThreadPlugin plugin takes reference and assembly fastas, split by chromosome, and aligns them with mummer4 scripts. It performs the same functions as are executed when the LoadAssemblyAnchors.sh script is called, but will run the mummer4 alignment programs in parallel. The number of processes run concurrently is dependent on a user supplied thread count.</p> <p>When determining the number of threads to run, please keep in mind the alignment of each ref-assembly fasta pair will take RAM commensurate with the size of the genomes. Consider the RAM limitations of your machine as well as the number of cores available.</p> <p>The required input to this script is a configuration file with database connection information, a keyfile with fasta information and an output directory.  Both the configuration file and the keyfile must live in a local machine directory that is mounted to the docker's /tempFileDir/data directory if this is run via docker.</p> <p>The ParalleleAssemblyAnchorsLoad.sh script uses the parameter cache option to load the configuration file into memory.  This is the -configParameters  directive given to the tassel run_pipeline.pl script used in the Docker container. if you run this plugin manually at the command line, you will need to include this yourself with a command similar to the one below: <pre><code>/tassel-5-standalone/run_pipeline.pl -Xmx100G -debug \\\n-configParameters myConfigFile.txt \\\n-AssemblyHaplotypesMultiThreadPlugin  \n    -keyFile myKeyFile.txt  \\\n    -outputDir /workdir/user\u0010/phg_assemblies/DockerOutput/ \\\n    -endPlugin\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/#addrefrangeasassemblyplugin-optional","title":"AddRefRangeAsAssemblyPlugin (optional)","text":"<p>This class was created to load a reference genome as an assembly.  The method is hard-coded to \"mummer4\", which is the assembly method name at the time of this writing. All the variants are reference records. The addition of these haplotypes allows for pulling </p> <ul> <li>-ref   Reference Genome File for aligning against. (REQUIRED) <li>-configFile  Path to config file for accessing/loading the DB (REQUIRED) <li>-methodName  Name of method used for processing. Should match the assembly or GATK method based on how user wants the ref haplotypes grouped (REQUIRED)"},{"location":"UserInstructions/CreatePHG_step2_alignAssemblies/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>If running through Docker, check that mount points are correct.</li> <li>Check that chromosome names match between reference and assembly genomes.</li> </ol> <p>Return to Step 2 pipeline</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step2_assemblyAnchorWavePluginDetails/","title":"AssemblyMAFFromAnchorWavePlugin Details","text":"<p>The first step in loading assembly haplotypes to the database is to align the assemblies to the reference genome with the AssemblyMAFFromAnchorWavePlugin plugin. It can be run separately using a shell script similar to the one detailed below:</p> <pre><code>WORKING_DIR=/workdir/lcj34/anchorwaveTesting/\nDOCKER_CONFIG_FILE=/phg/config.txt\n\n# correct parameters for this plugin should be in the config.txt file\n# replace &lt;version tag&gt; with the specific PHG version you wish to run\ndocker1 run --name anchorwave_assemblies_maf --rm \\\n    -v ${WORKING_DIR}/:/phg/ \\\n    -t maizegenetics/phg:&lt;version tag&gt; \\\n    /tassel-5-standalone/run_pipeline.pl -Xmx100G -debug -configParameters ${DOCKER_CONFIG_FILE} \\\n    -AssemblyMAFFromAnchorWavePlugin -endPlugin\n</code></pre> <p>The config file must contain the entries below to run this step. The AssemblyMAFFromAnchorWavePlugin does not require database access.  Replace the entry values below with your own parameter values.  The entries listed here assume you have setup your directory structure using the MakeDefaultDirectoryPlugin.  It also assumes you are running via docker with the docker mount points as defined in the example script above.  Note the parameter values have paths that are docker specific.</p> <pre><code>\nAssemblyMAFFromAnchorWavePlugin.outputDir=/phg/outputDir\nAssemblyMAFFromAnchorWavePlugin.keyFile=/phg/Ia453_K0326Y_keyfile.txt\nAssemblyMAFFromAnchorWavePlugin.gffFile=/phg/inputDir/reference/Zm-B73-REFERENCE-NAM-5.0_Zm00001e.1.gff3\nAssemblyMAFFromAnchorWavePlugin.refFasta=/phg/inputDir/reference/Zm-B73-REFERENCE-NAM-5.0.fa\nAssemblyMAFFromAnchorWavePlugin.threadsPerRun=4\nAssemblyMAFFromAnchorWavePlugin.numRuns=2\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_assemblyAnchorWavePluginDetails/#details","title":"Details","text":"<p>This class uses anchorwave to align full genome assembly fastas to a reference. It expects a keyfile that contains information on the assembly fastas, including information on the local and remote location of the fastas.  The former is used for aligning, the latter (the remote location) is used in later steps to populate the genome_file_data table with information on a public site where the reference and assembly fastas may be found.</p> <p>For more information on the anchorwave alignment method, please check out this link.</p>"},{"location":"UserInstructions/CreatePHG_step2_assemblyAnchorWavePluginDetails/#parameters","title":"Parameters","text":"<p>Run the following command to print a current list of parameters for this plugin:</p> <pre><code>docker run --rm  maizegenetics/phg /tassel-5-standalone/run_pipeline.pl -AssemblyMAFFromAnchorWavePlugin\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_assemblyAnchorWavePluginDetails/#parameter-descriptions","title":"Parameter Descriptions","text":"<ul> <li>outputDir: (required) Directory to which all output files (*.maf and others) will be written</li> <li>keyFile: (required) Name of the keyfile to process.  eetails on the keyfile are in a separate header below</li> <li>gffFile: (required) Full path to a  GFF file associated with the reference genome</li> <li>refFasta: (required) Full path to the reference fasta file to be used when aligning the assembly genomes</li> <li>threadsPerRun: (optional: default=2) The number of threads to use for each processed assembly.  This value plus the value for the numRUns parameter shoudl be determined based on available system threads and memory.</li> <li>numRuns: (optional: default=1) Number of simultaneous assemblies to process. The anchorwave application can take up to 50G per thread for each assembly processed, plus some overhead. Consider this memory factor when providing values for threadsPerRun and numRun</li> <li>minimap2Location: (optional: default=\"minimap2\") Location of Minimap2 on file system.  This can be defaulted if it is on the PATH environment variable.  Anchorwave uses minimap2 for parts of the alignment.</li> <li>anchorwaveLocation: (optional: default=\"anchorwave\") Location of anchorwave on file system.  This can be defaulted if it is on the PATH environment variable.</li> <li>refMaxAlignCov: (optional: default=1) anchorwave proali parameter R, indicating reference genome maximum alignment coverage.</li> <li>queryMaxAlignCov: (optional: default=1) anchorwave proali parameter Q, indicating query genome maximum alignment coverage</li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_assemblyAnchorWavePluginDetails/#keyfile","title":"Keyfile","text":"<p>The AssemblyMAFFromAnchorWavePlugin keyfile must have columns AssemblyServerDir, AssemblyGenomeFasta, AssemblyDir, AssemblyFasta, and AssemblyDBName.  The AssemblyFasta column should contain the name of the assembly fasta file for aligning.  This is a full genome fasta.  The AssemblyGenomeFasta column should contain the name of the full genome fasta from which the assembly fasta came (it may be the same name as the AssemblyGenomeFasta).</p> <p>The keyfile may also have an optional \"Description\" column.  If present, the data from this column will be used by PopulatePHGDBPipelinePlugin when creating the keyfile for the LoadHaplotypesFromGVCFPlugin().  It becomes the description for the sample and should contain any assembly genome data the user would like stored in the genotypes table \"description\" field associated with the assembly taxon.</p> <p>The keyfile should be of tab-deliminted format.  The headers should look as below: <code>AssemblyServerDir       AssemblyGenomeFasta     AssemblyDir     AssemblyFasta   AssemblyDBName  Description</code></p> <p>An example keyfile for this plugin is below. Note all fields are tab-delimited and because this is run in a docker, the reference and assembly directories are docker paths.  The AssemblyServerDir should be the more permanent accessible location outside the docker where these assembly fastas are stored.</p> <pre><code>AssemblyServerDir       AssemblyGenomeFasta     AssemblyDir     AssemblyFasta   AssemblyDBName  Description\nirods:AssemblyFilePath  LineA.fa        /phg/inputDir/assemblies/       LineA.fa        LineA_Assembly  smallSeq description for assembly LineA\nirods:AssemblyFilePath  LineB.fa        /phg/inputDir/assemblies/       LineB.fa        LineB_Assembly  smallSeq description for assembly LineB\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_assemblyAnchorWavePluginDetails/#memory-considerations","title":"Memory considerations","text":"<p>Running anchorwave can be very memory intensive. It can take up to 50G per thread when running anchorwave.  Consider both the number of threads available on your machine as well as the memory that will be used for each. For example: If your machine has 512G, no more than 10 threads may be used when running anchorwave for a single run.  If you want to run 2 alignments in parallel and your machine has 512G, no more than 4 threadsPerRun.</p> <pre><code>memoryCost ~ &lt;number of assemblies to process&gt; * (80+(&lt;numThreads-1&gt;)*50)\n\ne.g.: 2 assemblies with 4 threads each:\n  memoryCost ~ 2 * (80+(4-1)*50) == 460GB\n</code></pre> <p>In the calculation above, the first 80G is for the thread that was subtracted in )  This includes 50Gb for the alignment matrix, another 30Gb for genomes and longest path result etc. <p>When a user has 10 threads available, there are different options, e.g. 2 runs with 5 threads each, or 5 runs with 2 threads each.  AnchorWave uses a thread for each collinear block, for some cases the last single collinear block might take sometime during which it will be using a single thread. So, setting of 5 runs with 2 threads each is faster than 2 runs with 5 threads each, but it incurs a higher memory cost.</p>"},{"location":"UserInstructions/CreatePHG_step2_assemblyHaplotypeParameters/","title":"Quick Start","text":"<p>Aligning assemblies to the reference genome and adding the resulting haplotypes to the PHG database  is can be done by the AssemblyHaplotypesMultiThreadPlugin. It can be run separately using the following command with the maximum memory size (-Xmx option), the path to the config file, and the log file set correctly:</p> <pre><code>docker run --rm  maizegenetics/phg /tassel-5-standalone/run_pipeline.pl \\\n-Xmx50G -debug -configParameters /phg/config.txt \\\n-AssemblyHaplotypesMultiThreadPlugin -endPlugin &gt; /path/to/logfile.txt\n</code></pre> <p>Note that the log file path is outside the docker. If the log file is not supplied, then the log information will be written to the terminal (sysout).</p> <p>The config file must contain the following entries to run this step. The first 4 entries pertain to database access, the remaining entries are for processing your assembly data. Replace the entries with your own parameter values.  The entries listed here assume you have setup your directory structure using the MakeDefaultDirectoryPlugin.</p> <pre><code>#!java\nhost=localHost\nuser=sqlite\npassword=sqlite\nDB=/phg/phgAssemblyPath.db\nDBtype=sqlite\n\n#liquibase output directory/general output dir\noutputDir=/phg/outputDir\nliquibaseOutdir=/phg/outputDir/\n\nasmMethodName=mummer4\nasmKeyFile=/phg/load_asm_genome_key_file.txt\n\nAssemblyHaplotypesMultiThreadPlugin.outputDir=/phg/outputDir/align/\nAssemblyHaplotypesMultiThreadPlugin.keyFile=/phg/load_asm_genome_key_file.txt\n\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_assemblyHaplotypeParameters/#details","title":"Details","text":"<p>This class runs mummer4 alignment of chrom-chrom fastas in a parallel fashion. It expects a directory for both reference and assembly fastas, split by chromosome, as well as a keyFile.</p> <p>The keyfile provides information on the location and name of the full assembly genome file that was used for alignment.  This data lives in the \"AssemblyServerDir\" and \"AssemblyGenomeFasta\" columns.  The PHG mummer4 code aligns the ref and assembly on a per-chromosome basis.  The location of the directory and file names for the ref and assembly per-chromosome fastas  are denoted in the RefDir/RefFasta columns (for the reference) and AssemblyDir/AssemblyFasta columns (for the assembly).  The chromosome name and assembly name for loading to the database are also specified in the keyFile.  The file contains 8 columns and should be tab-delimited format.  The header columns should be as below:</p> <p><code>AssemblyServerDir RefDir RefFasta AssemblyDir AssemblyGenomeFasta AssemblyFasta   AssemblyDBName  Chromosome</code></p> <p>An example key file that has information for a single assembly genome is shown below.  Note all fields are tab-delimited and because this is run in a docker, the reference and assembly directories are docker paths.  The AssemblyServerDir should be an accessible location outside the docker.</p> <pre><code>AssemblyServerDir       RefDir  RefFasta        AssemblyDir     AssemblyGenomeFasta     AssemblyFasta   AssemblyDBName  Chromosome\nhttps://download.maizegdb.org/Zm-CML103-REFERENCE-NAM-1.0/  /phg/inputDir/reference/        B73chr1.fa  /phg/inputDir/assemblies/       Zm-CML103-REFERENCE-NAM-1.0.fa.gz        CML103chr1.fa        CML103_Assembly  1\nhttps://download.maizegdb.org/Zm-CML103-REFERENCE-NAM-1.0/  /phg/inputDir/reference/        B73chr2.fa  /phg/inputDir/assemblies/       Zm-CML103-REFERENCE-NAM-1.0.fa.gz        CML103chr2.fa        CML103_Assembly  2\nhttps://download.maizegdb.org/Zm-CML103-REFERENCE-NAM-1.0/  /phg/inputDir/reference/        B73chr3.fa  /phg/inputDir/assemblies/       Zm-CML103-REFERENCE-NAM-1.0.fa.gz        CML103chr3.fa        CML103_Assembly  3\n\n</code></pre> <p>An example key file that has per-chromosome fastas for multiple species is shown in the next example.  Note that the assembly chromosome fasta files must exist in the same directory.</p> <pre><code>AssemblyServerDir       RefDir  RefFasta        AssemblyDir     AssemblyGenomeFasta     AssemblyFasta   AssemblyDBName  Chromosome\nhttps://download.maizegdb.org/Zm-CML103-REFERENCE-NAM-1.0/  /phg/inputDir/reference/        B73chr1.fa  /phg/inputDir/assemblies/       Zm-CML103-REFERENCE-NAM-1.0.fa.gz        CML103chr1.fa        CML103_Assembly  1\nhttps://download.maizegdb.org/Zm-CML103-REFERENCE-NAM-1.0/  /phg/inputDir/reference/        B73chr2.fa  /phg/inputDir/assemblies/       Zm-CML103-REFERENCE-NAM-1.0.fa.gz        CML103chr2.fa        CML103_Assembly  2\nhttps://download.maizegdb.org/Zm-CML103-REFERENCE-NAM-1.0/  /phg/inputDir/reference/        B73chr3.fa  /phg/inputDir/assemblies/       Zm-CML103-REFERENCE-NAM-1.0.fa.gz        CML103chr3.fa        CML103_Assembly  3\nhttps://download.maizegdb.org/Zm-Ki3-REFERENCE-NAM-1.0/  /phg/inputDir/reference/        B73chr1.fa  /phg/inputDir/assemblies/       Zm-Ki3-REFERENCE-NAM-1.0.fa.gz        Ki3chr1.fa        Ki3_Assembly  1\nhttps://download.maizegdb.org/Zm-Ki3-REFERENCE-NAM-1.0/  /phg/inputDir/reference/        B73chr2.fa  /phg/inputDir/assemblies/       Zm-Ki3-REFERENCE-NAM-1.0.fa.gz        Ki3chr2.fa        Ki3_Assembly  2\nhttps://download.maizegdb.org/Zm-Ki3-REFERENCE-NAM-1.0/  /phg/inputDir/reference/        B73chr3.fa  /phg/inputDir/assemblies/       Zm-Ki3-REFERENCE-NAM-1.0.fa.gz        Ki3chr3.fa        Ki3_Assembly  3\n</code></pre> <p>The mummer4 alignment scripts will be kicked off in parallel based on the number of threads the user specifies.  Default is 3, which is essentially single threaded and 2 of the 3 are reserved for I/O.</p> <p>Note that running the alignments in parallel will take not just threads, but memory.  This should be considered when the user decides on the number of threads.  It has also been found that running with too many threads may cause contention for DB writing access.  If problems arise when using a large number of threads, consider reducing the number of threads.</p>"},{"location":"UserInstructions/CreatePHG_step2_assemblyHaplotypeParameters/#parameters","title":"Parameters","text":"<p>Run the following command to print a current list of parameters for this plugin:</p> <pre><code>docker run --rm  maizegenetics/phg /tassel-5-standalone/run_pipeline.pl -AssemblyHaplotypesMultiThreadPlugin\n</code></pre> <p>Running the command will print the following usage statement:</p> <pre><code>Usage:\nAssemblyHaplotypesMultiThreadPlugin &lt;options&gt;\n-outputDir &lt;Output Directory&gt; : Output directory including trailing / for writing files (required)\n-keyFile &lt;keyFile&gt; : Name of the Keyfile to process.  Must have columns RefDir, RefFasta, AssemblyDir, AssemblyFasta, AssemblyDBName, and Chromosome.   (required)\n-mummer4Path &lt;Mummer4 Executables Path&gt; : Path where mummer4 executables live: nucmer, delta-filter, show-snps, show-coords   (Default: /mummer/bin/)\n-clusterSize &lt;Mummer4 Nucmer Cluster Size &gt; : Cluster size to use with mummer4 nucmer script.  (Default: 250)\n-gvcfOutputDir &lt;GVCF Output Dir&gt; : Directory for gvcf files to be output for later use\n-entryPoint &lt;Assembly Entry Point&gt; : Where to begin processing. All runs everything.  Refilter means run from the re-filtering of the coords files. hapltypes runs just the haplotypes processing. (Default: all)\n-minInversionLen &lt;Minimum Inversion Length&gt; : Minimum length of inversion for it to be kept as part of the alignment. Default is 7500 (Default: 7500)\n-loadDB &lt;true | false&gt; : Boolean: true means load haplotypes to db, false means do not populate the database.  Defaults to true (Default: true)\n-assemblyMethod &lt;Assembly Method Name&gt; : Name to be stored to methods table for assembly method, default is mummer4   (Default: mummer4)\n-numThreads &lt;Num Threads&gt; : Number of threads used to process assembly chromosomes.  The code will subtract 2 from this number to have the number of worker threads.  It leaves 1 thread for IO to the DB and 1 thread for the Operating System. (Default: 3)\n</code></pre> <p>Description of required parameters:</p> <ul> <li>keyfile: The keyfile must be a tab-delimited file with 1 row for each per-chromosome assembly fasta you wish to align and load to the database.  The required columns for the keyfile are AssemblyServerDir,RefDir, RefFasta,  AssemblyDir, AssemblyGenomeFasta, AssemblyFasta, AssemblyDBName and Chromosome.  The oolumn descriptions are below.<ul> <li>AssemblyServerDir: machine and directory where the full genome fasta for the assembly may be found.  This should be data on a publicly available server, not a docker specific path.</li> <li>RefDir: full path to the directory containing the reference fasta file.</li> <li>RefFasta: name of the reference fasta file to be aligned to the assembly fasta file in this row.  This should be a fasta file at the chromosome level.  It is the reference file used for the ref-chromosome to assembly-chromosome alignment</li> <li>AssemblyDir: full path to the directory containing the assembly fasta files.</li> <li>AssemblyGenomeFasta:  name of the full genome fasta for this assembly that can be found at the location specified in the AssemblyServerDir column.  This is the file stored to the genome_file_data table.</li> <li>AssemblyFasta:  name of the assembly fasta file to be aligned to the reference fasta file in this row.  This should be a fasta file at the chromosome level.  It is the file that will be used for the ref-chromosome to assembly-chromosome alignment.</li> <li>AssemblyDBName: the name for this assembly to add as the \"line_name\" to the database's genotype table.</li> <li>Chromosome: the chromosome name that is being processed.  This name must match a reference chromosome that was loaded with the LoadGenomeIntervals.sh script.</li> </ul> </li> <li>configFile:  DB Config File containing properties host,user,password,DB and DBtype where DBtype is either \"sqlite\" or \"postgres\". This file is expected to live in the folder mounted to Docker tempFileDir/data/. This file may also contain values for optional parameters.</li> </ul> <p>The optional parameters that may be defined in the configuration file are  mummer4Path, clusterSize, entryPoint, minInversionLen, loadDB, assemblyMethod and numThreads. If you wish to set a value other than the default for these parameters, they should be included in the configuration file in the format shown below (with your own values replacing the default values shown here):</p> <ul> <li>AssemblyHaplotypesMultiThreadPlugin.mummer4Path=/mummer/bin</li> <li>AssemblyHaplotypesMultiThreadPlugin.clusterSize=250</li> <li>AssemblyHaplotypesMultiThreadPlugin.minInversionLen=7500</li> <li>AssemblyHaplotypesMultiThreadPlugin.loadDB=true</li> <li>AssemblyHaplotypesMultiThreadPlugin.assemblyMethod=mummer4</li> <li>AssemblyHaplotypesMultiThreadPlugin.numThreads=3</li> <li>AssemblyHaplotypesMultiThreadPlugin.entryPoint=all</li> </ul> <p>The \"entryPoint\" parameter is provided for users who wish to speed up processing by running the mummer4 alignments on multiple machines. You may do this, then gather all the mummer files to a single machine.  If you do this, set the loadDB flag to FALSE, run all your alignments, then gather them to a single machine for loading into the database.</p> <p>Description of optional parameters:</p> <ul> <li>mummer4Path: If the mummer4 executables exist in a path other than /mummer/bin, then provide that path via this parameter.  If you are running in the PHG docker, the path will be /mummer/bin and this parameter is not necessary.</li> <li>clusterSize:  This is a parameter to mummer4's nucmer alignment program.  It sets the minimum length for a cluster of matches.  We have found the value of 250 provides good coverage with acceptable speed.  If you wish a different value, set this parameter.</li> <li>minInversionLen: Minimum length of inversion for it to be kept as part of the alignment. Default is 7500</li> <li>Boolean: true means load haplotypes to db, false means do not populate the database.  Useful if the user is running mummer4 alignments on multiple machines and plans later to put all mummer4 files on 1 machine to load to a single database.  Defaults to true</li> <li>numThreads: Number of threads used to process assembly chromosomes.  The code will subtract 2 from this number to get the number of worker threads.  It leaves 1 thread for IO to the DB and 1 thread for the Operating System.</li> <li>entryPoint:  This parameter indicates at which point assembly processing should begin.  If a step other than \"all\" is chosen, files normally created from the previous steps must be present in a sub-directory named \"align\" in your output directory.  They must be named using the format shown below for the software to recognize them:<ul> <li>all: Run the entire pipeline.  The software creates all necessary files.</li> <li>deltafilter:  Assumes the nucmer aligning step has already been performed. Processing starts with mummer4 delta-filter step includes running mummer4 show-coords on both the original alignment and the filtered alignment as well as mummer4 show-snps.  The delta file must be available in a sub-directory named \"align\" in the directory mounted to docker's /phg/outputDir/.  The file name must be in the format:<ul> <li>ref___c.delta <li>ex:  ref_W22_1_c250.delta</li> <li>refilter: Assumes delta-filter and show-coords for both the delta and the delta-filter steps have been run. This step runs additional filtering to process overlapping alignments, remove embedded alignments and find a longest-path solution.Your output/align directory  must have the files for deltafilter plus:<ul> <li>ref___c.delta_filtered <li>ref___c.coords_orig <li>ref___c.coords_filtered haplotypes: This step takes the mummer4 output files and from them creates haplotype sequence fro the database.  If starting from this step, you must have the files from above plus: <li>ref___c.coords_filteredPlus_noEmbedded <li>ref___c.coords_final <li>ref___c.snps_prefiltered <li>ref___c.snps_final"},{"location":"UserInstructions/CreatePHG_step2_assemblyViaAnchorwave/","title":"Quick Start","text":"<p>Aligning assemblies to the reference genome and adding the resulting haplotypes to the PHG database  can be done by running the AssemblyMAFFromAnchorWavePlugin, the MAFToGVCFPlugin, and the LoadHaplotypesFromGVCFPlugin.</p> <p>The first plugin runs the anchorwave aligner and creates a UCSC Multiple Alignment Format (MAF) file. The second plugin takes the MAF output from the AssemblyMAFFromAnchorWavePlugin and creates a GVCF file (when running PHG versions 0.0.40 and below) or both the bgzipped and tabix'd version of the bgzipped file (when running PHG version 1.0 or above). The LoadHaplotypesFromGVCFPlugin takes the GVCF files created in the second step and loads this data as assembly haplotypes to the PHG database. </p> <p>While supported, it is not recommended to run anchorwave via the PopulatePHGDBPipelingPlugin.  Running the anchorwave aligner is very memory and time intensive.  If running through PopulatePHGDBPipelinePlugin, all the genome alignments are done before the MAFs are converted to GVCFs and then stored to the database.  It may prove optimal for a user to run the AssemblyMAFFromAnchorWavePlugin separately on a machine with larger memory than needed for the rest of the pipeline.  The generated MAF files could then be transferred to a single machine for processing into GVCFs and loaded to the database.  </p> <p>For details on running each of these plugins, click on the links below:</p> <p>AssemblyMAFFromAnchorWavePlugin</p> <p>MAFToGVCFPlugin</p> <p>LoadHaplotypesFromGVCFPlugin</p>"},{"location":"UserInstructions/CreatePHG_step2_consensus/","title":"Create consensus haplotypes","text":""},{"location":"UserInstructions/CreatePHG_step2_consensus/#details","title":"Details","text":"<p>The CreateConsensi.sh script is used to create Consensus Haplotypes for each genome interval using a specific set of raw haplotypes. Basically this script will create a haplotype graph and attempt to create consensus haplotypes for each anchor. Once a set of consensus haplotypes are created, the script will upload all consensus haplotypes to the Database. Once this process is done, a graph can be made using the consensus haplotypes.</p>"},{"location":"UserInstructions/CreatePHG_step2_consensus/#kitchen-sink","title":"Kitchen Sink","text":"<p>Steps in the create consensus pipeline:</p> <ol> <li>Create a Haplotype Graph</li> <li>For each anchor do the following:<ol> <li>If the graph does not have the variants, extract the variants from the DB for the raw haplotypes.</li> <li>Take all the GVCF variants and combine them into a single TASSEL GenotypeTable object.</li> <li>Remove any Positions covering an indel.</li> <li>Create consensus clusters and create GenotypeTables for each cluster.</li> <li>Reintroduce indels which were filtered out if they agree.  If they do not agree, it will follow the indelMergeRule defined in the configuration file.</li> </ol> </li> <li>Collect up to 1000 Consensus haplotypes at a time and upload them to the DB.</li> </ol> <p>There are a number of parameters needed for this step. We highly recommend tuning the clustering parameters to match the diversity present in the database you are working with. </p> <p>CreateConsensi parameters </p> <ul> <li>configFile: Path to config file containing DB parameters host, user, password, DB, DBtype. Used for making the database connection. DBtype must be either \"sqlite\" or \"postgres\" to identify db type for connection. This file will also contain parameters related to creating the consensus sequences and is described in the Consensus Parameters section below. A sample configuration file can be found here: Master Config File</li> <li>referenceFasta: Reference fasta file. If running on the command line, this parameter is set only in the config file. If running in the docker this is contained in the mount point /phg/inputDir/reference/.</li> <li>haplotypeMethod: Name of the raw haplotypes from which Consensus should be made.  This is a comma-separated list of the existing haplotype methods you want included when you create the consensus haplotypes</li> <li>consensusMethod: Name to be stored in the database denoting this run of CreateConsensi.  This is a user-defined method name.  It is often a name that includes parameters used to create the consensus haplotypes, e.g. \"collapseMethod_NAM_CONSENSUS_mxDiv_10ToNeg4\".  This name tells me that consensus was created using NAM haplotypes with a mxDiv parameter of 10 to the minus 4.  </li> </ul> <p>DB Parameters</p> <ul> <li>host - host name of the db</li> <li>user - db user name</li> <li>password - password for db</li> <li>DB - name of the db</li> <li>DBtype - sqlite or postgres</li> </ul> <p>Java Arguments</p> <ul> <li>Xmx - max heap space for the pipeline. This is similar to Java's -Xmx argument.</li> </ul> <p>Graph Building Parameters</p> <ul> <li>includeVariants - true. This needs to be true to work correctly. Given the new Variant Storage, the graphs variants can fit into memory much better.</li> </ul> <p>Consensus Parameters</p> <ul> <li>minFreq - Minimum allele frequency: At each position, if no allele has the minimum frequency, the consensus haplotype allele will be set to missing.</li> <li>mxDiv - maximum amount of divergence allowed when clustering. The default is 0.01. It is highly recommended you test this parameter at different levels to determine how much or little haplotype collapse is best for your database. </li> <li>maxClusters - The maximum number of clusters that will be created for a reference range. If mxDiv produces too many clusters then the cut height that produces maxClusters number of clusters will be substituted.</li> <li>minSites - minimum number of non-missing sites to be clustered. Any haplotypes with fewer number of non-missing calls will be ignored. The default is 20.</li> <li>minCoverage -  For each range, any taxon with coverage of less than this amount will not be used to generate consensus haplotypes and will not be included in any haplotype group for that range.</li> <li>minTaxa - Minimum number of taxa, default is 1.</li> <li>rankingFile - path to a ranking file. If your haplotypes were created from Assemblies, you need to produce a method ranking the taxon. This file will take the form: taxon\\tscore where higher scores mean we trust that taxon more highly.  Do not include a header line. When clustering assemblies, when we have a cluster of similar haplotypes, we choose whichever taxon in that group which has the higher ranking score. To break ties, be sure to give each taxon a different score. One simple way to score things is to count the number of haplotypes covered by each taxon in the DB and use that count as the score. Any other arbitrary ranking can be used.</li> <li>clusteringMode - either upgma_assembly(default) or kmer_assembly. Clustering mode \"upgma\" is supported in PHG versions 0.0.40 or below, but is not supported in PHG version 1.0 or higher.   The differences between the upgma and upgma_assembly is that upgma just builds a tree based on a pairwise calculated distance matrix and then will try to merge haplotypes together.  upgma_assembly will also do a pairwise distance matrix but then will select the better haplotype in the group based on the ranking file specified by the \"rankingFile\" parameter.   Clustering mode \"upgma\" is no longer supported in PHG versions supporting variants stored in gvcf files (PHG version 1.0 or higher) as the software now stores a gvcf file id from which each haplotype may be found.  With the upgma method, a haplotype is created by merging regions of different haplotypes together to form a new haplotype.  That new haplotype does not exist in any gvcf file associated with this clustered set of genomes.</li> <li>maxThreads - The maximum number of threads to be used to create consensi</li> <li>kmerSize - size of kmers for the kmer clustering method (default = 7)</li> <li>distanceCalculation - distance calculation type, required when clusteringMode = kmer_assembly</li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_consensus/#details-on-running-this-step-through-docker","title":"Details on running this step through docker","text":"<pre><code>#!bash\n\nWORKING_DIR=/workdir/lcj34/phg_newVersion/\nDOCKER_CONFIG_FILE=/phg/config.txt\n\ndocker1 run --name consensus_container --rm \\\n    -v ${WORKING_DIR}/:/phg/ \\\n    -t maizegenetics/phg:1.0 \\\n    /tassel-5-standalone/run_pipeline.pl -Xmx100G -debug -configParameters ${DOCKER_CONFIG_FILE} \\\n        -HaplotypeGraphBuilderPlugin -configFile ${DOCKER_CONFIG_FILE}.txt -methods method1:method2 \\ \n                -includeVariantContexts true -endPlugin \\\n        -RunHapConsensusPipelinePlugin \\\n                -referenceFasta /phg/inputDir/reference/Zm-B73-REFERENCE-NAM-5.0.fa \\\n                -dbConfigFile ${DOCKER_CONFIG_FILE}  \\\n                -collapseMethod NAM_CONSENSUS_mxDiv_10ToNeg4 \\\n                -collapseMethodDetails NAM_CONSENSUS_mxDiv_10ToNeg4 \\\n                -rankingFile /phg/rankingFile.txt \\\n                -mxDiv 0.0001 \\\n                -clusteringMode kmer_assembly -endPlugin\n</code></pre> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The -t directive indicates the Docker image of which this container will be an instance.  The last line tells the Docker container to run the CreateConsensi.sh script which is found in the root directory.  The items following are the parameters to the CreateConsensi.sh script.</p>"},{"location":"UserInstructions/CreatePHG_step2_consensus/#files","title":"Files","text":"<p>Config file</p> <p>An example can be found here: Master config file</p> <p>Reference Fasta file</p> <p>Fasta file for the reference genome you want to use in the database</p> <p>Ranking file</p> <p>If your haplotypes were created from Assemblies, you need to produce a method ranking the taxon. This file must be a tab-delimited file of the form: taxon\\tscore where higher scores mean we trust that taxon more highly. Do not include a header line. When clustering assemblies, when we have a cluster of similar haplotypes, we choose whichever taxon in that group which has the higher ranking score. To break ties, be sure to give each taxon a different score. One simple way to score things is to count the number of haplotypes covered by each taxon in the DB and use that count as the score. Any other arbitrary ranking can be used.</p> <p>An example ranking file is below.  In this file, B73 is the best taxon:</p> <pre><code>B73 4\nCML103  2\nMo17    1\nW22 3\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_consensus/#plugins","title":"Plugins","text":"<p>The RunHapConsensusPlugin goes through each reference range in the database in parallel and invokes software to merge and cluster the haplotype sequence for each range.  The consensus sequences created are then loaded to the database and associated with the method name provided in the plugin parameters.</p> <p>An example command to string these plugins together and run the create consensus step through TASSEL directly is below the plugin descriptions.</p>"},{"location":"UserInstructions/CreatePHG_step2_consensus/#runhapconsensusplugin","title":"RunHapConsensusPlugin","text":"<p>The RunHapCollapsePlugin method was written to consolidate calls to the collapse pipeline.  Invocation of this plugin results in the following functionality performed:</p> <p>Loop through each reference range in the graph:</p> <ul> <li>Extract the HaplotypeNodes with the VariantContexts (we assume that the user has not pulled these yet for memory reasons)</li> <li>Merge all the VariantContext records for each base pair (bp) of the reference range</li> <li>Export a GenotypeTable containing each bp</li> <li>Run HapCollapse Finding algorithm on this genotype table</li> <li>load consensus haplotypes to database haplotypes table</li> </ul> <p>Example command to chain the HaplotypeGraphBuilderPlugin and the RunHapConsensusPipelinePlugin when calling them outside of a docker instance:</p> <p>The ranking file is needed for both the upgma_assembly and kmer_assembly methods.  The upgma method is supported in PHG versions 0.0.40 and earlier, but not in PHG version 1.0 or later.</p> <pre><code>tassel-5-standalone/run_pipeline.pl -Xmx500G -debug -configParameters config.txt \\\n        -HaplotypeGraphBuilderPlugin -configFile config.txt -methods method1:method2 \\ \n                -includeVariantContexts true -endPlugin \\\n        -RunHapConsensusPipelinePlugin \\\n                -referenceFasta /workdir/zrm22/Maize2_0/BuildConsensus/Zm-B73-REFERENCE-NAM-5.0.fa \\\n                -dbConfigFile config.txt \\\n                -collapseMethod NAM_CONSENSUS_mxDiv_10ToNeg4 \\\n                -collapseMethodDetails NAM_CONSENSUS_mxDiv_10ToNeg4 \\\n                -rankingFile rankingFile.txt \\\n                -mxDiv 0.0001 \\\n                -clusteringMode kmer_assembly -isTestMethod true -endPlugin \n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_consensus/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>If you are not seeing as many haplotypes as you would like (too much collapse) try setting the mxDiv parameter to a smaller value.</li> <li>If you find you are missing taxa at more reference ranges than expected, try setting the minTaxa parameter to 1. This will keep divergent taxa that do not cluster with at least one other taxon in the database.</li> </ol> <p>Return to Step 2 pipeline, version 0.0.40 and earlier</p> <p>Return to Step 2 pipeline, version 1.0 and greater</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step2_haplotypeKeyfile/","title":"CreateHaplotype Key File","text":""},{"location":"UserInstructions/CreatePHG_step2_haplotypeKeyfile/#specification","title":"Specification:","text":"<p>The Keyfile is a tab-separated text file which is used to set up the alignment, HaplotypeCaller and GVCF Upload steps for the PHG CreateHaplotypes Scripts.</p> <p>The PHG will process the following columns:</p> HeaderName Description Required sample_name Name of the taxon to be processed. Yes sample_description Short Description of the sample_name. No, if not specified, an empty description will be used files Comma-separated list of file names to be processed. This file list should not have the full paths pre appended.  But rather it needs just the file names. Yes type Type of the files to be processed.  PHG Currently Supports FASTQ, BAM or GVCF. Yes chrPhased Are the Chromosomes Phased?  This  needs to be 'true' or 'false' Yes for GVCF type genePhased Are the Genes Phased? This  needs to be 'true' or 'false' Yes for GVCF type phasingConf What is the confidence of the phasing?  This needs to be between 0.0 and 1.0. If working with inbreds, this can be set close to 1.0. Yes for GVCF type libraryID What is the library ID of the fastq files.  This is only used if running BWA during CreateHaplotypesFromFastq.groovy Yes only for FASTQ type gvcfServerPath remote server name or address, followed by semicolon, followed by path on server where gvcfs files will be stored. Yes if running with PHG version 1.0 or higher <p>Because the entries in the 'files' column are comma separated, the PHG can do the following depending on the type:</p> <ul> <li>FASTQ : pairwise or single ended alignment using bwa mem.</li> <li>BAM : Run GATK/Sentieon HaplotypeCaller on all the BAM files specified in the list to create a single GVCF.</li> <li>GVCF : upload haplotypes for taxon with ploidy &gt; 1.  Each file in the list will create a new haplotype.  If using Heterozygous material, we expect you to phase the GVCF file prior to running CreateHaplotypesFromGVCF.groovy.</li> </ul>"},{"location":"UserInstructions/CreatePHG_step2_haplotypeKeyfile/#sample-file","title":"Sample File:","text":"<p>Note this file has the \"gvcfServerPath\" column required for PHG version 1.0 and above, but not required for  prior versions.</p> <pre><code>#!txt\n\nsample_name     sample_description      files   type    chrPhased       genePhased      phasingConf     libraryID       gvcfServerPath\nRef     Ref line aligned        Ref_R1.fastq    FASTQ   true    true    .99     dummyLib1       localhost;/Users/lcj34/temp/gvcfRemote/\nLineA   LineA line aligned      LineA_R1.fastq  FASTQ   true    true    .99     dummyLib1       localhost;/Users/lcj34/temp/gvcfRemote/\nLineB   LineB line aligned      LineB_R1.fastq  FASTQ   true    true    .99     dummyLib1       localhost;/Users/lcj34/temp/gvcfRemote/\nRefA1   RefA1 line aligned      RefA1_R1.fastq  FASTQ   true    true    .99     dummyLib1       localhost;/Users/lcj34/temp/gvcfRemote/\nLineA1  LineA1 line aligned     LineA1_R1.fastq FASTQ   true    true    .99     dummyLib1       localhost;/Users/lcj34/temp/gvcfRemote/\nLineB1  LineB1 line aligned     LineB1_R1.fastq FASTQ   true    true    .99     dummyLib1       localhost;/Users/lcj34/temp/gvcfRemote/\n\nRefA1   RefA1 Aligned using BWA RefA1_dummyLib1_srt_dedup.bam   BAM     true    true    .99     null    localhost;/Users/lcj34/temp/gvcfRemote/\nRef     Ref Aligned using BWA   Ref_dummyLib1_srt_dedup.bam     BAM     true    true    .99     null    localhost;/Users/lcj34/temp/gvcfRemote/\nLineB1  LineB1 Aligned using BWA        LineB1_dummyLib1_srt_dedup.bam  BAM     true    true    .99     null    localhost;/Users/lcj34/temp/gvcfRemote/\nLineA   LineA Aligned using BWA LineA_dummyLib1_srt_dedup.bam   BAM     true    true    .99     null    localhost;/Users/lcj34/temp/gvcfRemote/\nLineB   LineB Aligned using BWA LineB_dummyLib1_srt_dedup.bam   BAM     true    true    .99     null    localhost;/Users/lcj34/temp/gvcfRemote/\nLineA1  LineA1 Aligned using BWA        LineA1_dummyLib1_srt_dedup.bam  BAM     true    true    .99     null    localhost;/Users/lcj34/temp/gvcfRemote/\nRefA1   RefA1 Aligned using BWA RefA1_haplotype_caller_output_filtered.g.vcf.gz GVCF    true    true    .99     null    localhost;/Users/lcj34/temp/gvcfRemote/\nRef     Ref Aligned using BWA   Ref_haplotype_caller_output_filtered.g.vcf.gz   GVCF    true    true    .99     null    localhost;/Users/lcj34/temp/gvcfRemote/\nLineB1  LineB1 Aligned using BWA        LineB1_haplotype_caller_output_filtered.g.vcf.gz        GVCF    true    true    .99     null    localhost;/Users/lcj34/temp/gvcfRemote/\nLineA   LineA Aligned using BWA LineA_haplotype_caller_output_filtered.g.vcf.gz GVCF    true    true    .99     null    localhost;/Users/lcj34/temp/gvcfRemote/\nLineB   LineB Aligned using BWA LineB_haplotype_caller_output_filtered.g.vcf.gz GVCF    true    true    .99     null    localhost;/Users/lcj34/temp/gvcfRemote/\nLineA1  LineA1 Aligned using BWA        LineA1_haplotype_caller_output_filtered.g.vcf.gz        GVCF    true    true    .99     null    localhost;/Users/lcj34/temp/gvcfRemote/\n\n</code></pre> <p>Return to Step 2 pipeline, version 1.0+</p> <p>Return to Step 2 pipeline, version 0.0.40-</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/CreatePHG_step2_taxaGroupTables/","title":"Separate taxa  into different groups","text":""},{"location":"UserInstructions/CreatePHG_step2_taxaGroupTables/#details","title":"Details","text":"<p>Once you have taxa loaded to the database, you can group these taxa in various ways. This step lets you create multiple taxa groups after the original taxa has been loaded.  For example, for a maize database you may want to group taxa into NAM or or 282.  Individual taxons may belong to multiple groups.</p>"},{"location":"UserInstructions/CreatePHG_step2_taxaGroupTables/#kitchen-sink","title":"Kitchen Sink","text":"<p>The AddTaxaToTaxaGroupPlugin method creates groupings for the taxa defined in the genotypes table of your database. You can create as many sets of groupings in the database as you would like. </p> <p>There are 4 parameters used by this step:</p> <ul> <li>configFile: This file comes from the cached configuration parameters.  It is a path to file containing database access information. The config file contains separate lines for host=X, user=X, password=X, DB=X, and DBtype=X where X is defined by the user, and DBtype is either sqlite or postgres. (required) </li> <li>groupName: Group name for this group of taxa.  (required)</li> <li>taxa: Comma separated list of taxa.  Taxa names must match the names existing in the PHG database genotypes table. (required)</li> </ul> <p>The last 2 parameters may be defined on the command line or in the configuration file.</p>"},{"location":"UserInstructions/CreatePHG_step2_taxaGroupTables/#details-on-running-this-step-through-docker","title":"Details on running this step through docker","text":"<p>When AddTaxaToTaxaGroupPlugin is run as part of a Docker container script, the Docker script expects the following directory mount points.  It assumes the directory structure used matches that created from the MakeDefaultDirectoryPlugin, and that both the config.txt and ranges.txt files live at the top level in the folder mounted to /phg:</p> <ul> <li>mount point 1: folder that contains the configFile and </li> </ul> <p>An example Docker script to run the AddTaxaToTaxaGroupPlugin plugin is:</p> <pre><code>docker run --name group_intervals --rm \\\n    -v /localDir:/phg\n    -t maizegenetics/phg:latest \\\n    /tassel-5-standalone/run_pipeline.pl -Xmx10G -debug \\\n          -GetDBConnectionPlugin \\\n            -configParameters ${dbConfigFile} \\\n            -create false \\\n            -endPlugin \\\n          -AddTaxaToTaxaGroupPlugin -groupName ${methodName} -taxa ${taxa}\n</code></pre> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The -t directive indicates the Docker image of which this container will be an instance.  </p> <p>The last line tells the Docker container to run the GroupGenomeIntervals.sh script which is found in the root directory.  The items following are the parameters to the GroupGenomeIntervals.sh script.</p>"},{"location":"UserInstructions/CreatePHG_step2_taxaGroupTables/#addtaxatotaxagroupplugin","title":"AddTaxaToTaxaGroupPlugin","text":"<p>The plugin takes a taxa exsiting in the database, a group name for that group of taxa and the configuration file indicating database name and type. The plugin reads the list of taxa, verifying that all exist in the db.  If any do not, an exception is thrown and the missing taxa are printed.  Next, the plugin creates the group if it doesn't already exist, and adds the taxa.  If the group exists and any of the taxa are already in the group, they are ignored.</p> <p>Database tables updated via this method are:</p> <ul> <li>taxa_groups</li> <li>taxa_groups_genoid</li> </ul> <p>The parameters to this plugin are:</p> <ul> <li>-groupName  name for this taxa group. (REQUIRED) <li>-taxa  Comma separated list of taxa.  Taxa names must match the names existing in the PHG database genotypes table. (REQUIRED) <li>-configFile  Path to file containing database access information, separate lines for host=X, user=X, password=X, DB=X, DBtype=X where X is user defined, and DBtype is either sqlite or postgres. NOTE: - this file should come from the command line parameter cache when obtaining a database connection. <p>This plugin expects a database connection as input. This can be obtained by chaining the GetDBConnectionPlugin with the AddTaxaToTaxaGroupPlugin when called from the command line with the TASSEL run_pipeline.pl script.</p> <p>An example of chaining these plugins is below:</p> <pre><code>#!python\n\n/tassel-5-standalone/run_pipeline.pl -Xmx10G -debug \\\n-GetDBConnectionPlugin \\\n    -configParameters ${dbConfigFile} \\\n    -create true  \\\n    -endPlugin \\\n-AddTaxaToTaxaGroupPlugin \\\n    -ranges ${rangesFile} \\\n    -methodName ${method_name}  \\\n    -methodDetails ${method_details} \\\n    -endPlugin\n</code></pre>"},{"location":"UserInstructions/CreatePHG_step2_taxaGroupTables/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Check file paths and names for mistakes.</li> <li>Double check that all taxa in the taxa list  are present in the database (check the value of field \"line_name\" in your database's genotypes table). </li> </ol> <p>Return to Step 2 pipeline</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/ExampleDatabase/","title":"Creating a Simple Example Database","text":"<p>Creating a simple database with simulated data provides a convenient way to test whether you are able to run the  PHG software successfully. A simple reference genome along with fastq files of a few taxa can be created using the  CreateSmallGenomesPlugin. The database can be created either using the PHG Docker or using the standalone version of TASSEL. The following TASSEL command can be used to create the data after replacing \"/path/to/basedir/\" with a directory name. </p> <pre><code>./run_pipeline.pl -debug -Xmx1G -CreateSmallGenomesPlugin \\\n   -baseDir /path/to/basedir -geneLength 3000 -interGeneLength 3500 \\\n   -numberOfGenes 10 -endPlugin\n</code></pre> <p>The command can be run without the -baseDir parameter, in which case the baseDir default will be userDir/temp/phgSmallSeq.  If you use docker to run the command (below), userDir = (root). In that case using the default value, the data will be  written to root not to /phg/ and the data will disappear when the container is removed. In short, when running  CreateSmallGenomesPlugin using the docker always use the -baseDir parameter and start it with /phg/.</p> <p>When using a docker to run the command, a directory called \"basedir\" will be created in the directory specified by  \"/Path/to\" if it does not already exist. To run the command using the docker</p> <pre><code>docker run --name smallGenomes_container --rm -v /Path/to/:/phg/ -t maizegenetics/phg \\\n   /tassel-5-standalone/run_pipeline.pl -debug -Xmx1G \\\n   -CreateSmallGenomesPlugin -baseDir /phg/basedir \\\n   -geneLength 3000 -interGeneLength 3500 \\\n   -numberOfGenes 10 -endPlugin &gt; /path/to/smallGenomes_log.txt\n</code></pre> <p>The plugin runs MakeDefaultDirectoryPlugin and populates some of the resulting directories  with simulated data representing a single chromosome with 10 (-numberOfGenes) genes. Specifically, it creates two sub-directories, \"answer\" and \"dockerBaseDir\" in the folder specified by the -baseDir parameter. The \"answer\" directory holds data that can be used to validate imputation results from running the PHG. The \"dockerBaseDir\" directory holds data that can be used to populate the PHG, map reads,  and impute data. It will also hold files produced by running PHG. For the docker commands that follow, \"dockerBaseDir\" is the  directory that should be mounted as \"/phg/\".</p>"},{"location":"UserInstructions/ExampleDatabase/#using-the-example-database","title":"Using the Example Database","text":"<p>The four pipeline plugins used for the PHG are</p> <ul> <li>MakeDefaultDirectoryPlugin</li> <li>MakeInitialPHGDBPipelinePlugin</li> <li>PopulatePHGDBPipelinePlugin</li> <li>ImputePipelinePlugin</li> </ul> <p>If you ran CreateSmallGenomesPlugin one of the things it did was to run MakeDefaultDirectoryPlugin so that is already  done. Following are the docker commands and links to config files needed to run the other plugins. The first step of  each pipeline is to make a call to liquibase to  make sure that the version of the PHG software and the PHG database are  the same. As a result you need to have pulled the same versions of both maizegenetics/phg and maizegenetics/phg_liquibase. </p>"},{"location":"UserInstructions/ExampleDatabase/#makeinitialphgdbpipelineplugin","title":"MakeInitialPHGDBPipelinePlugin","text":"<p>The config file needed here was written to dockerBaseDir when CreateSmallGenomesPlugin was run. The docker command to run this pipeline is</p> <pre><code>docker run --name makeDb_container --rm -v /path1/to/dockerBaseDir:/phg/ -t maizegenetics/phg \\\n   /tassel-5-standalone/run_pipeline.pl -debug -Xmx1G -configParameters /phg/configSQLiteDocker.txt \\\n   -MakeInitialPHGDBPipelinePlugin -endPlugin &gt; /path2/to/makedb_log.txt\n</code></pre> <p>Replace \"/path/to\" with the path that is correct for your computer. The path to the log file is on your computer and  outside of docker. \"maizegenetics/phg\" is the name of the docker image. If the plugin ran correctly, \"phgSmallSeq.db\" will  have been created in dockerBaseDir. Also, you should look at the log file to make sure there are no error messages. </p>"},{"location":"UserInstructions/ExampleDatabase/#populatephgdbpipelineplugin","title":"PopulatePHGDBPipelinePlugin","text":"<p>The docker command to run this pipeline, after replacing both instances of \"/path/to\", is</p> <pre><code>docker run --name populateDb_container --rm -v /path/to/dockerBaseDir:/phg/ \\\n   -t maizegenetics/phg /tassel-5-standalone/run_pipeline.pl -debug -Xmx1G \\\n   -configParameters /phg/configSQLiteDocker.txt -PopulatePHGDBPipelinePlugin -endPlugin \\\n   &gt; /path/to/populatedb_log.txt\n\n</code></pre> <p>This step may take several minutes. It is aligning fastq files to reference and extracting WGS based haplotypes using the  script CreateHaplotypesFromFastq.groovy. It will populate the database with haplotypes and write some files to folders in  dockerBaseDir in the process. As usual, check the log file to make sure there were no errors and that the plugin ran as expected.</p>"},{"location":"UserInstructions/ExampleDatabase/#imputepipelineplugin","title":"ImputePipelinePlugin","text":"<p>The config file needed for this step is imputeConfig.txt. You will need to put a copy of that in dockerBaseDir. The docker command   to run this pipeline, after replacing both instances of \"/path/to\" and copying the config file to baseDockerDir, is</p> <pre><code>docker run --name impute_container --rm -v /path/to/dockerBaseDir:/phg/ -t maizegenetics/phg \\\n   /tassel-5-standalone/run_pipeline.pl -debug -Xmx1G -configParameters /phg/imputeConfig.txt \\\n   -ImputePipelinePlugin -imputeTarget pathToVCF -endPlugin &gt; /path/to/impute_log.txt\n</code></pre> <p>The final result is a vcf file, dockerBaseDir/outputDir/output.vcf.</p>"},{"location":"UserInstructions/ExampleDatabase/#examine-the-data","title":"Examine the data","text":"<p>An excellent way to look at the database contents is to use rPHG.</p>"},{"location":"UserInstructions/FindLikelyParents/","title":"Finding Likely Parents for Imputation","text":""},{"location":"UserInstructions/FindLikelyParents/#likelyparentsplugin","title":"LikelyParentsPlugin","text":"<p>The LikelyParentsPlugin uses read mapping results from the PHG database to determine which of the parents in a graph are most likely to carry the same haplotypes as the sample that generated the read mappings. To run the plugin, a HaplotypeGraph with all of the candidate parents must be built. Then, the number of reads mapping to each parent is counted. The parent with the highest count is the first addition to the list of likely parents. The reads that mapped to that parent are removed from the total set of read mappings, then using the remaining reads, the number of reads mapping to each parent is again counted. The parent with the highest number of reads is added to the list of likely parents. The process is repeated until a stopping condition is met. The plugin takes two parameters that determine the stopping condition, maxParents and minCoverage. Coverage is defined as the number of reads mapping to any of the likely parents divided by the total number of reads. Adding likely parents stops either when the number of likely parents equals maxParents or when the coverage  is greater than or equal to minCoverage, whichever occurs first.</p> <p></p> <p>A name and path must be provided for an output file using the parameter \"outFile\". A report will be written to that file with the headers sample_name, total_reads, parent, order, number_of_reads, and cumulative_reads. Total_reads is the total number of reads in the read mapping record. Parent is one of the taxa in the HaplotypeGraph that was chosen as a likely parent. Order is the order in which the likely parents were chosen. Number_of_reads is the number of reads that mapped to that parent but not to any of the previously chosen parents. Cumulative_reads is the number of reads mapping to all the likely parents chosen up to and including that parent.</p>"},{"location":"UserInstructions/FindLikelyParents/#as-part-of-path-finding","title":"As Part of Path Finding","text":"<p>The same method is used as part of path finding for the default (\"efficient\") algorithms for haploid and diploid path finding. For path finding, the default values of maxCoverage (1.0) and maxParents(Int.MAX_VALUE) mean that all taxa in the haplotype graph will be used  for path finding. In that case, the likely parents method will not be used. It will only be used if  maxParents &lt; number of taxa in the haplotype graph or maxCoverage &lt; 1.0.</p>"},{"location":"UserInstructions/HaplotypeMethod/","title":"Haplotype Methods","text":"<p>A haplotype method describes a set of haplotypes. In the database, haplotypes have methods which indicate how a haplotype was created. Haplotypes created from assemblies have a user defined method name with no default.   Haplotypes created from WGS have a user assigned method name, which defaults to \"GATK_PIPELINE\". Consensus haplotypes also have a user assigned method name, such as \"CONSENSUS\". In the PHG database, reference ranges are assigned to groups. The default initia two groups of reference ranges are named \"FocusRegion\" and \"FocueComplement\". Additional groups can be assigned by users. A haplotype method for building a graph can be a haplotype method, a comma-separated list of haplotype methods, a refRange group:haplotype method pair, or a comma-separated list of pairs.</p>"},{"location":"UserInstructions/ImputePHG_Simple/","title":"I have a fastq file - I want a PHG path","text":"<p>This walks through step by step how to use an existing PHG to go from fastq (WGS or GBS) to a PHG path</p>"},{"location":"UserInstructions/ImputePHG_Simple/#workflow-summary","title":"Workflow Summary","text":"<p>The workflow proceeds through a few distinct steps. The pipeline can run up to any of these steps:</p> <ol> <li>Write haplotypes to a pangenome.fa fasta</li> <li>Map reads (e.g. WGS or GBS fastq files) to the pangenome.fa using minimap2</li> <li>Use the reads to find the most likely path for each taxon through the Haplotype Graph</li> <li>Write the variants on each path to a VCF file </li> </ol>"},{"location":"UserInstructions/ImputePHG_Simple/#quick-start","title":"Quick Start","text":""},{"location":"UserInstructions/ImputePHG_Simple/#1-start-up-a-terminal","title":"1. Start up a terminal","text":"<p>For example, at Cornell we log into CBSU: <code>ssh user123@cbsumm123.tc.cornell.edu</code></p>"},{"location":"UserInstructions/ImputePHG_Simple/#2-install-the-phg-and-liquibase-docker-on-your-computer","title":"2. Install the PHG and liquibase docker on your computer","text":"<p>(IMPORTANT: on CBSU machines use docker1 instead of docker for all commands)  </p> <pre><code>```\ndocker pull maizegenetics/phg\n\ndocker pull maizegenetics/phg_liquibase \n```\n</code></pre>"},{"location":"UserInstructions/ImputePHG_Simple/#3-next-create-the-needed-directory-structure-in-a-high-speed-working-directory","title":"3. Next create the needed directory structure in a high speed working directory","text":"<p>(note: on CBSU machines this must be in your machine's local storage i.e. '/workdir')</p> <pre><code>cd /workdir/user123\nmkdir phgwd\nWORKING_DIR=/workdir/user123/phgwd/\ndocker run --name create_directory --rm \\\n    -v ${WORKING_DIR}/:/phg/ \\\n    -t maizegenetics/phg:latest \\\n    /tassel-5-standalone/run_pipeline.pl -debug -MakeDefaultDirectoryPlugin -workingDir /phg/ -endPlugin\n</code></pre> <p>If you are on a CBSU machine, you may need to claim the directory in order to add your files, e.g.</p> <pre><code>```\ndocker claim /workdir/user123/phgwd/\n```\n</code></pre>"},{"location":"UserInstructions/ImputePHG_Simple/#4-move-files-into-the-appropriate-folders","title":"4. Move files into the appropriate folders.","text":"<p>For example, fastq files should go into: '/workdir/user123/phgwd/inputDir/imputation/fastq`  </p> <p>If you already have a pangenome fasta and index, they go into: '/workdir/user123/phgwd/outputDir/pangenome/`  </p> <p>Your config and read mapping sample key files can go in the base directory: '/workdir/user123/phgwd/'</p> <p>For transferring files, at CBSU we like to use FileZilla - and follow these directions.</p>"},{"location":"UserInstructions/ImputePHG_Simple/#5-create-a-tab-delimited-txt-read-mapping-keyfile-with-information-about-reads-to-be-imputed-cultivar-flowcell_lane-and-filename-are-required-columns","title":"5. Create a tab-delimited (.txt) read mapping keyfile with information about reads to be imputed. cultivar, flowcell_lane and filename are required columns.","text":"<p>cultivar is the name that the sample will be given in the db. flowcell_lane can be a dummy variable.</p> <pre><code>cultivar    flowcell_lane   filename    PlateID\nRecLineB1RefA1gco4_wgs  wgsFlowcell RecLineB1RefA1gco4_R1.fastq wgs\nRecLineB1RefA1gco4_gbs  gbsFlowcell RecLineB1RefA1gco4_R1_gbs.fastq gbs\nRecRefA1LineBgco6_wgs   wgsFlowcell RecRefA1LineBgco6_R1.fastq  wgs\nRecRefA1LineBgco6_gbs   gbsFlowcell RecRefA1LineBgco6_R1_gbs.fastq  gbs\n</code></pre>"},{"location":"UserInstructions/ImputePHG_Simple/#6-create-a-config-file-and-fill-the-variables-with-unassigned-values","title":"6. Create a config file and fill the variables with UNASSIGNED values.","text":"<p>These variables with UNASSIGNED values are user-defined variables that will name how the data is stored with the values set on them. Other variables set parameters for each of the steps. The config file is set up in \"chunks\" that control individual steps of the imputation pipeline. See \"Details\" section at bottom of page for in-depth information.</p> <pre><code># Imputation Pipeline parameters for fastq or SAM files\n# How are strings parsed.\n\n#!!! Required Parameters !!!\n#--- Database ---\n#Database parameters will be specific to the database you are using\nhost=localHost\nuser=sqlite\npassword=sqlite\n#Name of DB (if sqlite include path, e.g. /workdir/user123/phgwd/phg_maize.db)\nDB=**UNASSIGNED**\nDBtype=sqlite\n\n#--- Used by liquibase to check DB version ---\nliquibaseOutdir=/phg/outputDir\n\n#--- Used for writing a pangenome reference fasta (not needed when inputType=vcf) ---\n#pangenomeHaplotypeMethod describes the set of haplotypes used to construct the pangenome fasta\n#e.g. pangenomeHaplotypeMethod=mummer4 (or can include FocusRegion -- i.e. haplotype method name, RefRange method name)\npangenomeHaplotypeMethod=**UNASSIGNED**\npangenomeDir=/phg/outputDir/pangenome\nindexKmerLength=21\nindexWindowSize=11\nindexNumberBases=90G\n\n#--- Used for mapping reads\ninputType=fastq\n#readMethod is the user-defined name for read mapping of parameters. e.g. readMethod=fastq_normal_error\nreadMethod=**UNASSIGNED**\nkeyFile=/phg/readMapping_key_file.txt\nfastqDir=/phg/inputDir/imputation/fastq/\nsamDir=/phg/inputDir/imputation/sam/\nlowMemMode=true\nmaxRefRangeErr=0.25\noutputSecondaryStats=false\nmaxSecondary=20\nfParameter=f1000,5000\nminimapLocation=minimap2\n\n#--- Used for path finding\n#pathHaplotypeMethod is the input graph to use and can be restricted to certain reference ranges\n#e.g. pathHaplotypeMethod=mummer4,FocusRegion  -- i.e. haplotype method name, RefRange method name\npathHaplotypeMethod=**UNASSIGNED*\n#pathMethod is the user-defined name for path finding parameters. e.g. pathMethod=pathMethod_2020_12_15\npathMethod=**UNASSIGNED*\nmaxNodes=1000\nmaxReads=10000\nminReads=1\nminTaxa=20\nminTransitionProb=0.001\nnumThreads=3\nprobCorrect=0.99\nremoveEqual=true\nsplitNodes=true\nsplitProb=0.99\nusebf=false\n#   used by haploid path finding only\nusebf=false\nminP=0.8\n#   used by diploid path finding only\nmaxHap=11\nmaxReadsKB=100\nalgorithmType=classic\n\n#--- Used to output a vcf file for pathMethod\n#e.g. outVcfFile=/phg/outputDir/myVCF.vcf.gz\noutVcfFile=**UNASSIGNED**\n#~~~ Optional Parameters ~~~\npangenomeIndexName=**OPTIONAL**\n#readMethodDescription=**OPTIONAL**\n#pathMethodDescription=**OPTIONAL**\n#debugDir=**OPTIONAL**\n#bfInfoFile=**OPTIONAL**\n</code></pre>"},{"location":"UserInstructions/ImputePHG_Simple/#7-run-one-of-the-commands-in-the-examples-section","title":"7. Run one of the commands in the examples section.","text":""},{"location":"UserInstructions/ImputePHG_Simple/#8-view-haplotypes-with-rphg","title":"8. View haplotypes with rPHG","text":""},{"location":"UserInstructions/ImputePHG_Simple/#examples-executing-workflows","title":"Examples: Executing workflows","text":"<p>In the examples below, all log and debug messages will be written to the console. To write them to a log file append  <code>&gt; /path/to/logfile.log</code> or <code>&amp;&gt; /path/to/logfile.log</code> to the end of your command. In the first format, stdout will be written to the log file and stderr  will be written to the console. In the second format, both stdout and stderr will be written to file.  </p> <p>imputeTarget, which describes the endpoint at which the workflow will stop, must equal one of pangenome, path, diploidPath, pathToVCF, or diploidPathToVCF. All steps required to reach the endpoint will be executed. If any of the steps have already been run, they will not be repeated but will be skipped instead.</p>"},{"location":"UserInstructions/ImputePHG_Simple/#a-create-a-pangenome-fasta-file-then-stop","title":"A. Create a pangenome fasta file then stop","text":"<ul> <li>config file parameters that need to be set: DB, pangenomeHaplotypeMethod</li> <li>command:</li> </ul> <pre><code>WORKING_DIR=/workdir/user123/phgwd/ \ndocker run --name pipeline_container --rm -v ${WORKING_DIR}:/phg/ \\ \n-t maizegenetics/phg:latest /tassel-5-standalone/run_pipeline.pl -Xmx20G -debug \\ \n-configParameters /phg/myConfigFile.txt -ImputePipelinePlugin -imputeTarget pangenome -endPlugin\n</code></pre> <ul> <li>result: The pangenome fasta and index will be written.</li> </ul>"},{"location":"UserInstructions/ImputePHG_Simple/#b-impute-variants-from-fastq-files-homozygous","title":"B. Impute variants from fastq files - homozygous","text":"<ul> <li>config file parameter to set: inputType=fastq</li> <li>command:</li> </ul> <pre><code>WORKING_DIR=/workdir/user123/phgwd/ \ndocker run --name pipeline_container --rm -v ${WORKING_DIR}:/phg/ \\ \n-t maizegenetics/phg:latest /tassel-5-standalone/run_pipeline.pl -Xmx20G -debug \\ \n-configParameters /phg/myConfigFile.txt -ImputePipelinePlugin -imputeTarget path -endPlugin\n</code></pre> <ul> <li>result: Read mapping counts and imputed paths will be stored in the PHG database.</li> </ul>"},{"location":"UserInstructions/ImputePHG_Simple/#c-impute-variants-from-fastq-files-heterozygous","title":"C. Impute variants from fastq files - heterozygous","text":"<ul> <li>config file parameter to set: inputType=fastq</li> <li>command:</li> </ul> <pre><code>WORKING_DIR=/workdir/user123/phgwd/ \ndocker run --name pipeline_container --rm -v ${WORKING_DIR}:/phg/ \\ \n-t maizegenetics/phg:latest /tassel-5-standalone/run_pipeline.pl -Xmx20G -debug \\ \n-configParameters /phg/myConfigFile.txt -ImputePipelinePlugin -imputeTarget diploidPath -endPlugin\n</code></pre> <ul> <li>result: Read mapping counts and imputed paths will be stored in the PHG database.</li> </ul>"},{"location":"UserInstructions/ImputePHG_Simple/#d-impute-variants-from-a-vcf-file-homozygous","title":"D. Impute variants from a VCF file - homozygous","text":"<ul> <li>config file parameter to set: inputType=vcf</li> <li>command:</li> </ul> <pre><code>WORKING_DIR=/workdir/user123/phgwd/ \ndocker run --name pipeline_container --rm -v ${WORKING_DIR}:/phg/ \\ \n-t maizegenetics/phg:latest /tassel-5-standalone/run_pipeline.pl -Xmx20G -debug \\ \n-configParameters /phg/myConfigFile.txt -ImputePipelinePlugin -imputeTarget path -endPlugin\n</code></pre> <ul> <li>result: Read mapping counts and imputed paths will be stored in the PHG database.</li> </ul>"},{"location":"UserInstructions/ImputePHG_Simple/#e-export-imputed-vcf-from-fastq-files-homozygous","title":"E. Export imputed VCF from fastq files - homozygous","text":"<ul> <li>config file parameters to set: inputType=fastq, outVcfFile=/path/to/vcf_output.vcf</li> <li>command:</li> </ul> <pre><code>WORKING_DIR=/workdir/user123/phgwd/ \ndocker run --name pipeline_container --rm -v ${WORKING_DIR}:/phg/ \\ \n-t maizegenetics/phg:latest /tassel-5-standalone/run_pipeline.pl -Xmx20G -debug \\ \n-configParameters /phg/myConfigFile.txt -ImputePipelinePlugin -imputeTarget pathToVCF -endPlugin\n</code></pre> <ul> <li>result: Read mapping counts and imputed paths will be stored in the PHG database. Variants for all the paths stored for pathMethod will be written to a VCF file.</li> </ul>"},{"location":"UserInstructions/ImputePHG_Simple/#f-export-imputed-vcf-from-fastq-files-heterozygous","title":"F. Export imputed VCF from fastq files - heterozygous","text":"<ul> <li>config file parameters to set: inputType=fastq, outVcfFile=/path/to/vcf_output.vcf</li> <li>command:</li> </ul> <pre><code>WORKING_DIR=/workdir/user123/phgwd/ \ndocker run --name pipeline_container --rm -v ${WORKING_DIR}:/phg/ \\ \n-t maizegenetics/phg:latest /tassel-5-standalone/run_pipeline.pl -Xmx20G -debug \\ \n-configParameters /phg/myConfigFile.txt -ImputePipelinePlugin -imputeTarget diploidPathToVCF -endPlugin\n</code></pre> <ul> <li>result: Read mapping counts and imputed paths will be stored in the PHG database. Variants for all the paths stored for pathMethod will be written to a VCF file.</li> </ul>"},{"location":"UserInstructions/ImputePHG_Simple/#details","title":"Details","text":""},{"location":"UserInstructions/ImputePHG_Simple/#writing-a-config-file","title":"Writing a config file","text":"<p>Before running the pipeline, you need to create a config file and put it in \"baseDir\" or some other directory accessible by the Docker. The config file name does not have to be config.txt. It also can be in any subdirectory of baseDir as long as the same file name location is used for -configParameters in the full pipeline command.  Two sample config files are provided, one for imputing haplotypes from sequence in fastq or SAM files and another for imputing haplotypes from variants in a VCF file.  Values of UNASSIGNED in a config file must be filled in before using it. See the section Setting parameters in the config file for more information about what those values should be.</p>"},{"location":"UserInstructions/ImputePHG_Simple/#to-run-the-pipeline","title":"To run the pipeline","text":"<p>The pipeline is run using a single TASSEL pipeline command. All of the parameters used by the pipeline are set in the config file. The following Docker command can be used to run the full pipeline, replacing baseDir, dockerImageName, and -Xmx10G with the appropriate values: <code>docker run --name pipeline_container --rm -v baseDir:/phg/ -t dockerImageName /tassel-5-standalone/run_pipeline.pl -Xmx10G -debug -configParameters /phg/config.txt -ImputePipelinePlugin -imputeTarget pathToVCF -endPlugin</code> When the pipeline is run it starts at the beginning and runs to the target. Possible values for imputeTarget are config, pangenome, map, path, diploidPath, pathToVCF, and diploidPathToVCF. \"pangenome\" stops after writing a pangenome fasta and using minimap2 to index it. \"map\" stops after mapping the sequence or variants to the pangenome haplotypes. \"path\" stops after writing the imputed paths to the database. In each case, prior steps in the pipeline already completed will not be rerun but will be skipped instead. Thus, read mappings or paths already computed will not be over-written.</p>"},{"location":"UserInstructions/ImputePHG_Simple/#running-the-pipeline-with-different-parameter-settings","title":"Running the pipeline with different parameter settings","text":"<p>When reprocessing the same samples with different parameter settings, the method names must be changed. If the method names are not changed, then read mappings or paths will already exist for those sample names and will not be overwritten. If read mapping parameters used to create the pangenome change, new readMethod and pathMethod names must be used. If only path finding parameters change, then only change pathMethod. In that case the pipeline will use the existing readMethod data to compute new paths.</p> <p>If an existing configuration file is modified with new parameter settings and method names, the new configuration file should be saved under a different name. The configuration files provide a record of how analyses were run.</p> <p>Parameter values used for read mapping are stored in the database with readMethod the first time that method is used. If any of those parameter values are changed, then the readMethod name should be changed as well so that the database has an accurate record of parameter values associated with each method name. The same is true for pathMethod. Because the pangenome fasta and index are not stored in the database, the parameters used for the pangenome are not stored either. Instead, the haplotype method and indexing parameters are encoded in the file names.</p>"},{"location":"UserInstructions/ImputePHG_Simple/#using-a-different-aligner-to-map-reads","title":"Using a different aligner to map reads","text":"<p>The PHG pipeline uses minimap2 to map reads to a pangenome created from haplotypes in the PHG database. Users can choose to use a different aligner by first creating the pangenome then using an alternative aligner to create SAM or BAM files, which can then be used as inputs to the pipeline instead of fastq files. To create a pangenome fasta, run the pipeline with -imputeTarget pangenome. After the pangenome is created, then an alternative aligner can be used to map reads to the pangenome reference. It the output is saved as either a SAM or BAM file, then the other pipeline imputeTargets can be run with inputType=sam set in the config file.</p>"},{"location":"UserInstructions/ImputePHG_Simple/#setting-parameters-in-the-config-file","title":"Setting parameters in the config file","text":""},{"location":"UserInstructions/ImputePHG_Simple/#database-parameters","title":"Database Parameters","text":"<p>The database parameters are required to make a connection to the PHG database.They are</p> <ul> <li>host</li> <li>user</li> <li>password</li> <li>DBtype</li> <li>DB</li> </ul> <p>The default values are those needed to connect to a SQLite database and must be changed if using PostgreSQL. \"DB\", which is the path to the database does not have a default and is required.</p>"},{"location":"UserInstructions/ImputePHG_Simple/#method-names","title":"Method names","text":"<ul> <li>pangenomeHaplotypeMethod</li> <li>readMethod</li> <li>pathHaplotypeMethod</li> <li>pathMethod</li> </ul> <p>The two xxxHaplotypeMethods must be set to a valid haplotype method, which can be a haplotype method or a comma-separated list of refRange group: haplotype method pairs. The pangenomeHaplotypeMethod describes the set of haplotypes used to construct the pangenome fasta. It can be any of the method names assigned to haplotypes when they were added to the database from assemblies or WGS or can be a consensus haplotype method. Both the pangenome writing step and the read mapping step have to use the same value for pangenomeHaplotypeMethod. As long as the same configuration file is used for both steps that happens automatically. The pathHaplotypeMethod describes the set of haplotypes to be used for pathing. It can be the same as the pangenomeHaplotypeMethod or a subset of it.</p> <p>The same data can be imputed in many ways using different combinations of parameter settings. Each combination of settings must be assigned a unique method name by the user and the resulting data is stored in the database under that method name. The parameter values for each method are stored in the database under \"method description\" as a JSON formatted string. The readMethod is associated with the read mapping parameters. If any of the read mapping parameters are changed, the method name should be changed as well. The pathMethod is a user assigned name that is associated with a particular set of path finding and read mapping parameters. </p>"},{"location":"UserInstructions/ImputePHG_Simple/#key-files","title":"Key files","text":"<p>Key files contain a list of samples and files to be processed. The parameter \"keyFile\" names the key file used to generate read mappings. The pipeline generates two additional key files from that. The pathing step uses a key file with the same name as the named read mapping keyFile plus the extension \"_pathKeyFile.txt\". The file with an extension of \"_mappingIds.txt\" is the original keyFile with the database ids of the stored read mappings. Generally, this will not be needed but can be used to generate specialized analyses. After the pipeline has been run, the _pathKeyFile and _mappingIds can be found in the same directory as keyFile.</p> <p>More information about keyfiles can be found here</p>"},{"location":"UserInstructions/ImputePHG_Simple/#file-locations","title":"File locations","text":"<p>A few parameters are required to define expected locations of files. The default values work with the PHG Docker when MakeDefaultDirectoryPlugin is used to create directories. These parameters are as follows:</p> <ul> <li>pangenomeDir [default = /phg/outputDir/pangenome/]</li> <li>fastqDir [default = /phg/inputDir/imputation/fastq/]</li> <li>samDir [default = /phg/inputDir/imputation/sam/]</li> <li>minimapLocation [default = minimap2]</li> <li>outVcfFile [no default, must be assigned a value if writing a VCF file]</li> </ul>"},{"location":"UserInstructions/ImputePHG_Simple/#pangenome-minimap2-index-parameters","title":"Pangenome minimap2 index parameters","text":"<p>Check minimap2 manual for documentation</p> <ul> <li>indexKmerLength [minimap2 -k, default=21]</li> <li>indexWindowSize=11 [minimap2 -w, default=11]</li> <li>indexNumberBases=90G [minimap2 -I, default=90G]</li> </ul>"},{"location":"UserInstructions/ImputePHG_Simple/#read-mapping-parameters-with-default-values","title":"Read mapping parameters with default values","text":"<ul> <li>lowMemMode [default=true]</li> <li>maxRefRangeErr [default=0.25]</li> <li>outputSecondaryStats [default=false]</li> <li>maxSecondary [default=20]</li> <li>fParam [default=f1000,5000]</li> </ul> <p>lowMemMode is an option that was introduced to reduce memory usage. It works so well that we recommend always using this setting and will probably remove the parameter at some point.</p> <p>When a read is mapped to a pangenome, it may map equally well to multiple haplotypes. All mappings with the lowest edit distance are used. When some of those reads map to different reference ranges, only those mapping to the most frequently hit reference range are used. If the proportion of the mappings to other reference ranges exceeds maxRefRangeErr, that read is discarded. When this parameter is set to a lower value, fewer reads are used but overall mapping accuracy may improve. The impact on final imputation accuracy depends on the pangenome being mapped and on other parameter settings. The optimal value for this parameter will need to be determined empirically.</p> <p>maxSecondary is the minimap2 -N parameter. When a read maps equally well to more than one haplotype, which is often the case, minimap2 calls one of those mappings the primary alignment and the rest secondary alignments. maxSecondary is the maximum number of secondary alignments saved. For that reason, maxSecondary should be greater than or equal to the maximum number of haplotypes per reference range. Otherwise, there may be a loss of imputation accuracy. fParam is the minimap2 -f parameter.</p>"},{"location":"UserInstructions/ImputePHG_Simple/#pathing-parameters-with-default-values","title":"Pathing parameters with default values","text":"<ul> <li>maxNodes [default=1000]</li> <li>minReads [default=1]</li> <li>minTaxa [default=20]</li> <li>minTransitionProb [default=0.001]</li> <li>numThreads [default=3]</li> <li>probCorrect [default=0.99]</li> <li>removeEqual [default=true]</li> <li>splitNodes [default=true]</li> <li>splitProb [default=0.99]</li> <li>algorithmType [default=efficient]</li> <li> </li> <li>maxReads [default=10000]</li> <li>usebf [default=false]</li> <li>minP [default=0.8]</li> <li> </li> <li>maxHap [default=11]</li> <li>maxReadsKB [default=100]</li> </ul> <p>Reference ranges with more than maxNodes number of nodes or fewer than minTaxa taxa will not be used for imputation. For each sample, ranges are also eliminated that have fewer than minReads mapped to that range or more then maxReads (haploid pathing) or maxReadsKB (dipoid pathing) reads per KB of reference sequence. If removeEqual=true and minReads &gt; 0, ranges for which all taxa have the same  number of reads mapping to them will be eliminated.</p> <p>After any ranges have been removed due to the filter criteria, a node with a zero-length haplotype is added for all taxa that have no haplotype in any given reference range. If splitNodes=true, nodes with more than one taxon are split into individual taxa. The transition probability between nodes of the same taxon are set to splitProb and the transition probability between a taxon and any other taxon is set to (1 - splitProb) / (n - 1) where n = number of taxa in the graph. The minimum value for any transition is set to minTransitionProb. It is recommended that minTransitionProbe &lt;= (1 - splitProb) / (n - 1). It is recommended that splitNodes not be set to false because this generally leads to lower imputation accuracy. If splitNodes = false, the transition probability is set to the proportion of times that transition is observed in the haplotypes in the database while unobserved transitions are set to minTransition.</p> <p>The number of threads that will be used to impute individual paths is numThreads - 2 because 2 threads are reserved for other operations. As a result, numThreads must be set to 4 or more in order to run pathing in parallel.</p> <p>The algorithmType=classic uses the classic Viterbi (or forward-backward) algorithms. algorithmType=efficient uses a more efficient version of the algorithms but is considered experimental at this time pending more extensive testing.</p> <p>When usebf=false, the Viterbi algorithm is used for pathing. If usebf=true, the forward-backward algorithm is used and  only nodes with a probability of minP or greater will be included in the final path. The forward-backward algorithm has been implemented  for haploid path finding but not for diploid path finding.</p> <p>For diploid path finding, only ranges with maxHap haplotypes or fewer will be used.</p>"},{"location":"UserInstructions/ImputePHG_Simple/#used-by-haploid-path-finding-only","title":"used by haploid path finding only","text":""},{"location":"UserInstructions/ImputePHG_Simple/#used-by-diploid-path-finding-only","title":"used by diploid path finding only","text":""},{"location":"UserInstructions/ImputePHG_Simple/#optional-parameters","title":"Optional parameters","text":"<ul> <li>pangenomeIndexName</li> <li>readMethodDescription</li> <li>pathMethodDescription</li> <li>debugDir</li> <li>bfInfoFile</li> </ul> <p>To use the optional parameters in the configuration file, replace OPTIONAL with the appropriate value and uncomment the line.</p> <p>*pangenomeIndexName is the name of an externally supplied pangenome index. It is best not to supply this values and instead to let the pipeline build and imdex the pangenome  and assign the index name to ensure that it is compatible with the rest of the pipeline.</p> <p>readMethodDescription and pathMethodDescription are optional user descriptions for these methods which will be   included in the database description with the tag \"notes=\" in addition to the parameter values used for that method.</p> <p>If a value is supplied for debugDir, for each sample fastq a text file containing the read mapping haplotype counts will be written. The same data  will stored in the database whether or not a value is assigned to debugDir. Generally this is not needed but may be useful  for debugging obscure problems.</p> <p>When usebf=true, the forward-backward algorithm will calculate probabilities for every haplotype node in the graph. These values are used  to determine which haplotypes are included in the path. If a value is supplied for bfInfoFile all of the haplotype probabilities  will be written to that file.</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/ImputeWithPHG_VCF/","title":"Impute variants from a VCF file","text":""},{"location":"UserInstructions/ImputeWithPHG_VCF/#in-progress-needs-updating","title":"In progress, needs updating","text":""},{"location":"UserInstructions/ImputeWithPHG_VCF/#quick-start","title":"Quick Start","text":"<ol> <li>Change <code>input.vcf</code>, <code>param3</code>, and <code>param4</code> in your config file to match file paths on your computer.</li> <li>Run <code>phg imputeFromVCF [config.txt]</code></li> </ol>"},{"location":"UserInstructions/ImputeWithPHG_VCF/#process-details","title":"Process Details","text":"<p>This process calls the SNPToReadMappingPlugin from tassel to assign haplotypes from variants in a VCF file. This bypasses the need for aligning reads to call haplotypes. These mapped haplotypes are then used for the Path and export vcf steps. A VCF is then created with variants from all reference ranges.  </p>"},{"location":"UserInstructions/ImputeWithPHG_VCF/#kitchen-sink","title":"Kitchen Sink","text":"<p>This process calls the SNPToReadMappingPlugin which makes it possible to assign haplotypes from variants in a VCF file. We recommend that you only use this option if you do not have fastq data available and are very confident that the SNPs in the VCF are representative of the population you are working with.</p> <p>There are XX parameters used in this step:</p> <p>add parameters and descriptions for whole step (reading VCF, comparing SNPs to PHG, predicting path through graph) when known</p> <ul> <li>keyFile</li> <li>indexFile</li> <li>vcfDir</li> <li>outputDir</li> <li>methodName</li> <li>methodDescription</li> <li>countAlleleDepths</li> <li>another parameter?</li> <li>another parameter?</li> </ul>"},{"location":"UserInstructions/ImputeWithPHG_VCF/#details-on-running-this-step-with-wrapper-scripts","title":"Details on running this step with wrapper scripts","text":"<p>When running this step on the command line, all file paths and parameters are set in the config file. The only call that needs to be run in the terminal is <code>phg imputeFromVCF /path/to/config.txt</code>. If you would like to overwrite the parameters set in the config file, you can do that by setting the parameters on the command line directly.</p> <p>For example, to ignore the config file vcfDir and set one directly, you could run:</p> <pre><code>phg findPaths -configFile /path/to/config.txt -vcfDir /path/to/vcf_files/\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_VCF/#details-on-running-this-step-through-docker","title":"Details on running this step through docker","text":""},{"location":"UserInstructions/ImputeWithPHG_VCF/#files","title":"Files","text":"<p>Config file</p> <p>An example can be found here: Master config file</p>"},{"location":"UserInstructions/ImputeWithPHG_VCF/#plugins","title":"Plugins","text":"<p>Four plugins are required to predict a path through the PHG database using SNPs from an input VCF file. The IndexHaplotypesBySNPPlugin indexes the pangenome and VCF files to make it possible to compare SNPs between them. It outputs an index file that is used by the SNPToReadMappingPlugin, which builds reads used by the FastqToMappingPlugin and BestHaplotypePathPlugin.</p> <p></p>"},{"location":"UserInstructions/ImputeWithPHG_VCF/#indexhaplotypesbysnpplugin","title":"IndexHaplotypesBySNPPlugin","text":"<p>Creates an index file of the PHG by the SNP values contained within the database. The output is a file with 7 columns in the form:</p> <pre><code>refRangeId  chr positionsrefAllele  altAllele   refHapIds   altHapIds\n1   1   24  A   T   102,104 103\n1   1   95  C   T   103,104 102\n1   1   101 G   T   103,104 102\n</code></pre> <p>This index is then used in the SNPToReadMappingPlugin step to compare a VCF file with the index and create readMappings.</p> <p>This plugin makes use of the graph stream so it is RAM efficient even though we are loading in the variants.</p> <p>Steps:</p> <ol> <li>Get a RangeMap of position to reference range Ids (used for fast lookup for what haplotypes to pull out for each SNP in VCF</li> <li>Associate each SNP with a reference range and add to a map</li> <li>Get the graphStream with Variants</li> <li>For each Reference range, go through each Haplotype's SNP list and check to see if they are one of the SNPs to score</li> <li>If so, add to either the reference or alternate list</li> <li>Collect the results from the graphStream</li> <li>Output the results to the index file.</li> </ol> <p>The IndexHaplotypesBySNPPlugin takes 4 parameters.</p> <ul> <li>-vcfFile  Name of the VCF file which holds the initial mappings to a single reference. (REQUIRED) <li>-outputIndexFile  Index file output (REQUIRED) <li>-configFile  Database configuration file (REQUIRED) <li>-methods  Pairs of methods (haplotype method name and range group method name). Method pair separated by a comma, and pairs separated by colon. The range group is optional \\n\" + \"Usage: ,:,: (REQUIRED) <p>The IndexHaplotypeBySNPPlugin requires a haplotype graph, which can be obtained by chaining the HaplotypeGraphBuilderPlugin to the IndexHaplotypeBySNPPlugin. Here is an example of how to do that:</p> <pre><code>example code needed here\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_VCF/#snptoreadmappingplugin","title":"SNPToReadMappingPlugin","text":"<p>This plugin takes as input a VCF file. It uses the SNP positions in the VCF and compares these to positions in the reference genome in the PHG. It builds 150bp Illumina reads using the SNPs in the VCF file, filling in non-SNP positions with reference sequence, then uses these reads with the FastqToMappingPlugin and BestHaplotypePathPlugin to identify the best path through the graph. </p> <p>The SNPToReadMappingPlugin requires output from the IndexHaplotypesBySNPPlugin. It takes 7 parameters.</p> <ul> <li>-keyFile  Name of the Keyfile to process.  Must have columns flowcell_lane and filename. In the filename column a VCF file needs to be specified.  The taxa name will be pulled from the VCF. (REQUIRED) <li>-indexFile  Name of the SNP to HapIdList index file created by IndexHaplotypesBySNPPlugin (REQUIRED) <li>-vcfDir  Directory of VCF files (REQUIRED) <li>-outputDir  Read Mapping Output Directory (REQUIRED) <li>-methodName  Method name to be stored in the DB (REQUIRED) <li>-methodDescription  Method description to be stored in the DB (REQUIRED) <li>-countAlleleDepths  Use the Allele Depths as the counts for the Read Mappings instead of SNPs (OPTIONAL) <p>Example for how to run the SNPToReadMappingPlugin using TASSEL plugins:</p> <pre><code>perl /tassel-5-standalone/run_pipeline.pl -debug ${fullXmx} \\\n-SNPToReadMappingPlugin \\\n    -keyFile ${keyFile} \\\n    -indexFile ${index} \\\n    -vcfDir ${VCF} \\\n    -outputDir ${output_dir} \\\n    -methodName method1 \\\n    -methodDescription ${description} \\\n    -countAlleleDepths true\n    -endPlugin\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_VCF/#fastqtomappingplugin","title":"FastqToMappingPlugin","text":"<p>The FastqToMappingPlugin executes code to count the number of reads which align to a haplotype node using Minimap 2. </p> <p>If running in paired mode, it will align the read pairs using minimap and process the resulting SAM records. Only equally optimal mappings (by Edit Distance (NM) are kept. Additional filtering is done to remove reads which are unmapped or clipped. </p> <p>If running in paired mode, the reads must be on opposite strands and both must currently hit the same haplotype in a single reference range. Optimal mappins across reference ranges are processed, but reads are only assigned to haplotypes in the reference range with the most hits (providing that at least 1-maxRefRangeError percentage hit that reference range). Based on testing, this seems to work the best to balance the number of reads used with accuracy. The plugin requires a PHG Haplotype Graph, which can be acquired by chaining the running of HaplotypeGraphBuilderPlugin and this plugin. </p> <p>The output from the HaplotypeGraphBuilderPlugin will be input to the BestHaplotypePathPlugin.</p> <p>The parameters for this plugin are:</p> <ul> <li>-configFile  DB Config File containing properties host,user,password,DB and DBtype where DBtype is either sqlite or postgres (Default=null) (REQUIRED) A sample config file can be found here:Master Config File. <li>-minimap2IndexFile  : Name of the indexFile file to process (required) <li>-keyFile  : Name of the Keyfile to process. Must have columns cultivar, flowcell_lane, filename, and PlateID. Optionally for paired end reads, filename2 is needed. If filename2 is not supplied, Minimap2 will run in single end mode. Otherwise will be paired. (required) <li>-fastqDir  : Name of the Fastq dir to process. (required) <li>-maxRefRangeErr  : Maximum allowed error when choosing best reference range to count.  Error is computed 1 - (mostHitRefCount/totalHits) (Default: 0.25) <li>-lowMemMode  : Run in low memory mode. (Default: true) <li>-maxSecondary  : Maximum number of secondary alignments to be returned by minimap2. This will be the value of the -N parameter in the minimap2 command line. If the value is too low, some valid read mappings will not be reported. (Default: 20) <li>-minimapLocation  : Location of Minimap2 on file system.  This defaults to use minimap2 if it is on the PATH environment variable. (Default: minimap2) <li>-methodName  : Method name to be stored in the DB. (required) <li>-methodDescription  : Method description to be stored in the DB. (required) <li>-debugDir  : Directory to write out the read mapping files.  This is optional for debug purposes. (Default: ) <li>-outputSecondaryStats  : Ouptput Secondary Mapping Statistics such as total AS for each haplotype ID (Default: false) <p>The FastqToMappingPlugin requires a haplotype graph, which can be obtained with the HaplotypeGraphBuilderPlugin. An example for chaining these plugins is below:</p> <pre><code>perl /tassel-5-standalone/run_pipeline.pl -debug ${fullXmx} \\\n-configParameters ${CONFIGFILE} \\\n-HaplotypeGraphBuilderPlugin \\\n    -configFile ${CONFIGFILE} \\\n    -methods ${HAPLOTYPE_METHOD} \\\n    -includeVariantContexts false \\\n    -includeSequences false \\\n    -endPlugin \\\n-FastqToMappingPlugin \\\n    -minimap2IndexFile ${HAPLOTYPE_INDEX} \\\n    -keyFile ${KEY_FILE} \\\n    -fastqDir ${FASTQ_DIR}/ \\\n    -methodName ${HAP_COUNT_METHOD} \\\n    -methodDescription READ_MAPPING_DESCRIPTION \\\n    -debugDir $OUTPUT_DIR \\\n    -endPlugin\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_VCF/#besthaplotypepathplugin","title":"BestHaplotypePathPlugin","text":"<p>This plugin takes a haplotype graph and a set of read mappings to infer the best (most likely) path through the graph given the read mappings. Read mappings are a list of reads with a set of haplotypes to which that read aligned. </p> <p>The plugin can (1) take a file of read mappings and return a file with a list of haplotypes or (2) take read mappings from a PHG DB and store the resulting list of haplotypes in the DB.</p> <p>If (1), the input is a file, then the plugin can take either a file or a directory containing multiple files. If a directory, all read mapping files will be processed and the haplotype lists output as separate files to an output directory. If the output directory is not specified, then the lists will be written to the input directory. Any path files of the same name will not be overwritten; a message will be written to the log to that effect, unless the \"overwrite\" flag is set to true.</p> <p>If (2), the input comes from a PHG DB, an input read map method and the output path method must be supplied. In addition, a specific taxon or list of taxa for which paths are to be imputed can be suppled. If paths for any of the taxa and methods exist, the paths will not be imputed and a warning message will be written to the log file. If the \"overwrite\" flag is set to true, any existing paths will be overwritten and a message to that effect will be written to the log. </p> <p>The parameters for this plugin are:</p> <ul> <li>-keyFile  : KeyFile file name.  Must be a tab separated file using the following headers: SampleName    ReadMappingIds  LikelyParents<ul> <li>ReadMappingIds and LikelyParents need to be comma separated for multiple values (required)</li> </ul> <li>-readFile  : Filename of read mappings. Do not supply both a filename and a directory. <li>-readDir  : Directory of read mapping files. If this is supplied, do not also assign a read filename. <li>-outDir  : Directory to which path files will be written. <li>-readMethod  : The name of the read mapping method in the PHG DB (required) <li>-pathMethod  : The name of the path method used to write the results to the PHG DB (required) <li>-overwrite  : If an output pathfile already exists for a taxon, then it will be overwritten if overwrite = true. Otherwise, it will not and a warning will be written to the log. Likewise for paths in the PHG DB. (Default: false) <li>-minTaxa  : minimum number of taxa per anchor reference range. Ranges with fewer taxa will not be included in the output node list. (Default: 20) <li>-minReads  : minimum number of reads per anchor reference range. Ranges with fewer reads will not be included in the output node list. (Default: 1) <li>-maxReads  : maximum number of include counts per anchor reference range Kb. Ranges with more reads will not be included in the output node list. (Default: 10000) <li>-maxNodes  : maximum number of nodes per reference range. Ranges with more nodes will not be included in the output node list. (Default: 1000) <li>-minTransitionProb  : minimum probability of a transition between nodes at adjacent reference ranges. (Default: 0.001) <li>-probCorrect  : minimum number of reads per anchor reference range. Ranges with fewer reads will not be included in the output node list. (Default: 0.99) <li>-splitNodes  : split consensus nodes into one node per taxon. (Default: true) <li>-splitProb  : When the consensus nodes are split by taxa, this is the transition probability for moving from a node to the next node of the same taxon. It equals 1 minus the probability that the path will switch between taxa. (Default: 0.99) <li>-usebf  : Use the Backward-Forward algorithm instead of the Viterbi algorithm for the HMM. (Default: false) <li>-minP  : Only nodes with minP or greater probability will be kept in the path when using the Backward-Forward algorithm, (Default: 0.8) <li>-bfInfoFile  : The base name of the file to node probabilities from the backward-forward algorithm will be written. taxonName.txt will be appended to each file. <li>-removeEqual  : Ranges with equal read counts for all haplotypes should be removed from the graph. Defaults to true but will be always be false if minReads = 0. (Default: true) <li>-numThreads  : Number of threads used to find paths. The path finding will subtract 2 from this number to have the number of worker threads.  It leaves 1 thread for IO to the DB and 1 thread for the Operating System. (Default: 3) <li>-requiredTaxa  : Optional list of taxa required to have haplotypes. Any reference range that does not have a haplotype for one of these taxa will not be used for path finding. This can be a comma separated list of taxa (no spaces unless surrounded by quotes), file (.txt) with list of taxa names to include, or a taxa list file (.json or .json.gz). By default, all taxa will be included. <li>-algorithmType  : the type of algorithm. Choices are classic, which is the original implementation described by Rabiner 1989, or efficient, which is modified for improved computational efficiency. [classic, efficient] (Default: efficient) <p>The BestHaplotypePathPlugin requires a haplotype graph, which can be obtained with the HaplotypeGraphBuilderPlugin. An example for chaining these plugins is below:</p> <pre><code>/tassel-5-standalone/run_pipeline.pl -debug ${fullXmx} \\\n-configParameters ${CONFIGFILE} \\\n-HaplotypeGraphBuilderPlugin \\\n    -configFile ${CONFIGFILE} \\\n    -methods ${HAPLOTYPE_METHOD_FIND_PATH} \\\n    -includeVariantContexts false \\\n    -includeSequences false \\\n    -endPlugin \\\n-BestHaplotypePathPlugin \\\n    -keyFile ${PATH_KEY_FILE} \\\n    -outDir ${HAP_COUNT_BEST_PATH_DIR} \\\n    -readMethod ${HAP_COUNT_METHOD} \\\n    -pathMethod ${PATH_METHOD} \\\n    -endPlugin\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_VCF/#troubleshooting","title":"Troubleshooting","text":"<p>Return to Step 3 pipeline</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/ImputeWithPHG_compare/","title":"Compare PHG variants to existing variants","text":""},{"location":"UserInstructions/ImputeWithPHG_compare/#quick-start","title":"Quick Start","text":"<ol> <li>Change Truth_set.vcf, <code>param3</code>, and <code>param4</code> in your config file to match file paths on your computer.</li> <li>Run phg VCF_compare [config.txt]</li> </ol>"},{"location":"UserInstructions/ImputeWithPHG_compare/#process-details","title":"Process Details","text":"<p>This process does not exist formally at the moment.  But one can imagine basically specifying a trustworth VCF with high confidence SNPs and comparing to those variants imputed from PHG and computing accuracy.  </p>"},{"location":"UserInstructions/ImputeWithPHG_compare/#kitchen-sink","title":"Kitchen Sink","text":""},{"location":"UserInstructions/ImputeWithPHG_compare/#troubleshooting","title":"Troubleshooting","text":"<p>Return to Step 3 pipeline</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/ImputeWithPHG_exportVCF/","title":"Export variants as VCF","text":""},{"location":"UserInstructions/ImputeWithPHG_exportVCF/#quick-start","title":"Quick Start","text":"<ol> <li>Change <code>MethodName</code>, <code>Database</code>,  and <code>output.vcf</code> in your config file to match file paths on your computer.</li> <li>Run <code>phg pathsToVCF [config.txt]</code></li> </ol>"},{"location":"UserInstructions/ImputeWithPHG_exportVCF/#details","title":"Details","text":"<p>This Process exports a VCF corresponding to haplotype calls from the mapping and path plguins. It looks for the indicated path methodName and export variants for all taxa form this method. The name of the output VCF must be indicated in the config file.  </p>"},{"location":"UserInstructions/ImputeWithPHG_exportVCF/#kitchen-sink","title":"Kitchen Sink","text":"<p>The PathsToVCFPlugin is called after paths have been found and stored in the PHG database (see imputation pipeline) . It is used to create VCF files from haplotype paths. The user may specify certain reference ranges to be included in the exported file. Paths will have been added to the database and/or exported to text files with the previous step, which ran the FastqToMappingPlugin and the BestHaplotypePathPlugin. </p> <p>If haplotypes were not stored in the database and were only output to files, this step also requires that the ImportHaplotypePathFilePlugin is called before the PathsToVCFPlugin. </p> <p>The output from this step is a VCF file.</p> <p>There are 4 parameters used in this step:</p> <ul> <li>CONFIG_FILE: Configuration file containing properties host, user, password, DB and DBtype where DBtype is either sqlite or postgres. A sample config file can be found here:Master Config File</li> <li>CONSENSUS_METHOD: The Consensus Method used to create haplotypes in graph.</li> <li>OUTPUT_FILE: Output VCF file</li> <li>PATH_METHOD_NAME: Used to grab the correct paths from the DB.  This needs to match what was used in FindPathMinimap2.sh</li> </ul>"},{"location":"UserInstructions/ImputeWithPHG_exportVCF/#details-on-running-this-step-with-wrapper-scripts","title":"Details on running this step with wrapper scripts","text":"<p>When running this step on the command line, all file paths and parameters are set in the config file. The only call that needs to be run in the terminal is <code>phg pathsToVCF /path/to/config.txt</code>. If you would like to overwrite the parameters set in the config file, you can do that by setting the parameters on the command line directly.</p> <p>For example, to ignore the config file PATH_METHOD_NAME level and set one directly, you could run:</p> <pre><code>phg findPaths -configFile /path/to/config.txt -PATH_METHOD_NAME MyNewPath1\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_exportVCF/#details-on-running-this-step-through-docker","title":"Details on running this step through docker","text":"<p>When ExportPath.sh runs in a Docker container, the following mounted directories are expected</p> <ul> <li>Mount localMachine:/pathToOutputs/FindPathDir/ to docker:/tempFileDir/outputDir/. To make this work correctly, the DB must also be here.  If the DB is defined in a different place within the config file, you will need to make a new mount point accordingly.</li> <li>Mount localMachine:/pathToInputs/config.txt to docker:/tempFileDir/data/config.txt</li> </ul> <p>In addition, it is expected the database is stored in the User FindPathDir that is mounted below</p> <p>An example Docker script to run the ExportPath.sh shell script is:</p> <pre><code>#!python\n\ndocker run --name cbsu_phg_container_exportPath --rm \\\n    -v /workdir/user/DockerTuningTests/DockerOutput/FindPathDir/:/tempFileDir/outputDir/ \\\n    -v /workdir/user/DockerTuningTests/InputFiles/config.txt:/tempFileDir/data/config.txt \\\n    -t maizegenetics/phg:latest \\\n        /ExportPath.sh config.txt CONSENSUS testOutput1.vcf PATH_METHOD\n</code></pre> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The -t directive indicates the Docker image of which this container will be an instance.  The last line tells the Docker container to run the ExportPath.sh script which is found in the root directory.  The items following are the parameters to the ExportPath.sh script.</p>"},{"location":"UserInstructions/ImputeWithPHG_exportVCF/#files","title":"Files","text":"<p>Config file</p> <p>An example can be found here: Master config file</p> <p>Haplotype Path files (optional)</p> <p>By default, haplotype paths are added to the database. However, in the previous step you may have chosen to output haplotype paths as files instead of adding them to the database. If you output haplotype files in the previous step, you will have a set of files with one haplotype ID per line. Each file corresponds to the predicted path for one taxon.</p>"},{"location":"UserInstructions/ImputeWithPHG_exportVCF/#plugins","title":"Plugins","text":""},{"location":"UserInstructions/ImputeWithPHG_exportVCF/#pathstovcfplugin","title":"PathsToVCFPlugin","text":"<p>The PathsToVCFPlugin is used to create VCF files from haplotype paths.  The user may specific specific reference ranges to be included in the exported file.  This plugin can be chained with the HaplotypeGraphBuilderPlugin and the ImportHaplotypePathFilePlugin. It takes input from both and uses this to create the requested VCF files.</p> <p>The parameters to this plugin are:</p> <ul> <li>-outputFile  Output VCF File Name. (Default=null)(OPTIONAL) <li>-refRangeFileVCF  Reference Range file used to further subset the paths for only specified regions of the genome. (OPTIONAL) <li>-positions  Genotype file (i.e. VCF, Hapmap, etc.), bed file, or json file containing the requested positions. (OPTIONAL) <li>-ref  Reference Genome. (OPTIONAL) <li>-outputAllSNPs  Whether to output all SNPs known by haplotype graph. <p>Here is an example of how to chain the HaplotypeGraphBuilderPlugin to the PathsToVCFPlugin.</p> <pre><code>perl /tassel-5-standalone/run_pipeline.pl $fullXmx -debug \\\n-configParameters ${DATABASE_CONFIG} \\\n-HaplotypeGraphBuilderPlugin \\\n    -configFile $DATABASE_CONFIG \\\n    -methods $CONSENSUS_METHOD \\\n    -includeVariantContexts true \\\n    -endPlugin \\\n-PathsToVCFPlugin \\\n    -outputFile $OUTPUT_FILE \\\n    -endPlugin\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_exportVCF/#importhaplotypepathfileplugin","title":"ImportHaplotypePathFilePlugin","text":"<p>Path information is stored to the PHG database by default, so you are unlikely to need to run this. However, if you chose in the previous step to output a file with haplotype paths and not store paths to the database, you will need to run the ImportHaplotypePathFilePlugin before the PathsToVCFPlugin. </p> <p>This plugin expects a PHG Haplotype Graph to be sent as an incoming parameter, which can be aquired by chaining the HaplotypeGraphBuilderPlugin to the ImportHaplotypePathFilePluign. The output from the ImportHaplotypePathFilePlugin is the input to the PathsToVCFPlugin.</p> <p>The ImportHaplotypePathFilePlugin takes the following parameters;</p> <ul> <li>-inputFileDirectory  A directory containing the paths text files to be imported and used to create the VCF files. (Default=null) (REQUIRED)</li> </ul> <p>Here is an example of how to chain all three of these plugins to read in haplotype path files and export a VCF.</p> <pre><code>perl /tassel-5-standalone/run_pipeline.pl $fullXmx -debug \\\n-configParameters ${DATABASE_CONFIG} \\\n-HaplotypeGraphBuilderPlugin \\\n    -configFile $DATABASE_CONFIG \\\n    -methods $CONSENSUS_METHOD \\\n    -includeVariantContexts true \\\n    -endPlugin \\\n-ImportHaplotypePathFilePlugin \\\n    -pathMethodName ${PATH_METHOD_NAME} \\\n    -endPlugin \\\n-PathsToVCFPlugin \\\n    -outputFile $OUTPUT_FILE \\\n    -endPlugin\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_exportVCF/#troubleshooting","title":"Troubleshooting","text":"<p>Return to Step 3 pipeline</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/ImputeWithPHG_extractPangenome/","title":"Extract a pangenome fasta file","text":""},{"location":"UserInstructions/ImputeWithPHG_extractPangenome/#quick-start","title":"Quick Start","text":"<ol> <li>Change <code>param1</code> to match file paths on your computer.</li> <li>Run <code>phg extractPangenome [config.txt]</code></li> </ol>"},{"location":"UserInstructions/ImputeWithPHG_extractPangenome/#details","title":"Details","text":"<p>This step extracts haplotypes from the PHG database and writes these haplotypes to a pangenome fasta file which is then indexed. Both the fasta file and the .mmi minimap index file are output. The fasta file will be formatted so that the header of each sequence is named with the haplotype ID from the database. </p> <p>This step only needs to be executed once</p>"},{"location":"UserInstructions/ImputeWithPHG_extractPangenome/#kitchen-sink","title":"Kitchen Sink","text":"<p>This shell script will extract out the haplotypes from the PHG database and will then run minimap2 to index the pangenome. This is needed for subsequent steps in the findPaths pipeline. The outputs of this step are a fasta file containing the haplotypes for the graph and a .mmi index file created by minimap indexing said fasta file.</p> <p>he fasta file will be formatted so that the header of each sequence is named with the haplotype ID from the database. Every haplotype from every reference range will be included in this pangenome fasta file, so the total number of sequences will be equal to the number of haplotypes * the number of reference ranges in your database.</p>"},{"location":"UserInstructions/ImputeWithPHG_extractPangenome/#pipeline-steps","title":"Pipeline Steps","text":"<ol> <li>The Script will first check to see if the fasta file already is located in the path specified.<ol> <li>If not, Pull out the fasta using WriteFastaFromGraphPlugin</li> </ol> </li> <li>The Script will then check to see if the fasta file has already been indexed with the parameters specified.<ol> <li>If not, Index using minimap2.</li> </ol> </li> </ol> <p>There are 5 parameters that can be set in this step.</p> <ul> <li>BASE_HAPLOTYPE_NAME: This is the base of the name of the haplotype fasta file.</li> <li>CONFIG_FILE_NAME: This is the path to the config.txt file used to create the DB. All the needed DB connection information should be in here.</li> <li>HAPLOTYPE_METHOD: This is the HAPLOTYPE_METHOD_NAME used when creating the PHG. This is used to pull out the correct haplotypes from the database. This name can also be a list of methods to be pulled at the same time.</li> <li>NUM_BASES_LOADED: This is a Minimap2 parameter for how many base pairs should be loaded into the database for each batch. Typically you should set this to be larger than the number of Base Pairs in your Pangenome. If this is less, Minimap2 is inconsistent in its mapping. We have used 90G for the assembly DB in maize.</li> <li>MINIMIZER_SIZE: This is the kmer size used by Minimap2 to do alignments. We suggest you use a k of 21.</li> <li>WINDOW_SIZE: This is the number of consecutive Minimizer windows used by Minimap.  We suggest using 11.</li> </ul>"},{"location":"UserInstructions/ImputeWithPHG_extractPangenome/#details-on-running-this-step-with-wrapper-scripts","title":"Details on running this step with wrapper scripts","text":"<p>This step can be run directly from the command line or from a wrapper script. When running this step with a wrapper script, all file paths and parameters are set in the config file and the only call that needs to be run in the terminal is <code>phg extractPangenome /path/to/config.txt</code>. </p> <p>If you would like to overwrite the parameters set in the config file, you can do that by setting the parameters on the command line directly.</p> <p>For example, to ignore the config file base haplotype name and set one directly, you could run:</p> <pre><code>phg extractPangenome -CONFIG_FILE_NAME /path/to/config.txt -BASE_HAPLOTYPE_NAME Pangenome1\n</code></pre> <p>You can also run this script directly from the command line using the IndexPangenome.sh bash script. An example command is below:</p> <pre><code>#!bash\n\nIndexPangenome.sh [BASE_HAPLOTYPE_NAME] [CONFIG_FILE_NAME] [HAPLOTYPE_METHOD] [NUM_BASES_LOADED] [MINIMIZER_SIZE] [WINDOW_SIZE]\n\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_extractPangenome/#details-on-running-this-step-through-docker","title":"Details on running this step through docker","text":"<p>Mount points for use with the PHG Docker:</p> <ul> <li>Mount ${OUTPUT_DIR} (localMachine:/pathToOutputDir/) to docker:/tempFileDir/outputDir/. If using sqlite, the database mentioned in the config file should be in this directory.</li> <li>Mount ${DB} (localMachine:/pathToDBDir/DB) to docker:/tempFileDir/outputDir/phgTestMaizeDB.db. This should be your PHG database file. </li> <li>Mount ${CONFIG_FILE} (localMachine:/pathToDataDir/) to docker:/tempFileDir/data/configSQLite.txt. This contains the config file.</li> </ul> <pre><code>#!bash\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nOUTPUT_DIR=/workdir/user/DockerTuningTests/DockerOutput/PangenomeFasta/\nCONFIG_FILE=/workdir/user/DockerTuningTests/configSQLite.txt\ndocker run --name index_pangenome_container --rm \\\n                    -w / \\\n                    -v ${OUTPUT_DIR}:/tempFileDir/outputDir/pangenome/ \\\n                    -v ${DB}:/tempFileDir/outputDir/phgTestMaizeDB.db \\\n                    -v ${CONFIG_FILE}:/tempFileDir/data/configSQLite.txt \\\n                    -t maizegenetics/phg \\\n                    /IndexPangenome.sh pangenome_fasta configSQLite.txt CONSENSUS 4G 15 10\n</code></pre> <p>This command will extract the pan genome to a fasta named pangenome_fasta.fa and will also create an index file named pangenome_fasta.mmi for use in subesquent pathfinding steps. It will be saved to the directory specified by OUTPUT_DIR.</p> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.  NOTE:  If you are using postgreSQL instead of SQLite db, you will not need to include the -V for the database file.</p> <p>The -t directive indicates the Docker image of which this container will be an instance.  The last line tells the Docker container to run the IndexPangenome.sh script which is found in the root directory.  The items following are the parameters to the IndexPangenome.sh script.</p> <p>Also note the -w / parameter. This is needed to guarantee that the script will run correctly. When running a normal docker, this is likely not needed, but if running on a system like cbsu, the working directory needs to be set to the root directory.</p>"},{"location":"UserInstructions/ImputeWithPHG_extractPangenome/#files","title":"Files","text":"<p>Config file</p> <p>An example can be found here: Master config file</p>"},{"location":"UserInstructions/ImputeWithPHG_extractPangenome/#steps-and-plugins","title":"Steps and Plugins","text":""},{"location":"UserInstructions/ImputeWithPHG_extractPangenome/#writefastafromgraphplugin","title":"WriteFastaFromGraphPlugin","text":"<p>The WriteFastaFromGraphPlugin method pulls sequence information from the PHG database and writes the sequence to a fasta file, with each entry in the fasta file corresponding to one haplotype from one reference range. The fasta entries are named with the haplotype_id from the database. The plugin will also BWA index the fasta file if possible.</p> <p>The parameters to this plugin are:</p> <ul> <li>-outputFile  Output filename prefix for pangenome fasta. (REQUIRED) <p>The plugin requires a haplotype graph, which can be acquired by chaining the HaplotypeGraphBuilderPlugin with the WriteFastaFromGraphPlugin. Here is an example command to call this pipeline with TASSEL:</p> <pre><code>perl /tassel-5-standalone/run_pipeline.pl $fullXmx -debug \\\n-HaplotypeGraphBuilderPlugin \\\n    -configFile $CONFIGFILE \\\n    -methods $HAPLOTYPE_METHOD \\\n    -includeVariantContexts false \\\n    -endPlugin \\\n-WriteFastaFromGraphPlugin \\\n    -outputFile $HAPLOTYPE_FASTA \\\n    -endPlugin\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_extractPangenome/#index-fasta-with-minimap2","title":"Index fasta with Minimap2","text":"<p>If the haplotype fasta index file does not exist, you can create one with minimap2. This is needed for the next step in the pipeline.</p> <pre><code>time /minimap2/minimap2 -d ${HAPLOTYPE_INDEX} -k ${MINIMIZER_SIZE} -I ${NUM_BASES_LOADED} -w ${WINDOW_SIZE} ${HAPLOTYPE_FASTA}\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_extractPangenome/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>If you seem to be getting poor-quality mappings, try increasing the value for the NUM_BASES_LOADED parameter. This is a Minimap2 parameter for how many base pairs should be loaded into the database for each batch. Typically you should set this to be larger than the number of base pairs in your pangenome. If this is less than the number of base pairs in your pangenome, Minimap2 is inconsistent in its mapping. We have used 90G for the assembly DB in maize, but it may need to be set larger for larger genomes.</li> </ol> <p>Return to Step 3 pipeline</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/ImputeWithPHG_fastq-heterozygous/","title":"Impute variants from any fastq file","text":""},{"location":"UserInstructions/ImputeWithPHG_fastq-heterozygous/#quick-start","title":"Quick Start","text":"<ol> <li>Change <code>param1</code>, <code>param2</code>, <code>param3</code>, and <code>param4</code> to match file paths on your computer.</li> <li>Run <code>phg findpaths_diploid [config.txt]</code></li> </ol> <p>By Default this process will run FindPathMinimap2.sh and the DiploidPath plugin</p>"},{"location":"UserInstructions/ImputeWithPHG_fastq-heterozygous/#details","title":"Details","text":"<p>This process will align all fastqs in the directory indicated <code>param1</code> to the indexed pangenome created in IndexPangenome.sh. Mappings to haplotypes are then reported, picking the 1-2 best haplotypes hit at every reference range. An HMM is then run to find the most probable path through the haplotype graph. The default is to report haplotypes at every reference range with parameter <code>minreads=0</code>, however to restrict it to sampled ranges, you can increase this value.  The process is performed twice in parallel to produce 2 paths through the graph, representing the diploid genome.</p> <p>This shell script will use the Minimap2 index created by IndexPangenome.sh and will align a set of reads to the graph and then will use a HMM to find the most likely path through the graph given the alignments. The database \"paths\" and \"read_mapping_paths\" tables are populated from the plugins called by this script.</p>"},{"location":"UserInstructions/ImputeWithPHG_fastq-heterozygous/#kitchen-sink","title":"Kitchen Sink","text":"<ol> <li> <p>Run FastqToMappingPlugin to map a set of reads to the pangenome fasta file.  This plugin will make use of a keyfile and will store the mappings in the DB.</p> </li> <li> <p>Run HapCountBestPathToTextPlugin to take the mappings from the DB and use a HMM to find the optimal path through the DB.  This will then store the paths into the DB.</p> </li> </ol> <p>There are 8 parameters used in this step:</p> <ul> <li>BASE_HAPLOTYPE_NAME: This is the base of the name of the haplotype fasta file. This file should have been indexed in the previous step.</li> <li>CONFIG_FILE_NAME: This is the path to the config.txt file used to create the DB. All the needed DB connection information should be in here.</li> <li>HAPLOTYPE_METHOD: Method name of the haplotypes in the graph that were used to generate the haplotype fasta file in IndexPangenome.sh. This needs to match exactly otherwise the results will not be correct.</li> <li>HAPLOTYPE_METHOD_FIND_PATH: This method can be the same as HAPLOTYPE_METHOD, but it is typically used to only include anchor reference ranges when running FindPath. Typically, finding paths through the interanchor reference ranges can cause additional errors. An example of what could be used here is this: HAPLOTYPE_METHOD,refRegionGroup to only include the refRegionGroup refRange group in the Graph used for finding the path.</li> <li>HAPCOUNT_METHOD_NAME: Name of the Haplotype Mapping method used to upload the ReadMapping files to the DB. This is currently not used so a dummy value can be used. It will be implemented in the future.</li> <li>PATH_METHOD_NAME: Name of the Path Mapping method used to upload the Paths to the DB. This method name will be used in the next step (<code>phg exportPath</code>) to extract out the paths from the DB.</li> <li>READ_KEY_FILE: This name needs to match the name of the keyfile. This keyfile will describe what fastq files need to be aligned together and also denotes the required metadata fields which are stored in the DB.</li> <li>PATH_KEY_FILE: This name is what the path finding keyfile will be named.  FastqToMappingPlugin will create this file and then BestHaplotypePathPlugin will use it to find paths. Note that FastqToMappingPlugin will group reads by taxon and all the mappings for a given taxon will be used when finding the paths.</li> </ul>"},{"location":"UserInstructions/ImputeWithPHG_fastq-heterozygous/#details-on-running-this-step-with-wrapper-scripts","title":"Details on running this step with wrapper scripts","text":"<p>When running this step on the command line, all file paths and parameters are set in the config file. The only call that needs to be run in the terminal is <code>phg findPath /path/to/config.txt</code>. If you would like to overwrite the parameters set in the config file, you can do that by setting the parameters on the command line directly.</p> <p>For example, to ignore the config file HAPCOUNT_METHOD_NAME level and set one directly, you could run:</p> <pre><code>phg findPaths -configFile /path/to/config.txt -HAPCOUNT_METHOD_NAME MyNewMethod1\n</code></pre> <p>You can also run the <code>FindPathMinimap2.sh</code> bash script directly with the following run command:</p> <pre><code>#!bash\n\nFindPathMinimap2.sh [BASE_HAPLOTYPE_NAME] [CONFIG_FILE_NAME] [HAPLOTYPE_METHOD] [HAPLOTYPE_METHOD_FIND_PATH] [HAPCOUNT_METHOD_NAME] [PATH_METHOD_NAME] [READ_KEY_FILE] [PATH_KEY_FILE]\n\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_fastq-heterozygous/#details-on-running-this-step-through-docker","title":"Details on running this step through docker","text":"<p>When FindPathMinimap2.sh is run as part of a Docker container script, the Docker script expects the following directory mount points:</p> <ul> <li>Mount localMachine:/pathToInputs/FastQFiles/ to docker:/tempFileDir/data/fastq</li> <li>Mount localMachine:/pathToOutputs/ to docker:/tempFileDir/outputDir/</li> <li>Mount localMachine:/pathToPangenomeIndex/ to docker:/tempFileDir/outputDir/pangenome/</li> <li>Mount localMachine:/pathToInputs/config.txt to docker:/tempFileDir/data/config.txt</li> <li>Mount localMachine:/pathToInputs/phg.db to docker:/tempFileDir/outputDir/phgTestMaizeDB.db.  This needs to match what is in the config file.</li> </ul> <p>It is expected the database is stored in the User specified outputDir that is mounted below and the config.txt specifies the database name and login parameters.</p> <p>It is critical that the .mmi file is mounted to /tempFileDir/outputDir/pangenome/ in the docker.  Otherwise this will not work correctly. </p> <p>If you see this error: ERROR net.maizegenetics.plugindef.AbstractPlugin - Haplotype count methodid not found in db for method : HAP_COUNT_METHOD, it means that something went wrong during the ReadMapping step.  Double check the -v parameters and make sure the mmi file is in /tempFileDir/outputDir/pangenome/</p> <p>An example Docker script to run the FindPathMinimap2.sh shell script is:</p> <pre><code>#!bash\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nPANGENOME_DIR=/workdir/user/DockerTuningTests/DockerOutput/PangenomeFasta/  \n\ndocker run --name small_seq_test_container --rm \\ \n                    -w / \\\n                    -v /workdir/user/DockerTuningTests/DockerOutput/:/tempFileDir/outputDir/ \\\n                    -v /workdir/user/DockerTuningTests/InputFiles/GBSFastq/:/tempFileDir/data/fastq/ \\\n                    -v /workdir/user/DockerTuningTests/InputFiles/config.txt:/tempFileDir/data/configSQLite.txt \\\n                    -v ${DB}:/tempFileDir/outputDir/phgTestMaizeDB.db \\\n                    -v ${PANGENOME_DIR}:/tempFileDir/outputDir/pangenome/ \\\n                    -t maizegenetics/phg:latest \\\n                    /FindPathMinimap2.sh phgSmallSeqSequence configSQLite.txt \\\n                    CONSENSUS CONSENSUS,refRegionGroup \\\n                    HAP_COUNT_METHOD PATH_METHOD \\\n                    /tempFileDir/data/fastq/genotypingKeyFile.txt \\\n                    /tempFileDir/data/fastq/genotypingKeyFile_pathKeyFile.txt\n\n</code></pre> <p>PANGENOME_DIR must match the directory set in the IndexPangenome step.</p> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The \"-t\" directive indicates the Docker image of which this container will be an instance.  The last line tells the Docker container to run the FindPath.sh script which is found in the root directory.  The items following are the parameters to the FindPath.sh script.</p>"},{"location":"UserInstructions/ImputeWithPHG_fastq-heterozygous/#files","title":"Files","text":"<p>Config file</p> <p>An example can be found here: Master config file</p> <p>KeyFiles </p> <p>The FindPathMinimap2 Pipeline will require the use of 2 keyfiles.  The second is autogenerated, but could be changed if different results are desired.  Both files need to be tab-separated.  If there are entries in the keyfile, but not found on the filesystem, the pipelines will skip over those entries.</p> <p>More information about the keyfiles can be found here:</p>"},{"location":"UserInstructions/ImputeWithPHG_fastq-heterozygous/#plugins","title":"Plugins","text":""},{"location":"UserInstructions/ImputeWithPHG_fastq-heterozygous/#plugin-1","title":"Plugin 1","text":""},{"location":"UserInstructions/ImputeWithPHG_fastq-heterozygous/#plugin-2","title":"Plugin 2","text":""},{"location":"UserInstructions/ImputeWithPHG_fastq-heterozygous/#troubleshooting","title":"Troubleshooting","text":"<p>Return to Step 3 pipeline</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/ImputeWithPHG_fastq-homozygous/","title":"Impute variants from any fastq file - process for homozygous individuals","text":""},{"location":"UserInstructions/ImputeWithPHG_fastq-homozygous/#quick-start","title":"Quick Start","text":"<ol> <li>Change <code>keyfile1</code>, <code>param2</code>, <code>param3</code>, and <code>param4</code> in your config file to match file paths on your computer.</li> <li>Run <code>phg findPaths [config.txt]</code></li> </ol>"},{"location":"UserInstructions/ImputeWithPHG_fastq-homozygous/#details","title":"Details","text":"<p>This step uses the Minimap2 index and pangenome fasta files created in the previous step of the pipeline. It aligns a set of reads from an input fastq file to the graph and then uses an HMM to find the most likely path through the graph given the alignments. The database \"paths\" and \"read_mapping_paths\" tables are populated from the plugins called by this script.</p>"},{"location":"UserInstructions/ImputeWithPHG_fastq-homozygous/#kitchen-sink","title":"Kitchen Sink","text":"<p>This step uses the pangenome fasta to find paths for new individuals. It requires a set of fastq reads. The type of sequencing data does not matter, but you should be working with fairly homozygous material. If you would like to run the findPaths step with heterozygous material, use the heterozygous findPaths step.</p> <p>This pipeline has two main steps:</p> <ol> <li> <p>Run FastqToMappingPlugin to map a set of reads to the pangenome fasta file.  This plugin will make use of a keyfile and will store the mappings in the DB.</p> </li> <li> <p>Run BestHaplotypePathPlugin to take the mappings from the DB and use a HMM to find the optimal path through the DB.  This will then store the paths into the DB.</p> </li> </ol> <p>The findPaths pipeline requires the use of 2 keyfiles. The second is autogenerated, but could be changed if different results are desired.  Both files need to be tab-separated. If there are entries in the keyfile, but not found on the filesystem, the pipelines will skip over those entries. More information about these keyfiles can be found here. </p> <p>There are 8 parameters used in this step:</p> <ul> <li>BASE_HAPLOTYPE_NAME: This is the base of the name of the haplotype fasta file. This file should have been indexed in the previous step.</li> <li>CONFIG_FILE_NAME: This is the path to the config.txt file used to create the DB. All the needed DB connection information should be in here.</li> <li>HAPLOTYPE_METHOD: Method name of the haplotypes in the graph that were used to generate the haplotype fasta file in IndexPangenome.sh. This needs to match exactly otherwise the results will not be correct.</li> <li>HAPLOTYPE_METHOD_FIND_PATH: This method can be the same as HAPLOTYPE_METHOD, but it is typically used to only include anchor reference ranges when running FindPath. Typically, finding paths through the interanchor reference ranges can cause additional errors. An example of what could be used here is this: HAPLOTYPE_METHOD,refRegionGroup to only include the refRegionGroup refRange group in the Graph used for finding the path.</li> <li>HAPCOUNT_METHOD_NAME: Name of the Haplotype Mapping method used to upload the ReadMapping files to the DB. This is currently not used so a dummy value can be used. It will be implemented in the future.</li> <li>PATH_METHOD_NAME: Name of the Path Mapping method used to upload the Paths to the DB. This method name will be used in the next step (<code>phg exportPath</code>) to extract out the paths from the DB.</li> <li>READ_KEY_FILE: This name needs to match the name of the keyfile. This keyfile will describe what fastq files need to be aligned together and also denotes the required metadata fields which are stored in the DB.</li> <li>PATH_KEY_FILE: This name is what the path finding keyfile will be named.  FastqToMappingPlugin will create this file and then BestHaplotypePathPlugin will use it to find paths. Note that FastqToMappingPlugin will group reads by taxon and all the mappings for a given taxon will be used when finding the paths.</li> </ul>"},{"location":"UserInstructions/ImputeWithPHG_fastq-homozygous/#details-on-running-this-step-with-wrapper-scripts","title":"Details on running this step with wrapper scripts","text":"<p>When running this step on the command line, all file paths and parameters are set in the config file. The only call that needs to be run in the terminal is <code>phg findPath /path/to/config.txt</code>. If you would like to overwrite the parameters set in the config file, you can do that by setting the parameters on the command line directly.</p> <p>For example, to ignore the config file HAPCOUNT_METHOD_NAME level and set one directly, you could run:</p> <pre><code>phg findPaths -configFile /path/to/config.txt -HAPCOUNT_METHOD_NAME MyNewMethod1\n</code></pre> <p>You can also run the <code>FindPathMinimap2.sh</code> bash script directly with the following run command:</p> <pre><code>#!bash\n\nFindPathMinimap2.sh [BASE_HAPLOTYPE_NAME] [CONFIG_FILE_NAME] [HAPLOTYPE_METHOD] [HAPLOTYPE_METHOD_FIND_PATH] [HAPCOUNT_METHOD_NAME] [PATH_METHOD_NAME] [READ_KEY_FILE] [PATH_KEY_FILE]\n\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_fastq-homozygous/#details-on-running-this-step-through-docker","title":"Details on running this step through docker","text":"<p>When this step is run as part of a Docker container script, the Docker expect the following directory mount points: </p> <ul> <li>Mount localMachine:/pathToInputs/FastQFiles/ to docker:/tempFileDir/data/fastq</li> <li>Mount localMachine:/pathToOutputs/ to docker:/tempFileDir/outputDir/</li> <li>Mount localMachine:/pathToPangenomeIndex/ to docker:/tempFileDir/outputDir/pangenome/</li> <li>Mount localMachine:/pathToInputs/config.txt to docker:/tempFileDir/data/config.txt</li> <li>Mount localMachine:/pathToInputs/phg.db to docker:/tempFileDir/outputDir/phgTestMaizeDB.db.  This needs to match what is in the config file.</li> </ul> <p>It is expected the database is stored in the User specified outputDir that is mounted below and the config.txt specifies the database name and login parameters.</p> <p>It is critical that the .mmi file is mounted to /tempFileDir/outputDir/pangenome/ in the docker.  Otherwise this will not work correctly. </p> <p>An example Docker script to run the FindPathMinimap2.sh shell script is:</p> <pre><code>#!bash\nDB=/workdir/user/DockerTuningTests/DockerOutput/phgTestMaizeDB.db\nPANGENOME_DIR=/workdir/user/DockerTuningTests/DockerOutput/PangenomeFasta/  \n\ndocker run --name small_seq_test_container --rm \\ \n    -w / \\\n    -v /workdir/user/DockerTuningTests/DockerOutput/:/tempFileDir/outputDir/ \\\n    -v /workdir/user/DockerTuningTests/InputFiles/GBSFastq/:/tempFileDir/data/fastq/ \\\n    -v /workdir/user/DockerTuningTests/InputFiles/config.txt:/tempFileDir/data/configSQLite.txt \\\n    -v ${DB}:/tempFileDir/outputDir/phgTestMaizeDB.db \\\n    -v ${PANGENOME_DIR}:/tempFileDir/outputDir/pangenome/ \\\n    -t maizegenetics/phg:latest \\\n    /FindPathMinimap2.sh phgSmallSeqSequence configSQLite.txt \\\n        CONSENSUS CONSENSUS,refRegionGroup \\\n        HAP_COUNT_METHOD PATH_METHOD \\\n        /tempFileDir/data/fastq/genotypingKeyFile.txt \\\n        /tempFileDir/data/fastq/genotypingKeyFile_pathKeyFile.txt\n\n</code></pre> <p>PANGENOME_DIR must match the directory set in the IndexPangenome step.</p> <p>The --name parameter provides a name for the container.  This is optional.</p> <p>The --rm parameter indicates the container should be deleted when the program finishes executing.  This is optional.</p> <p>Note the -w / parameter. This is needed to guarantee that the script will run correctly. When running a normal docker, this is likely not needed, but if running on a system like cbsu, the working directory needs to be set to the root directory.</p> <p>The -v directives are used to mount data from the user machine into the Docker.  The path preceding the \":\" is the path on the user machine.  The directory path following the \":\" are the paths inside the Docker where the user home directories will be mounted.</p> <p>The -t directive indicates the Docker image of which this container will be an instance.  The last line tells the Docker container to run the FindPath.sh script which is found in the root directory. The items following are the parameters to the FindPath.sh script.</p>"},{"location":"UserInstructions/ImputeWithPHG_fastq-homozygous/#files","title":"Files","text":"<p>Config file</p> <p>An example can be found here: Master config file</p> <p>Pangenome fasta file (with index)</p> <p>This is the pangenome fasta file created in the previous step. The name of that file should be the same as the value you set for the BASE_HAPLOTYPE_NAME parameter.</p> <p>Read key file</p> <p>Tab-delimited text file that sets up fastq alignment to the pangenome fasta. Has a header with \"cultivar\\tflowcell_lane\\tfilename\\tPlateID\" and corresponding information for each fastq file used in the findPaths step. Only the first three columns are required. More information and an example are here: findPaths keyfiles</p> <p>Path key file</p> <p>This file is created automatically, so you will not need to create one on your own unless you would like to change some of the values in it. This file has three columns: \"sampleName\\tReadMappingIds\\tLikelyParents\". Only the first two columns are required. More information and an example are here: findPaths keyfiles</p>"},{"location":"UserInstructions/ImputeWithPHG_fastq-homozygous/#plugins","title":"Plugins","text":""},{"location":"UserInstructions/ImputeWithPHG_fastq-homozygous/#fastqtomappingplugin","title":"FastqToMappingPlugin","text":"<p>The FastqToMappingPlugin executes code to count the number of reads which align to a haplotype node using Minimap 2. </p> <p>If running in paired mode, it will align the read pairs using minimap and process the resulting SAM records. Only equally optimal mappings (by Edit Distance (NM) are kept. Additional filtering is done to remove reads which are unmapped or clipped. </p> <p>If running in paired mode, the reads must be on opposite strands and both must currently hit the same haplotype in a single reference range. Optimal mappins across reference ranges are processed, but reads are only assigned to haplotypes in the reference range with the most hits (providing that at least 1-maxRefRangeError percentage hit that reference range). Based on testing, this seems to work the best to balance the number of reads used with accuracy. The plugin requires a PHG Haplotype Graph, which can be acquired by chaining the running of HaplotypeGraphBuilderPlugin and this plugin. </p> <p>The output from the HaplotypeGraphBuilderPlugin will be input to the BestHaplotypePathPlugin.</p> <p>The parameters for this plugin are:</p> <ul> <li>-configFile  DB Config File containing properties host,user,password,DB and DBtype where DBtype is either sqlite or postgres (Default=null) (REQUIRED) A sample config file can be found here:Master Config File. <li>-minimap2IndexFile  : Name of the indexFile file to process (required) <li>-keyFile  : Name of the Keyfile to process. Must have columns cultivar, flowcell_lane, filename, and PlateID. Optionally for paired end reads, filename2 is needed. If filename2 is not supplied, Minimap2 will run in single end mode. Otherwise will be paired. (required) <li>-fastqDir  : Name of the Fastq dir to process. (required) <li>-maxRefRangeErr  : Maximum allowed error when choosing best reference range to count.  Error is computed 1 - (mostHitRefCount/totalHits) (Default: 0.25) <li>-lowMemMode  : Run in low memory mode. (Default: true) <li>-maxSecondary  : Maximum number of secondary alignments to be returned by minimap2. This will be the value of the -N parameter in the minimap2 command line. If the value is too low, some valid read mappings will not be reported. (Default: 20) <li>-minimapLocation  : Location of Minimap2 on file system.  This defaults to use minimap2 if it is on the PATH environment variable. (Default: minimap2) <li>-methodName  : Method name to be stored in the DB. (required) <li>-methodDescription  : Method description to be stored in the DB. (required) <li>-debugDir  : Directory to write out the read mapping files.  This is optional for debug purposes. (Default: ) <li>-outputSecondaryStats  : Ouptput Secondary Mapping Statistics such as total AS for each haplotype ID (Default: false) <p>The FastqToMappingPlugin requires a haplotype graph, which can be obtained with the HaplotypeGraphBuilderPlugin. An example for chaining these plugins is below:</p> <pre><code>perl /tassel-5-standalone/run_pipeline.pl -debug ${fullXmx} \\\n-configParameters ${CONFIGFILE} \\\n-HaplotypeGraphBuilderPlugin \\\n    -configFile ${CONFIGFILE} \\\n    -methods ${HAPLOTYPE_METHOD} \\\n    -includeVariantContexts false \\\n    -includeSequences false \\\n    -endPlugin \\\n-FastqToMappingPlugin \\\n    -minimap2IndexFile ${HAPLOTYPE_INDEX} \\\n    -keyFile ${KEY_FILE} \\\n    -fastqDir ${FASTQ_DIR}/ \\\n    -methodName ${HAP_COUNT_METHOD} \\\n    -methodDescription READ_MAPPING_DESCRIPTION \\\n    -debugDir $OUTPUT_DIR \\\n    -endPlugin\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_fastq-homozygous/#besthaplotypepathplugin","title":"BestHaplotypePathPlugin","text":"<p>This plugin takes a haplotype graph and a set of read mappings to infer the best (most likely) path through the graph given the read mappings. Read mappings are a list of reads with a set of haplotypes to which that read aligned. </p> <p>The plugin can (1) take a file of read mappings and return a file with a list of haplotypes or (2) take read mappings from a PHG DB and store the resulting list of haplotypes in the DB.</p> <p>If (1), the input is a file, then the plugin can take either a file or a directory containing multiple files. If a directory, all read mapping files will be processed and the haplotype lists output as separate files to an output directory. If the output directory is not specified, then the lists will be written to the input directory. Any path files of the same name will not be overwritten; a message will be written to the log to that effect, unless the \"overwrite\" flag is set to true.</p> <p>If (2), the input comes from a PHG DB, an input read map method and the output path method must be supplied. In addition, a specific taxon or list of taxa for which paths are to be imputed can be suppled. If paths for any of the taxa and methods exist, the paths will not be imputed and a warning message will be written to the log file. If the \"overwrite\" flag is set to true, any existing paths will be overwritten and a message to that effect will be written to the log. </p> <p>The parameters for this plugin are:</p> <ul> <li>-keyFile  : KeyFile file name.  Must be a tab separated file using the following headers: SampleName    ReadMappingIds  LikelyParents<ul> <li>ReadMappingIds and LikelyParents need to be comma separated for multiple values (required)</li> </ul> <li>-readFile  : Filename of read mappings. Do not supply both a filename and a directory. <li>-readDir  : Directory of read mapping files. If this is supplied, do not also assign a read filename. <li>-outDir  : Directory to which path files will be written. <li>-readMethod  : The name of the read mapping method in the PHG DB (required) <li>-pathMethod  : The name of the path method used to write the results to the PHG DB (required) <li>-overwrite  : If an output pathfile already exists for a taxon, then it will be overwritten if overwrite = true. Otherwise, it will not and a warning will be written to the log. Likewise for paths in the PHG DB. (Default: false) <li>-minTaxa  : minimum number of taxa per anchor reference range. Ranges with fewer taxa will not be included in the output node list. (Default: 20) <li>-minReads  : minimum number of reads per anchor reference range. Ranges with fewer reads will not be included in the output node list. (Default: 1) <li>-maxReads  : maximum number of include counts per anchor reference range Kb. Ranges with more reads will not be included in the output node list. (Default: 10000) <li>-maxNodes  : maximum number of nodes per reference range. Ranges with more nodes will not be included in the output node list. (Default: 1000) <li>-minTransitionProb  : minimum probability of a transition between nodes at adjacent reference ranges. (Default: 0.001) <li>-probCorrect  : minimum number of reads per anchor reference range. Ranges with fewer reads will not be included in the output node list. (Default: 0.99) <li>-splitNodes  : split consensus nodes into one node per taxon. (Default: true) <li>-splitProb  : When the consensus nodes are split by taxa, this is the transition probability for moving from a node to the next node of the same taxon. It equals 1 minus the probability that the path will switch between taxa. (Default: 0.99) <li>-usebf  : Use the Backward-Forward algorithm instead of the Viterbi algorithm for the HMM. (Default: false) <li>-minP  : Only nodes with minP or greater probability will be kept in the path when using the Backward-Forward algorithm, (Default: 0.8) <li>-bfInfoFile  : The base name of the file to node probabilities from the backward-forward algorithm will be written. taxonName.txt will be appended to each file. <li>-removeEqual  : Ranges with equal read counts for all haplotypes should be removed from the graph. Defaults to true but will be always be false if minReads = 0. (Default: true) <li>-numThreads  : Number of threads used to find paths. The path finding will subtract 2 from this number to have the number of worker threads.  It leaves 1 thread for IO to the DB and 1 thread for the Operating System. (Default: 3) <li>-requiredTaxa  : Optional list of taxa required to have haplotypes. Any reference range that does not have a haplotype for one of these taxa will not be used for path finding. This can be a comma separated list of taxa (no spaces unless surrounded by quotes), file (.txt) with list of taxa names to include, or a taxa list file (.json or .json.gz). By default, all taxa will be included. <li>-algorithmType  : the type of algorithm. Choices are classic, which is the original implementation described by Rabiner 1989, or efficient, which is modified for improved computational efficiency. [classic, efficient] (Default: efficient) <p>The BestHaplotypePathPlugin requires a haplotype graph, which can be obtained with the HaplotypeGraphBuilderPlugin. An example for chaining these plugins is below:</p> <pre><code>/tassel-5-standalone/run_pipeline.pl -debug ${fullXmx} \\\n-configParameters ${CONFIGFILE} \\\n-HaplotypeGraphBuilderPlugin \\\n    -configFile ${CONFIGFILE} \\\n    -methods ${HAPLOTYPE_METHOD_FIND_PATH} \\\n    -includeVariantContexts false \\\n    -includeSequences false \\\n    -endPlugin \\\n-BestHaplotypePathPlugin \\\n    -keyFile ${PATH_KEY_FILE} \\\n    -outDir ${HAP_COUNT_BEST_PATH_DIR} \\\n    -readMethod ${HAP_COUNT_METHOD} \\\n    -pathMethod ${PATH_METHOD} \\\n    -endPlugin\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_fastq-homozygous/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>If you see this error: ERROR net.maizegenetics.plugindef.AbstractPlugin - Haplotype count methodid not found in db for method : HAP_COUNT_METHOD, it means that something went wrong during the ReadMapping step.  Double check the -v parameters and make sure the mmi file is in /tempFileDir/outputDir/pangenome/</li> </ol> <p>Return to Step 3 pipeline</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/ImputeWithPHG_findPathKeyFiles/","title":"Find Path Key Files","text":"<p>These two keyfiles are tab-separated text files which setup fastq alignment to the pangenome and which readMappings need to be aggregated when finding Paths.</p>"},{"location":"UserInstructions/ImputeWithPHG_findPathKeyFiles/#fastqtomappingplugin-keyfile-specification","title":"FastqToMappingPlugin KeyFile Specification:","text":"<p>The PHG will process the following columns:</p> HeaderName Description Required cultivar Name of the taxon to be processed. Yes flowcell_lane Name of the flow cell this sample came from Yes filename Name of the Fastq file to be processed Yes filename2 Second Fastq file to be processed.  If set, minimap2 will operate in paired end mode No PlateID The id of the Plate. No <p>Note that cultivar + flowcell_lane + PlateID must be a unique value across all samples in the keyfile.  If this is not, the PHG will not work correctly. After running through FastqToMapping, an additional column will be added denoting the ReadMappingID values in the DB.  This is purely for reference and is not used currently in the pipeline.</p>"},{"location":"UserInstructions/ImputeWithPHG_findPathKeyFiles/#sample-file","title":"Sample File:","text":"<pre><code>#!txt\n\ncultivar    flowcell_lane   filename    PlateID\nRecLineB1RefA1gco4_wgs  wgsFlowcell RecLineB1RefA1gco4_R1.fastq wgs\nRecLineB1RefA1gco4_gbs  gbsFlowcell RecLineB1RefA1gco4_R1_gbs.fastq gbs\nRecRefA1LineBgco6_wgs   wgsFlowcell RecRefA1LineBgco6_R1.fastq  wgs\nRecRefA1LineBgco6_gbs   gbsFlowcell RecRefA1LineBgco6_R1_gbs.fastq  gbs\nLineB1_wgs  wgsFlowcell LineB1_R1.fastq wgs\nLineB1_gbs  gbsFlowcell LineB1_R1_gbs.fastq gbs\nLineA_wgs   wgsFlowcell LineA_R1.fastq  wgs\nLineA_gbs   gbsFlowcell LineA_R1_gbs.fastq  gbs\nLineB_wgs   wgsFlowcell LineB_R1.fastq  wgs\nLineB_gbs   gbsFlowcell LineB_R1_gbs.fastq  gbs\nLineA1_wgs  wgsFlowcell LineA1_R1.fastq wgs\nLineA1_gbs  gbsFlowcell LineA1_R1_gbs.fastq gbs\nRecLineALineB1gco3_wgs  wgsFlowcell RecLineALineB1gco3_R1.fastq wgs\nRecLineALineB1gco3_gbs  gbsFlowcell RecLineALineB1gco3_R1_gbs.fastq gbs\nRecLineB1LineBgco7_wgs  wgsFlowcell RecLineB1LineBgco7_R1.fastq wgs\nRecLineB1LineBgco7_gbs  gbsFlowcell RecLineB1LineBgco7_R1_gbs.fastq gbs\nRefA1_wgs   wgsFlowcell RefA1_R1.fastq  wgs\nRefA1_gbs   gbsFlowcell RefA1_R1_gbs.fastq  gbs\nRef_wgs wgsFlowcell Ref_R1.fastq    wgs\nRef_gbs gbsFlowcell Ref_R1_gbs.fastq    gbs\nRecLineBLineB1gco8_wgs  wgsFlowcell RecLineBLineB1gco8_R1.fastq wgs\nRecLineBLineB1gco8_gbs  gbsFlowcell RecLineBLineB1gco8_R1_gbs.fastq gbs\nRecLineA1LineA1gco2_wgs wgsFlowcell RecLineA1LineA1gco2_R1.fastq    wgs\nRecLineA1LineA1gco2_gbs gbsFlowcell RecLineA1LineA1gco2_R1_gbs.fastq    gbs\nRecLineBLineB1gco5_wgs  wgsFlowcell RecLineBLineB1gco5_R1.fastq wgs\nRecLineBLineB1gco5_gbs  gbsFlowcell RecLineBLineB1gco5_R1_gbs.fastq gbs\nRecLineA1RefA1gco1_wgs  wgsFlowcell RecLineA1RefA1gco1_R1.fastq wgs\nRecLineA1RefA1gco1_gbs  gbsFlowcell RecLineA1RefA1gco1_R1_gbs.fastq gbs\n\n</code></pre>"},{"location":"UserInstructions/ImputeWithPHG_findPathKeyFiles/#find-path-keyfile-specification","title":"Find Path Keyfile Specification","text":"<p>Note: This file will likely not be needed to create.  FastqToMappingPlugin will export a sample keyfile given its inputs.  This will probably be good enough for the majority of use cases.</p> <p>The PHG will process the following columns:</p> HeaderName Description Required sampleName Name of the taxon to be processed. Yes ReadMappingIds ReadMappingIds in the DB. All readMappings need to be comma separated. Yes LikelyParents List of taxon which the user believes are the likely parents for this sample.  Currently this is not used. No"},{"location":"UserInstructions/ImputeWithPHG_findPathKeyFiles/#sample-file_1","title":"Sample File:","text":"<pre><code>#!txt\n\nSampleName  ReadMappingIds  LikelyParents\nRecLineB1RefA1gco4_wgs  1       \nRecLineB1RefA1gco4_gbs  2       \nRecRefA1LineBgco6_wgs   3       \nRecRefA1LineBgco6_gbs   4       \nLineB1_wgs  5       \nLineB1_gbs  6       \nLineA_wgs   7       \nLineA_gbs   8       \nLineB_wgs   9       \nLineB_gbs   10      \nLineA1_wgs  11      \nLineA1_gbs  12      \nRecLineALineB1gco3_wgs  13      \nRecLineALineB1gco3_gbs  14      \nRecLineB1LineBgco7_wgs  15      \nRecLineB1LineBgco7_gbs  16      \nRefA1_wgs   17      \nRefA1_gbs   18      \nRef_wgs 19      \nRef_gbs 20      \nRecLineBLineB1gco8_wgs  21      \nRecLineBLineB1gco8_gbs  22      \nRecLineA1LineA1gco2_wgs 23      \nRecLineA1LineA1gco2_gbs 24      \nRecLineBLineB1gco5_wgs  25      \nRecLineBLineB1gco5_gbs  26      \nRecLineA1RefA1gco1_wgs  27      \nRecLineA1RefA1gco1_gbs  28      \n\n</code></pre> <p>Return to Step 3 pipeline version 0.0.40 or older</p> <p>Return to Step 3 pipeline version 0.1.0 or newer</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/ImputeWithPHG_main/","title":"Step 3: Impute Variants or Haplotypes","text":""},{"location":"UserInstructions/ImputeWithPHG_main/#step-3-impute-variants-with-the-phg","title":"Step 3: Impute variants with the PHG","text":"<p>This step uses stored haplotype graph data to infer genotypes from skim sequence, GBS data, or other variant information. It uses the input fastq or variant files to match new individuals to haplotypes in the database and generates paths through the haplotype graph. Paths are stored in the database paths table once they are found. The path information can be output as either haplotype node IDs from the haplotypes table or exported to a VCF file containing SNPs for the taxa processed.</p>"},{"location":"UserInstructions/ImputeWithPHG_main/#quick-start","title":"Quick Start","text":"<ol> <li>Write a config file and fill in the unassigned values.</li> <li>Create a keyfile with information about reads to be imputed.</li> <li>Run one of the commands in the examples section.</li> </ol>"},{"location":"UserInstructions/ImputeWithPHG_main/#details","title":"Details","text":""},{"location":"UserInstructions/ImputeWithPHG_main/#writing-a-config-file","title":"Writing a config file","text":"<p>Before running the pipeline, you need to create a config file and put it in \"baseDir\" or some other directory accessible by the Docker. The config file name does not have to be config.txt. It also can be in any subdirectory of baseDir as long as the same file name location is used for -configParameters in the full pipeline command.  Two sample config files are provided, one for imputing haplotypes from sequence in fastq or SAM files (link here) and another for imputing haplotypes from variants in a VCF file (link here).  Values of UNASSIGNED in a config file must be filled in before using it. See the section Setting parameters in the config file for more information about what those values should be.</p>"},{"location":"UserInstructions/ImputeWithPHG_main/#to-run-the-pipeline","title":"To run the pipeline","text":"<p>The pipeline is run using a single TASSEL pipeline command. All of the parameters used by the pipeline are set in the config file. The following Docker command can be used to run the full pipeline, replacing baseDir, dockerImageName, and -Xmx10G with the appropriate values: <code>docker run --name pipeline_container --rm -v baseDir:/phg/ -t dockerImageName /tassel-5-standalone/run_pipeline.pl -Xmx10G -debug -configParameters /phg/config.txt -ImputePipelinePlugin -imputeTarget pathToVCF -endPlugin</code> When the pipeline is run it starts at the beginning and runs to the target. Possible values for imputeTarget are config, pangenome, map, path, diploidPath, pathToVCF, and diploidPathToVCF. \"pangenome\" stops after writing a pangenome fasta and using minimap2 to index it. \"map\" stops after mapping the sequence or variants to the pangenome haplotypes. \"path\" stops after writing the imputed paths to the database. In each case, prior steps in the pipeline already completed will not be rerun but will be skipped instead. Thus, read mappings or paths already computed will not be over-written.</p>"},{"location":"UserInstructions/ImputeWithPHG_main/#running-the-pipeline-with-different-parameter-settings","title":"Running the pipeline with different parameter settings","text":"<p>When reprocessing the same samples with different parameter settings, the method names must be changed. If the method names are not changed, then read mappings or paths will already exist for those sample names and will not be overwritten. If read mapping parameters used to create the pangenome change, new readMethod and pathMethod names must be used. If only path finding parameters change, then only change pathMethod. In that case the pipeline will use the existing readMethod data to compute new paths.</p> <p>If an existing configuration file is modified with new parameter settings and method names, the new configuration file should be saved under a different name. The configuration files provide a record of how analyses were run.</p> <p>Parameter values used for read mapping are stored in the database with readMethod the first time that method is used. If any of those parameter values are changed, then the readMethod name should be changed as well so that the database has an accurate record of parameter values associated with each method name. The same is true for pathMethod. Because the pangenome fasta and index are not stored in the database, the parameters used for the pangenome are not stored either. Instead, the haplotype method and indexing parameters are encoded in the file names.</p>"},{"location":"UserInstructions/ImputeWithPHG_main/#using-a-different-aligner-to-map-reads","title":"Using a different aligner to map reads","text":"<p>The PHG pipeline uses minimap2 to map reads to a pangenome created from haplotypes in the PHG database. Users can choose to use a different aligner by first creating the pangenome then using an alternative aligner to create SAM or BAM files, which can then be used as inputs to the pipeline instead of fastq files. To create a pangenome fasta, run the pipeline with -imputeTarget pangenome. After the pangenome is created, then an alternative aligner can be used to map reads to the pangenome reference. It the output is saved as either a SAM or BAM file, then the other pipeline imputeTargets can be run with inputType=sam set in the config file.</p>"},{"location":"UserInstructions/ImputeWithPHG_main/#setting-parameters-in-the-config-file","title":"Setting parameters in the config file","text":""},{"location":"UserInstructions/ImputeWithPHG_main/#database-parameters","title":"Database Parameters","text":"<p>The database parameters are required to make a connection to the PHG database.They are</p> <ul> <li>host</li> <li>user</li> <li>password</li> <li>DBtype</li> <li>DB</li> </ul> <p>The default values are those needed to connect to a SQLite database and must be changed if using PostgreSQL. \"DB\", which is the path to the database does not have a default and is required.</p>"},{"location":"UserInstructions/ImputeWithPHG_main/#method-names","title":"Method names","text":"<ul> <li>pangenomeHaplotypeMethod</li> <li>readMethod</li> <li>pathHaplotypeMethod</li> <li>pathMethod</li> </ul> <p>The two xxxHaplotypeMethods must be set to a valid haplotype method, which can be a haplotype method or a comma-separated list of refRange group: haplotype method pairs. The pangenomeHaplotypeMethod describes the set of haplotypes used to construct the pangenome fasta. It can be any of the method names assigned to haplotypes when they were added to the database from assemblies or WGS or can be a consensus haplotype method. Both the pangenome writing step and the read mapping step have to use the same value for pangenomeHaplotypeMethod. As long as the same configuration file is used for both steps that happens automatically. The pathHaplotypeMethod describes the set of haplotypes to be used for pathing. It can be the same as the pangenomeHaplotypeMethod or a subset of it.</p> <p>The same data can be imputed in many ways using different combinations of parameter settings. Each combination of settings must be assigned a unique method name by the user and the resulting data is stored in the database under that method name. The parameter values for each method are stored in the database under \"method description\" as a JSON formatted string. The readMethod is associated with the read mapping parameters. If any of the read mapping parameters are changed, the method name should be changed as well. The pathMethod is a user assigned name that is associated with a particular set of path finding and read mapping parameters. </p>"},{"location":"UserInstructions/ImputeWithPHG_main/#key-files","title":"Key files","text":"<p>Key files contain a list of samples and files to be processed. The parameter \"keyFile\" names the key file used to generate read mappings. The pipeline generates two additional key files from that. The pathing step uses a key file with the same name as the named read mapping keyFile plus the extension \"_pathKeyFile.txt\". The file with an extension of \"_mappingIds.txt\" is the original keyFile with the database ids of the stored read mappings. Generally, this will not be needed but can be used to generate specialized analyses. After the pipeline has been run, the _pathKeyFile and _mappingIds can be found in the same directory as keyFile.</p> <p>More information about keyfiles can be found here</p>"},{"location":"UserInstructions/ImputeWithPHG_main/#file-locations","title":"File locations","text":"<p>A few parameters are required to define expected locations of files. The default values work with the PHG Docker when MakeDefaultDirectoryPlugin is used to create directories. These parameters are as follows:</p> <ul> <li>pangenomeDir [default = /phg/outputDir/pangenome/]</li> <li>fastqDir [default = /phg/inputDir/imputation/fastq/]</li> <li>samDir [default = /phg/inputDir/imputation/sam/]</li> <li>minimapLocation [default = minimap2]</li> <li>outVcfFile [no default, must be assigned a value if writing a VCF file]</li> </ul>"},{"location":"UserInstructions/ImputeWithPHG_main/#pangenome-minimap2-index-parameters","title":"Pangenome minimap2 index parameters","text":"<p>Check minimap2 manual for documentation</p> <ul> <li>indexKmerLength [minimap2 -k, default=21]</li> <li>indexWindowSize=11 [minimap2 -w, default=11]</li> <li>indexNumberBases=90G [minimap2 -I, default=90G]</li> </ul>"},{"location":"UserInstructions/ImputeWithPHG_main/#read-mapping-parameters-with-default-values","title":"Read mapping parameters with default values","text":"<ul> <li>lowMemMode [default=true]</li> <li>maxRefRangeErr [default=0.25]</li> <li>outputSecondaryStats [default=false]</li> <li>maxSecondary [default=20]</li> <li>fParam [default=f1000,5000]</li> </ul> <p>lowMemMode is an option that was introduced to reduce memory usage. It works so well that we recommend always using this setting and will probably remove the parameter at some point.</p> <p>When a read is mapped to a pangenome, it may map equally well to multiple haplotypes. All mappings with the lowest edit distance are used. When some of those reads map to different reference ranges, only those mapping to the most frequently hit reference range are used. If the proportion of the mappings to other reference ranges exceeds maxRefRangeErr, that read is discarded. When this parameter is set to a lower value, fewer reads are used but overall mapping accuracy may improve. The impact on final imputation accuracy depends on the pangenome being mapped and on other parameter settings. The optimal value for this parameter will need to be determined empirically.</p> <p>maxSecondary is the minimap2 -N parameter. When a read maps equally well to more than one haplotype, which is often the case, minimap2 calls one of those mappings the primary alignment and the rest secondary alignments. maxSecondary is the maximum number of secondary alignments saved. For that reason, maxSecondary should be greater than or equal to the maximum number of haplotypes per reference range. Otherwise, there may be a loss of imputation accuracy. fParam is the minimap2 -f parameter.</p>"},{"location":"UserInstructions/ImputeWithPHG_main/#pathing-parameters-with-default-values","title":"Pathing parameters with default values","text":"<ul> <li>maxNodes [default=1000]</li> <li>minReads [default=1]</li> <li>minTaxa [default=20]</li> <li>minTransitionProb [default=0.001]</li> <li>numThreads [default=3]</li> <li>probCorrect [default=0.99]</li> <li>removeEqual [default=true]</li> <li>splitNodes [default=true]</li> <li>splitProb [default=0.99]</li> <li>algorithmType [default=efficient]</li> <li>maxParents = [default = Int.MAX_VALUE]</li> <li>minCoverage = [default = 1.0]</li> <li> </li> <li>maxReads [default=10000]</li> <li>usebf [default=false]</li> <li>minP [default=0.8]</li> <li> </li> <li>maxHap [default=11]</li> <li>maxReadsKB [default=100]</li> </ul> <p>Reference ranges with more than maxNodes number of nodes or fewer than minTaxa taxa will not be used for imputation. For each sample, ranges are also eliminated that have fewer than minReads mapped to that range or more then maxReads (haploid pathing) or maxReadsKB (dipoid pathing) reads per KB of reference sequence. If removeEqual=true and minReads &gt; 0, ranges for which all taxa have the same  number of reads mapping to them will be eliminated.</p> <p>After any ranges have been removed due to the filter criteria, a node with a zero-length haplotype is added for all taxa that have no haplotype in any given reference range. If splitNodes=true, nodes with more than one taxon are split into individual taxa. The transition probability between nodes of the same taxon are set to splitProb and the transition probability between a taxon and any other taxon is set to (1 - splitProb) / (n - 1) where n = number of taxa in the graph. The minimum value for any transition is set to minTransitionProb. It is recommended that minTransitionProbe &lt;= (1 - splitProb) / (n - 1). It is recommended that splitNodes not be set to false because this generally leads to lower imputation accuracy. If splitNodes = false, the transition probability is set to the proportion of times that transition is observed in the haplotypes in the database while unobserved transitions are set to minTransition.</p> <p>The number of threads that will be used to impute individual paths is numThreads - 2 because 2 threads are reserved for other operations. As a result, numThreads must be set to 4 or more in order to run pathing in parallel.</p> <p>The algorithmType=classic uses the classic Viterbi (or forward-backward) algorithms. algorithmType=efficient uses a more efficient version of the algorithms but is considered experimental at this time pending more extensive testing.</p> <p>When usebf=false, the Viterbi algorithm is used for pathing. If usebf=true, the forward-backward algorithm is used and  only nodes with a probability of minP or greater will be included in the final path. The forward-backward algorithm has been implemented  for haploid path finding but not for diploid path finding.</p> <p>For diploid path finding, only ranges with maxHap haplotypes or fewer will be used.</p> <p>For path finding, the values of maxParents and minCoverage determine whether or not read mapping data is used to find the most likely parents for each individual being imputed. If maxParents is less than the number of parents in the pathHaplotypeMethod or coverage is  less than 1.0. Only the the most likely parents will be used for imputation. See the LikelyParentsPlugin for details.</p>"},{"location":"UserInstructions/ImputeWithPHG_main/#used-by-haploid-path-finding-only","title":"used by haploid path finding only","text":""},{"location":"UserInstructions/ImputeWithPHG_main/#used-by-diploid-path-finding-only","title":"used by diploid path finding only","text":""},{"location":"UserInstructions/ImputeWithPHG_main/#optional-parameters","title":"Optional parameters","text":"<ul> <li>pangenomeIndexName</li> <li>readMethodDescription</li> <li>pathMethodDescription</li> <li>debugDir</li> <li>bfInfoFile</li> <li>parentOutputFile</li> </ul> <p>To use the optional parameters in the configuration file, replace OPTIONAL with the appropriate value and uncomment the line.</p> <p>*pangenomeIndexName is the name of an externally supplied pangenome index. It is best not to supply this values and instead to let the pipeline build and imdex the pangenome  and assign the index name to ensure that it is compatible with the rest of the pipeline.</p> <p>readMethodDescription and pathMethodDescription are optional user descriptions for these methods which will be   included in the database description with the tag \"notes=\" in addition to the parameter values used for that method.</p> <p>If a value is supplied for debugDir, for each sample fastq a text file containing the read mapping haplotype counts will be written. The same data  will stored in the database whether or not a value is assigned to debugDir. Generally this is not needed but may be useful  for debugging obscure problems.</p> <p>When usebf=true, the forward-backward algorithm will calculate probabilities for every haplotype node in the graph. These values are used  to determine which haplotypes are included in the path. If a value is supplied for bfInfoFile all of the haplotype probabilities  will be written to that file.</p> <p>If the most likely parents are found and used for imputation, then a report of which parents were chosen for each line being imputed will be written to parentOutputFile, if it is set.</p>"},{"location":"UserInstructions/ImputeWithPHG_main/#workflow-summary","title":"Workflow Summary","text":"<p>The workflow proceeds through a few distinct steps as illustrated by the flowchart. They are</p> <ol> <li>Write haplotypes to a pangenome.fa fasta</li> <li>Map reads to the pangenome.fa using minimap2</li> <li>Use the reads to find the most likely path for each taxon through the Haplotype Graph</li> <li>Write a the variants on each path to a VCF file</li> </ol>"},{"location":"UserInstructions/ImputeWithPHG_main/#examples-executing-workflows","title":"Examples: Executing workflows","text":"<p>In the examples below, all log and debug messages will be written to the console. To write them to a log file append  <code>&gt; /path/to/logfile</code> or <code>&amp;&gt; /path/to/logfile</code>. In the first format, stdout will be written to the log file and stderr  will be written to the console. In the second format, both stdout and stderr will be written to file. imputeTarget, which describes the endpoint at which the workflow will stop, must equal one of pangenome, path, diploidPath, pathToVCF, or diploidPathToVCF. All steps required to reach the endpoint will be executed. If any of the steps have already been run, they will not be repeated but will be skipped instead.</p> <p>A. Create a pangenome Fasta File then stop</p> <ul> <li>config file parameters that need to be set: DB, pangenomeHaplotypeMethod</li> <li>command: <code>docker run --name pipeline_container --rm -v baseDir:/phg/ -t dockerImageName /tassel-5-standalone/run_pipeline.pl -Xmx20G -debug -configParameters myConfigFile.txt -ImputePipelinePlugin -imputeTarget pangenome -endPlugin</code></li> <li>result: The pangenome fasta will be written.</li> </ul> <p>B. Impute variants from fastq files - homozygous</p> <ul> <li>parameter: inputType=fastq</li> <li>command: <code>docker run --name pipeline_container --rm -v baseDir:/phg/ -t dockerImageName /tassel-5-standalone/run_pipeline.pl -Xmx20G -debug -configParameters myConfigFile.txt -ImputePipelinePlugin -imputeTarget path -endPlugin</code></li> <li>result: Read mapping counts and imputed paths will be stored in the PHG database.</li> </ul> <p>C. Impute variants from fastq files - heterozygous</p> <ul> <li>parameter: inputType=fastq</li> <li>command: <code>docker run --name pipeline_container --rm -v baseDir:/phg/ -t dockerImageName /tassel-5-standalone/run_pipeline.pl -Xmx20G -debug -configParameters myConfigFile.txt -ImputePipelinePlugin -imputeTarget diploidPath -endPlugin</code></li> <li>result: Read mapping counts and imputed paths will be stored in the PHG database.</li> </ul> <p>D. Impute variants from a VCF file - homozygous</p> <ul> <li>parameter: inputType=vcf</li> <li>command: <code>docker run --name pipeline_container --rm -v baseDir:/phg/ -t dockerImageName /tassel-5-standalone/run_pipeline.pl -Xmx20G -debug -configParameters myConfigFile.txt -ImputePipelinePlugin -imputeTarget path -endPlugin</code></li> <li>result: Read mapping counts and imputed paths will be stored in the PHG database.</li> </ul> <p>E. Export imputed VCF from fastq files - homozygous</p> <ul> <li>parameter: inputType=fastq</li> <li>command: <code>docker run --name pipeline_container --rm -v baseDir:/phg/ -t dockerImageName /tassel-5-standalone/run_pipeline.pl -Xmx20G -debug -configParameters myConfigFile.txt -ImputePipelinePlugin -imputeTarget pathToVCF -endPlugin</code></li> <li>result: Read mapping counts and imputed paths will be stored in the PHG database. Variants for all the paths stored for pathMethod will be written to a VCF file.</li> </ul> <p>F. View haplotypes with rPHG</p>"},{"location":"UserInstructions/ImputeWithPHG_main/#writing-a-vcf-for-a-subset-of-haplotypes-or-paths","title":"Writing a VCF for a subset of haplotypes or paths","text":"<p>Writing a VCF for a subset of imputed paths or for haplotypes stored in the PHG database can be useful, but cannot be done using the ImputePipelinePlugin. Instead the PathsToVCFPlugin must be called directly with a HaplotypeGraph containing the desired taxa  Following are example commands for performing those tasks:</p> <p>A. Write a VCF for a subset of assemblies (or WGS haplotypes) from the database. If the -taxa parameter is not used, SNPs for all the haplotypes for the method specified by the -methods parameter will be output.</p> <p>If running with PHG version 1.0 or greater, the GVCF Files used to stored the haplotype variants must first be downloaded to a local folder.  This folder is passed as a parameter to the HaplotypeGraphBuilderPlugin.  It can be specified along with other HapltoypeGraphBuilderPlugin parameters in the config file.  Then entry should look as below (replace the parameter value with your own local folder value - relative to docker if running the command via docker).</p> <pre><code>\nHaplotypeGraphBuilderPlugin.localGVCFFolder=/phg/remoteGvcfs\n</code></pre> <p>The diagram below shows the flow when creating a VCF file from PHG paths.  If running with PHG version 0.0.40 or earlier, you will not have the external VCF files to download.  Otherwise, the work flow is the same between the earlier and later PHG versions.</p> <p></p> <p>Note that all parameters may be stored in the config file or passed specifically on the command line.</p> <p>The VCF file is created by first calling HaplotypeGraphBuilderPlugin to create a graph that includes haplotypes based on the user specified methods.  This graph is passed along with a PATH method name and optional list of taxa to the ImportDiploidPathPlugin. The ImportDiplolidPathPlugin returns the graph along with a map of haplotype paths.  Finally, the data from the ImportDiploidPathPlugin output is sent as input to the PathsToVCFPlugin.</p> <p>When running the PathsToVCFPlugin we recommend using a positions list to limit the number of entries in the output VCF File to something manageable.  The positions list can be specified by Genotype file (i.e. VCF, Hapmap, etc.), bed file, or json file containing the requested positions.</p> <p>An example of chaining these plugin calls in a docker command is below.</p> <pre><code>\ndocker run --name pipeline_container --rm -v baseDir:/phg/ -t dockerImageName  \\\n   /tassel-5-standalone/run_pipeline.pl -Xmx200G -debug -configParameters &lt;configFile&gt; \\\n   -HaplotypeGraphBuilderPlugin -configFile &lt;configFile&gt; -methods haplotypeMethod1 \\\n         -includeVariantContexts true -includeSequences false -taxa taxon1,taxon2 -endPlugin \\\n   -ImportDiploidPathPlugin -pathMethodName pathMethod1 -endPlugin \\\n   -PathsToVCFPlugin -outputFile &lt;vcfOutputFile&gt; -referenceFasta &lt;referenceFasta.fa&gt; \\\n         -positions &lt;positions&gt; -endPlugin\n</code></pre> <p>An example of a singularity script to run these plugin calls for PHG version 1.0 or greater is below.</p> <pre><code>WORKING_DIR=/workdir/lcj34/phg_testGVCFasVariants/\nREMOTE_GVCF_DIR=/workdir/zrm22/remoteGvcfs/\nDOCKER_CONFIG_FILE=/phg/configDockerPostgres_gvcf.txt\nHAPLOTYPE_METHOD=NAM_CONSENSUS_mxDiv_10ToNeg4\nPOSITIONS_FILE=/phg/TUMPLANTBREEDING_Maize600k_elitelines_AGPv5_crossmapSORTED.vcf\n\n# make sure configFile has all parameters needed for HaplotypeGraphBuilderPlugin and \n# ImportDiploidPathPlugin and PathsToVCFPlugin\n# NOTe that include variants is TRUE - this is real vcf, not haplotypes\nsingularity exec -B ${WORKING_DIR}:/phg/ \\\n        -B ${REMOTE_GVCF_DIR}:/remoteGvcfs/ phgGVCF_v12.simg \\\n        /tassel-5-standalone/run_pipeline.pl -Xmx500G -debug -configParameters ${DOCKER_CONFIG_FILE} \\\n        -HaplotypeGraphBuilderPlugin -configFile ${DOCKER_CONFIG_FILE} -methods ${HAPLOTYPE_METHOD} \\\n        -includeVariantContexts true -endPlugin \\\n        -ImportDiploidPathPlugin  -endPlugin \\\n        -PathsToVCFPlugin -positions ${POSITIONS_FILE} -endPlugin\n\n</code></pre> <p>B. Write a VCF for a subset of paths</p> <p>Return to PHG version 1.0 or later home</p> <p>Return to PHG version 0.0.40 or earlier  home</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/ImputeWithPHG_paramTuning/","title":"Tune pathfinding parameters","text":"<p>This process does not currently exist, but when it does this page will walk users through common parameters to tune to produce the best variant calls from the PHG (e.g. minReads, taxa, transition probabilities)</p>"},{"location":"UserInstructions/ImputeWithPHG_paramTuning/#quick-start","title":"Quick Start","text":""},{"location":"UserInstructions/ImputeWithPHG_paramTuning/#details","title":"Details","text":""},{"location":"UserInstructions/ImputeWithPHG_paramTuning/#kitchen-sink","title":"Kitchen Sink","text":""},{"location":"UserInstructions/ImputeWithPHG_paramTuning/#troubleshooting","title":"Troubleshooting","text":"<p>Return to Step 3 pipeline</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/MakeDefaultDirectory/","title":"Make Default Directory Plugin","text":""},{"location":"UserInstructions/MakeDefaultDirectory/#note-this-plugin-is-designed-for-use-on-phg-version-0021-or-later","title":"Note: This Plugin is designed for use on PHG Version 0.0.21 or later.","text":"<p>MakeDefaultDirectoryPlugin is an optional first step to run in the PHG.  It will create a set of Directories on the local file system which will make the Docker commands in the next steps much easier. This plugin will also make a default config file with all of the necessary parameters which the plugins need to have set along with any default values.  Some parameters will need to be set before the next steps so please fill them out after MakeDefaultDirectoryPlugin is run. After this plugin is done, you must copy files into the required folders. </p>"},{"location":"UserInstructions/MakeDefaultDirectory/#quick-start","title":"Quick Start","text":"<p>Note this can be run outside of the docker as well.</p> <p>You will need to set WORKING_DIR for things to work correctly.  This should be the location where you are planning to put all the PHG files.  In the example below, change  to the specific tag you have downloaded from docker hub. <pre><code>WORKING_DIR=local/directory/where/files/will/go/\ndocker run --name create_directory --rm \\\n    -v ${WORKING_DIR}/:/phg/ \\\n    -t maizegenetics/phg:&lt;phg version tag&gt; \\\n    /tassel-5-standalone/run_pipeline.pl -debug -MakeDefaultDirectoryPlugin -workingDir /phg/ -endPlugin\n</code></pre>"},{"location":"UserInstructions/MakeDefaultDirectory/#details","title":"Details","text":"<p>This plugin will make the following directories and files in WORKING_DIR.</p> <pre><code>${WORKING_DIR}/config.txt\n${WORKING_DIR}/load_asm_genome_key_file.txt\n${WORKING_DIR}/load_wgs_genome_key_file.txt\n${WORKING_DIR}/readMapping_key_file.txt\n\n${WORKING_DIR}/inputDir/\n${WORKING_DIR}/inputDir/assemblies/\n${WORKING_DIR}/inputDir/loadDB/\n${WORKING_DIR}/inputDir/loadDB/bam/\n${WORKING_DIR}/inputDir/loadDB/bam/temp/\n${WORKING_DIR}/inputDir/loadDB/bam/dedup/\n${WORKING_DIR}/inputDir/loadDB/bam/mapqFiltered/\n${WORKING_DIR}/inputDir/loadDB/fastq/\n${WORKING_DIR}/inputDir/loadDB/gvcf/\n${WORKING_DIR}/inputDir/loadDB/temp/\n\n${WORKING_DIR}/inputDir/reference/\n${WORKING_DIR}/inputDir/reference/load_genome_data.txt\n\n${WORKING_DIR}/outputDir/\n${WORKING_DIR}/outputDir/align/\n${WORKING_DIR}/outputDir/align/gvcfs/\n\n${WORKING_DIR}/tempFileDir/\n\n${WORKING_DIR}/README.txt\n</code></pre> <p>The PHG plugins expect certain files to be loaded into these directories.</p> <p>The reference fasta will need to be copied/moved into </p> <pre><code>${WORKING_DIR}/inputDir/reference/\n</code></pre> <p>You will also need to fill out the following:</p> <pre><code>${WORKING_DIR}/inputDir/reference/load_genome_data.txt\n</code></pre> <p>An example of this file for the PHG version 0.1.X can be found here.</p> <p>An example of this file for the PHG version 0.0.X can be found here.  </p> <p>If you are planning on loading Haplotypes from Assemblies the assembly fasta files should be copied to folder:</p> <pre><code>${WORKING_DIR}/inputDir/assemblies/\n</code></pre> <p>When processing Haplotypes from Assemblies in the older PHG versions, you must split the assemblies by chromosome and copy each chromosome fasta to the folder mentioned above.  If you are running with PHG version  1.0 or later, there should only be 1 fasta file for each assembly genome. </p> <p>To process haplotypes from assemblies, you will also need to provide a valid keyfile or fill out the details in: </p> <pre><code>${WORKING_DIR}/load_asm_genome_key_file.txt\n</code></pre> <p>An example of a keyfile for processing assemblies via anchorwave with the PHG 1.0 version can be seen here: keyfile version 1.0</p> <p>Examples of key files for the older PHG versions can be seen here: keyfile example 1 and keyfile example 2.</p> <p>When processing from the PHG version 1.0 or later, see the section devoted to assembly processing.</p> <p>When processing from the old PHG version temporary files will be written to:</p> <pre><code>${WORKING_DIR}/outputDir/align/\n${WORKING_DIR}/outputDir/align/gvcfs/\n</code></pre> <p>If you are planning on loading Haplotypes from WGS fastqs, bams or GVCFs, you should put the corresponding files in:</p> <pre><code>${WORKING_DIR}/inputDir/loadDB/fastq/\n${WORKING_DIR}/inputDir/loadDB/bam/\n${WORKING_DIR}/inputDir/loadDB/gvcf/\n</code></pre> <p>You will also need to provide a valid keyfile or fill out the details in:</p> <pre><code>${WORKING_DIR}/load_wgs_genome_key_file.txt\n</code></pre> <p>Return to Step 1 pipeline version 0.0.40 or earlier</p> <p>Return to Step 1 pipeline version 1.0 or later</p>"},{"location":"UserInstructions/MakeInitialPHGDBPipeline/","title":"Make Initial PHG DB Pipeline Plugin","text":"<p>MakeInitialPHGDBPipelinePlugin is the first required step to build a PHG.  It will run the following plugins:</p> <pre><code>GetDBConnectionPlugin\nLoadAllIntervalsToPHGdbPlugin\nLiquibaseUpdatePlugin\n</code></pre> <p>GetDBConnectionPlugin will create a new DB with the required Schema and LoadAllIntervalsToPHGdbPlugin will then begin to populate the DB with Reference Range Information.  It will also load reference haplotypes into the PHG for the provided Reference Ranges.  LiquibaseUpdatePlugin is then run to mark the DB version as compatible with the current PHG software.   More information is in Details. </p>"},{"location":"UserInstructions/MakeInitialPHGDBPipeline/#quick-start","title":"Quick Start","text":"<p>Note this can be run outside of the docker as well.</p> <p>You will need a valid intervals file for this step.  For details on the interval file contents and creation, click here .</p> <p>You will need to set WORKING_DIR for things to work correctly.  This should be the location where MakeDefaultDirectory put all the PHG files.</p> <pre><code>WORKING_DIR=local/directory/where/MakeDefaultDirectory/was/run/\nDOCKER_CONFIG_FILE=/phg/config.txt\n\ndocker run --name create_initial_db --rm \\\n    -v ${WORKING_DIR}/:/phg/ \\\n    -t maizegenetics/phg:latest \\\n    /tassel-5-standalone/run_pipeline.pl -Xmx100G -debug -configParameters ${DOCKER_CONFIG_FILE} \\\n    -MakeInitialPHGDBPipelinePlugin -endPlugin\n\n</code></pre> <p>Note we are assuming that you used MakeDefaultDirectoryPlugin and are using a config file named config.txt.  If you use something else, replace the file name in DOCKER_CONFIG_FILE.</p>"},{"location":"UserInstructions/MakeInitialPHGDBPipeline/#details","title":"Details","text":"<p>As mentioned above, this plugin will first run GetDBConnectionPlugin which will create the DB schema for the PHG.  Then LoadAllIntervalsToPHGdbPlugin is run which will take in a BED file containing the coordinates for the Reference Ranges and will begin loading in that information.  Once the ReferenceRanges have been loaded, the plugin will also extract out sequence information for each Reference Range from the Reference Fasta file and will create a haplotype entry in the DB.  These haplotypes can be used for later analysis pipelines. </p> <p>Once the Reference Haplotypes are created, the LiquibaseUpdatePlugin is run to mark the DB as up-to-date, indicating that all existing database updates have been applied. This allows the DB to be updated via the Liquibase migration tool should future database changes occur.  Further information can be found here:</p> <p>LoadAllIntervalsToPHGdbPlugin</p> <p>LiquibaseUpdatePlugin</p>"},{"location":"UserInstructions/MakeInitialPHGDBPipeline/#config-file-parameters-for-this-step","title":"Config File Parameters for this step","text":"<p>The MakeInitialPHGDBPipelinePlugin makes use of parameters stored in a config file. The relevant config file parameters are as follows. Change the values to match your configuration.  The DBType must be either \"sqlite\" or \"postgres\".  The host, user, password and DB should match your own system configuration.  Change other file names to match the files you have defined.  The value for \"DB\" should be a docker-relevant value if you are running this plugins via docker.  When running via docker, you will have mounted your local folder to something inside docker.  </p> <pre><code>host=localHost\nuser=sqlite\npassword=sqlite\nDB=/phg/phg_db_name.db\nDBtype=sqlite\n# Load genome intervals parameters\nreferenceFasta=/phg/inputDir/reference/Ref.fa\nanchors=/phg/anchors.bed\ngenomeData=/phg/inputDir/reference/load_genome_data.txt\nrefServerPath=irods:/ibl/home/assemblies/\n#liquibase results output directory, general output directory\noutputDir=/phg/outputDir\nliquibaseOutdir=/phg/outputDir\n</code></pre> <p>This example config file follows the directory structure defined by MakeDefaultDirectoryPlugin but is the Docker specific version of the paths.  If running outside of the docker, please make the necessary adjustments.</p> <p>When running a postgresl data base, the value for \"DB\" should be just the db name without a path.  </p> <p>Note the \"refServerPath\" is a field that stores where future users may find the source genome Fasta used for populating the reference genome. It can be any string the user wants.  This example shows a directory on an i iRods server.</p> <p>Return to Step 1 pipeline version 0.0.40 or earlier</p> <p>Return to Step 1 pipeline version 0.1.0 or later</p>"},{"location":"UserInstructions/MergeFastqPluginDetailedDocs/","title":"MergeFastqPlugin Detailed Documentation","text":"<p>This plugin will take a key file and a directory full of single end GBS-like fastq files and will concatenated the reads together into batched fastq files. It will also output a Grouping File which holds records of which reads are in which batched fastq file and a series of alignment script templates which can be used to run minimap2.</p>"},{"location":"UserInstructions/MergeFastqPluginDetailedDocs/#example-command","title":"Example Command","text":"<pre><code>time ./tassel-5-standalone/run_pipeline.pl -Xmx100G -debug -MergeFastqPlugin \\\n                        -fastqDir fastqDir/ \\\n                        -outputDir outputDir/ \\\n                        -outputBAMDir outputBAMDir/ \\\n                        -makeBAMDir true \\\n                        -outputGroupingFile outputGroupingFile.txt \\\n                        -numToMerge 50 \\\n                        -scriptTemplate minimap2Script \\\n                        -numberOfScriptOutputs 2 \\\n                        -numThreads 20 \\\n                        -minimapLocation minimap2 \\\n                        -minimap2IndexFile phgIndexFile.mmi \\\n                        -keyFile keyFile.txt  -endPlugin &gt; mergeFastq.log\n\n</code></pre> <p>In the above command, MergeFastqPlugin will make batches of 50 fastq files per batch and will be saved in -outputDir.  It will also make 2(based on -numberOfScriptOutputs) scripts labeled minimap2Script_1.sh and minimap2Script_2.sh which can be run to execute the Minimap2 commands.  In those scripts, it will use the -outputBAMDir to setup the correct BAM directory in the script, the -numThreads to set the -t parameter in minimap2, the -minimap2Location to specify where the minimap2 executable actually is and the -minimap2IndexFile to specify where the PHG index file is stored.  If all these parameters are correctly set, the scripts should be able to be run without any modification. </p>"},{"location":"UserInstructions/MergeFastqPluginDetailedDocs/#parameter-documentation","title":"Parameter documentation","text":"<p>This plugin has the following Parameters available:</p> <ul> <li>-fastqDir(required): Name of the Fastq Directory to Process.  Must be an existing directory on the machine and must be filled with Single End GBS-like fastq files.</li> <li>-outputDir(required): Directory to write out the Merge Fastq files.</li> <li>-outputBAMDir(default: ):  This is the expected BAM directory written in the alignment scripts.  If left as the default , an easy find and replace can update it to the real BAM directory. <li>-makeBAMDir(default: true):  Option to add in a mkdir command to the alignment scripts.  If the directory already exists, this can be set to false.</li> <li>-outputGroupingFile(required): Output file to keep track of how the Fastq files were merged together.</li> <li>-numToMerge(default: 50): The number of fastq files to merge per batch.  Larger batches will require more processing time per batch, but overall less time as the minimap2 index only needs to be loaded once.</li> <li>-scriptTemplate(default: runMinimapTemp.sh):  This is the first portion of the output script name.  When this is run, a _1.sh, _2.sh ... will be appended to the end of the provided name.</li> <li>-numberOfScriptOutputs(default: 1): This sets the number of output scripts to write.  The Plugin will append _1.sh,_2.s ... to the end of the -scriptTemplate parameter.</li> <li>-numThreads(default: 20): This sets the number of threads parameter for minimap2 to use in the output scripts. </li> <li>-minimap2Location(default: minimap2): This sets the minimap2 executable location in the output script.  Consider changing this if Minimap2 is not stored on the PATH.</li> <li>-minimap2IndexFile(default: ): This sets the Index filename written to the output scripts.  By default it will write out  for an easy find and replace. <li>-keyFile(required) : File name for the genotyping keyfile.  This tab delimited keyfile must require the following columns: cultivar, flowcell_lane, and filename.  It must also have the headers as well. </li> <li>-minimapN(default: 60): Integer which sets the minimap2 -N parameter in the output alignment scripts.  Please refer back to the minimap2 documentation to change this appropriately.</li> <li>-minimapf(default: \"5000,6000\"): String which sets the minimap2 -f parameter in the output alignment scripts.  Please refer back to the minimap2 documentation to change this appropriately. </li> <li>-outputSams(default: false): If set to true, the output alignment scripts will output SAM files instead of BAM files.</li>"},{"location":"UserInstructions/MultisampleBAMPipeline/","title":"Step 3a: Create ReadMappings using the MultisampleBAM Pipeline","text":"<p>This step can replace the read mapping step of the impute pipeline for Single End GBS Fastq files.  The idea is that this pipeline is faster as it involves batching up the Fastq files and running minimap2 on each batch individually.  This allows minimap2 to only have to load in the PHG index file once per batch(instead of once per sample).  Then after alignment is run, we run MultisampleBAMToMappingPlugin to split out the SAM/BAM file into the original taxa and upload them to the DB for use in path finding.</p>"},{"location":"UserInstructions/MultisampleBAMPipeline/#quick-start","title":"Quick Start","text":"<ol> <li>Write a config file and fill in the unassigned values.</li> <li>Create a keyfile with information about reads to be imputed.</li> <li>Run MergeFastqPlugin</li> <li>Run the alignment scripts</li> <li>Run MultisampleBAMToMappingPlugin</li> </ol>"},{"location":"UserInstructions/MultisampleBAMPipeline/#details","title":"Details","text":""},{"location":"UserInstructions/MultisampleBAMPipeline/#writing-a-config-file","title":"Writing a config file","text":"<p>Before running the pipeline, you need to create a config file and put it in \"baseDir\" or some other directory accessible by the Docker. The config file name does not have to be config.txt. It also can be in any subdirectory of baseDir as long as the same file name location is used for -configParameters in the full pipeline command.  Two sample config files are provided, one for imputing haplotypes from sequence in fastq or SAM files (link here) and another for imputing haplotypes from variants in a VCF file (link here).  Values of UNASSIGNED in a config file must be filled in before using it. See the section Setting parameters in the config file for more information about what those values should be.</p>"},{"location":"UserInstructions/MultisampleBAMPipeline/#creating-a-key-file","title":"Creating a Key File","text":""},{"location":"UserInstructions/MultisampleBAMPipeline/#mergefastqplugin","title":"MergeFastqPlugin","text":"<p>This plugin will take a key file and a directory full of single end GBS-like fastq files and will concatenated the reads together into batched fastq files.  It will also output a Grouping File which holds records of which reads are in which batched fastq file and a series of alignment script templates which can be used to run minimap2.</p> <p>To run please use the following command:</p> <pre><code>time ./tassel-5-standalone/run_pipeline.pl -Xmx100G -debug -MergeFastqPlugin \\\n                        -fastqDir fastqDir/ \\\n                        -outputDir outputDir/ \\\n                        -outputBAMDir outputBAMDir/ \\\n                        -makeBAMDir true \\\n                        -outputGroupingFile outputGroupingFile.txt \\\n                        -numToMerge 50 \\\n                        -scriptTemplate minimap2Script \\\n                        -numberOfScriptOutputs 2 \\\n                        -numThreads 20 \\\n                        -minimapLocation minimap2 \\\n                        -minimap2IndexFile phgIndexFile.mmi \\\n                        -keyFile keyFile.txt  -endPlugin &gt; mergeFastq.log\n\n</code></pre> <p>In the above command, MergeFastqPlugin will make batches of 50 fastq files per batch and will be saved in -outputDir.  It will also make 2(based on -numberOfScriptOutputs) scripts labeled minimap2Script_1.sh and minimap2Script_2.sh which can be run to execute the Minimap2 commands.  In those scripts, it will use the -outputBAMDir to setup the correct BAM directory in the script, the -numThreads to set the -t parameter in minimap2, the -minimap2Location to specify where the minimap2 executable actually is and the -minimap2IndexFile to specify where the PHG index file is stored.  If all these parameters are correctly set, the scripts should be able to be run without any modification. </p> <p>For more detailed documentation please look here:</p>"},{"location":"UserInstructions/MultisampleBAMPipeline/#run-alignment-scripts","title":"Run Alignment Scripts","text":"<p>The next step is to run the alignment scripts created by the MergeFastqPlugin.  Each script will run a number of alignment commands to align each batched fastq file to the PHG minimap2 index.  It is recommended that you run these in parallel as it will greatly speed up processing time.</p> <p>To Execute the script run the following command</p> <pre><code>\ntime ./minimap2Script_1.sh\n</code></pre> <p>Once all the alignments have been processed, we can move onto the MultisampleBAMToMappingPlugin.</p>"},{"location":"UserInstructions/MultisampleBAMPipeline/#run-multisamplebamtomappingplugin","title":"Run MultisampleBAMToMappingPlugin","text":"<p>This plugin will take the BAM files along with the grouping file made in MergeFastqPlugin and will re-separate out each alignment into the original source fastq.  It will then be converted into a ReadMapping file and then uploaded to the DB for use in Path Finding. </p> <p>To run this step run the following command:</p> <pre><code>\ntime tassel-5-standalone/run_pipeline.pl -debug -Xmx100G -configParameters config.txt \\\n            -HaplotypeGraphBuilderPlugin \\\n                    -configFile config.txt \\\n                    -methods HAPLOTYPE_METHOD \\\n                    -includeVariantContexts false -endPlugin \\\n            -MultisampleBAMToMappingPlugin \\\n                    -keyFile keyFile.txt \\\n                    -fastqGroupingFile /outputGroupingFile.txt \\\n                    -samDir outputBAMDir/ \\\n                    -methodName GBS_PATH_METHOD \\\n                    -methodDescription GBS_PATH_METHOD \\\n                    -debugDir readMappingDebugDir/ \\\n                    -outputSecondaryStats false \\\n                    -isTestMethod true -endPlugin &gt; multisampleBAM.log\n</code></pre> <p>To have this work correctly, you will need to set HAPLOTYPE_METHOD to match what you used to create the PHG Fasta file.  The -keyFile, -fastqGroupingFile and the -samDir need to match the parameters in MergeFastqPlugin. -methodName is the name of this PathMethod to be stored and the -methodDescription is the description to be stored in the DB.  Once this is done running you are able to run PathFinding to impute the haplotypes.</p>"},{"location":"UserInstructions/PopulatePHGDBPipeline/","title":"Populate PHG DB Pipeline Plugin","text":"<p>PopulatePHGDBPipelinePlugin is the second step to create a PHG DB.</p> <p>If you provide assembly haplotype files(and a keyfile), the assemblies will be aligned to the reference genome.  There is a choice of using either mummer4 as an aligner (the original code) or using the newer anchorage aligner.  The anchorwave alignment has been shown to be more sensitive aligner than other aligners.  But it is a more time intensive step. Using mummer4 will be quicker but not as sensitive as anchorwave.  The pipeline uses mummer4 as the default.  </p> <p>You can change the default assembly aligner by setting the \"asmAligner\" parameter to \"anchor4\" when invoking the PopulatePHGDBPipelinePlugin plugin, however it is not recommended to used anchorwave with this particular plugin.  The initial alignment step is slow and takes alot of memory.  We suggest, when using anchorwave, that the AssemblyMAFFromAnchorWavePlugin be run on your assemblies.  The resulting MAF files should then be processed into GVCF files using the MAFToGVCFPlugin, and then loaded to the database using the LoadHaplotypesFromGVCFPlugin.</p> <p>If WGS fastq and a wgs keyfile are provided, the sequence will be aligned to reference with bwa mem and the resulting haplotypes stored in the database. Alternatively, aligned sequence can be loaded from BAM or GVCF files (and a keyfile). </p> <p>Both haplotype sequences and variants called from reference are stored in the DB.</p> <p>Once some haplotype information has been loaded into the DB, similar haplotypes can be clustered to create a set of consensus haplotypes. This is computationally and biologically useful and is highly recommended.</p>"},{"location":"UserInstructions/PopulatePHGDBPipeline/#quick-start-using-the-phg-docker","title":"Quick Start Using the PHG Docker","text":"<p>The value of WORKING_DIR should be the same as that used to run MakeDefaultDirectory.</p> <pre><code>WORKING_DIR=/local/directory/where/MakeDefaultDirectory/was/run/\nDOCKER_CONFIG_FILE=/phg/config.txt\n\ndocker run --name load_haplotypes --rm \\\n    -v ${WORKING_DIR}:/phg/ \\\n    -t maizegenetics/phg:latest \\\n    /tassel-5-standalone/run_pipeline.pl -Xmx100G -debug -configParameters ${DOCKER_CONFIG_FILE} \\\n    -PopulatePHGDBPipelinePlugin -endPlugin\n</code></pre>"},{"location":"UserInstructions/PopulatePHGDBPipeline/#details","title":"Details","text":"<p>This pipeline focuses mostly on creating haplotypes and loading them into the DB as well as merging similar haplotypes during the Create Consensus step.  Depending on the input data, different plugins will be run. It makes use of the following plugins/Docker Scripts:</p> <pre><code>AssemblyHaplotypesMultiThreadPlugin\nAssemblyMAFFromAnchorWavePlugin\nCreateHaplotypesFromFastq.groovy\nCreateHaplotypesFromBAM.groovy\nCreateHaplotypesFromGVCF.groovy\nHaplotypeGraphBuilderPlugin\nLoadHaplotypesFromGVCFPlugin\nMAFToGVCFPlugin\nRunHapConsensusPipelinePlugin\n</code></pre> <p>Further information about the plugins and scripts along with detailed parameter descriptions can be found here:</p> <p>Align assemblies using anchorwave aligner</p> <p>Align assemblies using mummer4 aligner</p> <p>WGS Haplotype Creation</p> <p>Making Consensus Haplotypes</p> <p>This pipeline will make use of parameters stored in a config file.  There are a lot of optional parameters for these plugins.  Please refer back to the links above for more information.</p> <p>A sample config file with a basic set of pipeline parameters follows. The first 5 values are for database access.  They should be changed to match values specific to your database.  The database type must be either \"sqlite\" or \"postgres\".  Not all optional parameters are included:</p> <pre><code>host=localHost\nuser=sqlite\npassword=sqlite\nDB=/phg/phg_db_name.db\nDBtype=sqlite\n\nnumThreads=5\n\n#Liquibase output directory\nliquibaseOutdir=/phg/outputDir\n\n# Haplotype creation parameters\nreferenceFasta=/phg/inputDir/reference/Ref.fa\n\n# Assembly Loading Parameters\n# Set the desired assembly aligner when running via PopulatePHGDBPopelinePlugin\nPopulatePHGDBPipelinePlugin.asmAligner=anchorwave\n\n#Assembly Loading Parameters when running the mummer4 aligner\nasmMethodName=anchorwave\nasmKeyFile=/phg/asm_keyFile.txt\noutputDir=/phg/outputDir/align/\ngvcfOutputDir=/phg/outputDir/align/gvcfs/\n\n#Assembly Loading Parameters when running the anchorwave aligner\n# Note running the anchorwave aligner from PopulatePHGDBPipelinePlugin involves\n# the execution of 3 plugins.  PopulatePHGDBPipelinPlugin needs the AssemblyMAFFromAnchorWavePlugin \n# parameters to be defined in the config file.  It will infer the parameters needed for the other 3\n# plugins from the AssemblyMAFFromAnchorWavePlugin parameters. It will also create the keyfile needed\n# for both the MAFToGVCFPlugin and LoadHaplotypesFromGVCFPlugin classes.\nAssemblyMAFFromAnchorWavePlugin.outputDir=/phg/outputDir\nAssemblyMAFFromAnchorWavePlugin.keyFile=/phg/Ia453_K0326Y_keyfile.txt\nAssemblyMAFFromAnchorWavePlugin.gffFile=/phg/inputDir/reference/Zm-B73-REFERENCE-NAM-5.0_Zm00001e.1.gff3\nAssemblyMAFFromAnchorWavePlugin.refFasta=/phg/inputDir/reference/Zm-B73-REFERENCE-NAM-5.0.fa\nAssemblyMAFFromAnchorWavePlugin.threadsPerRun=4\nAssemblyMAFFromAnchorWavePlugin.numRuns=2\n\n\n# WGS Loading Parameters\nwgsMethodName=GATK_PIPELINE\nhaplotypeMethodName=GATK_PIPELINE\nrefRangeMethods=FocusRegion,FocusComplement\nwgsKeyFile=/phg/keyFile.txt\ngvcfDir=/phg/inputDir/loadDB/gvcf/\nextendedWindowSize=0\n\n# WGS Filtering Parameters\nGQ_min=50\nDP_poisson_min=.01\nDP_poisson_max=.99\nfilterHets=true\n\n# CONSENSUS Parameters\ninputConsensusMethods=GATK_PIPELINE\nconsensusMethodName=CONSENSUS\nminFreq=0.05\nmaxClusters=30\nminSites=30\nminCoverage=0.1\nminTaxa=1\nmxDiv=0.001\n</code></pre> <p>This example config file follows the directory structure defined by MakeDefaultDirectoryPlugin but is the Docker specific version of the paths.  Docker paths all start with \"/phg/\". If running outside of the docker, please make the necessary adjustments.</p>"},{"location":"UserInstructions/PopulatePHGDBPipeline/#controlling-which-steps-are-run","title":"Controlling Which Steps Are Run","text":"<p>Running the assembly loading step requires an assembly key file. To skip loading haplotypes from assemblies, comment(#)  or delete the line in the config file that starts with \"asmKeyFile\".  Running the WGS loading step requires a WGS key file. To skip loading haplotypes from WGS, comment (#)  or delete the line in the config file that starts with \"wgsKeyFile\". Running the step that creates consensus haplotypes requires a consensus method name. To avoid running consensus,  comment(#) or delete the line starting with \"consensusMethodName\". The parameter \"inputConsensusMethods\" determines which haplotypes are used as the basis of the consensus haplotypes.  In the sample config file, that is set to GATK_PIPELINE, which creates the consensus from the WGS haplotypes.  Setting inputConsensusMethods=mummer4 would create consensus haplotypes from the assembly haplotypes.</p> <p>If you have already created/loaded haplotypes to the database and wish to only run consensus, then comment or delete both the \"asmKeyFile\" and \"wgsKeyFile\" lines.</p>"},{"location":"UserInstructions/SampleHaplotypeConfigFile/","title":"Sample Config file for Loading Haplotypes.","text":"<p>To Load haplotypes a config file will be needed.  Here is a sample one with most of the needed information filled out.  Anything marked with UNASSIGNED will need to be updated with a correct value and anything OPTIONAL is optional and depending on your use should be removed.</p> <pre><code>#!\n\n###Example config file. \n### Anything marked with UNASSIGNED needs to be set for at least one of the steps\n### If it is marked as OPTIONAL, it will only need to be set if you want to run specific steps. \nhost=localHost\nuser=sqlite\npassword=sqlite\nDB=/phg/smallSeqDB.db\nDBtype=sqlite\n\n#System parameters.  Xmx is the java heap size and numThreads will be used to set threads available for multithreading components.\nXmx=10G\nnumThreads=10\n\nliquibaseOutdir=/phg/outputDir\n\nanchors=***UNASSIGNED***\ngenomeData=***UNASSIGNED***\n\nreferenceFasta=***UNASSIGNED***\n\nasmMethodName=mummer4\nasmKeyFile=***OPTIONAL***\n\nwgsMethodName=GATK_PIPELINE\nwgsKeyFile=***OPTIONAL***\n\nconsensusMethodName=CONSENSUS\ninputConsensusMethods=GATK_PIPELINE\n\nfastqFileDir=/phg/inputDir/loadDB/fastq/\ndedupedBamDir=/phg/inputDir/loadDB/bam/dedup/\n\ngvcfFileDir=/phg/inputDir/loadDB/gvcf/\nfilteredBamDir=/phg/inputDir/loadDB/bam/mapqFiltered/\n\n# BAM and GVCF uploading parameters\nmapQ=48\nrefRangeMethods=FocusRegion,FocusComplement\nextendedWindowSize=1000\n\n# WGS Haplotype Filtering criteria.  These are the defaults.\nGQ_min=50\nQUAL_min=200\nDP_poisson_min=.01\nDP_poisson_max=.99\nfilterHets=true\n\n## If you have a sentieon license you can set the server location here(and remove the #).  If it is set, it will use Sentieon instead of GATK\n#sentieon_license= ***OPTIONAL***\n\n\n##Consensus Plugin Parameters\nminFreq=0.5\nmaxClusters=30\nminSite=30\nminCoverage=0.1\nmaxThreads=10\nminTaxa=1\nmxDiv=0.01\n\n#This sets the type of clustering mode.\n#Valid params are: upgma, upgma_assembly, and kmer_assembly\n#The two assembly parameters are designed for assembly haplotypes and will choose a representative haplotype as the consensus instead of attempting to merge calls like with upgma.\nclusteringMode=upgma\n\n#If you want to use an assembly clusteringMode, you must have a ranking file.\n#The ranking file must be a tab separated list of taxon\\trankingScore where higher numbers are a better rank.  This file is used to chose the representative haplotype\nrankingFile=***OPTIONAL***\n\n##Optional if you want to use kmer_assembly as the clusteringMode. Otherwise is ignored \nkmerSize=7\ndistanceCalculation=Euclidean\n\n\n\n</code></pre>"},{"location":"UserInstructions/UpdatePHGSchema/","title":"Update an existing PHG database to latest schema with liquibase","text":""},{"location":"UserInstructions/UpdatePHGSchema/#quick-start","title":"Quick Start","text":"<p>Running one of the pipeline plugins automatically uses liquibase to make sure the software and database versions are compatible. Running individual plugins, however, does not result in the running of liquibase. For these cases the liquibase check must be run independently as follows:</p> <p>Download the latest version of the PHG Docker. If an earlier version of the PHG Docker is used, the liquibase check will make sure that the PHG database is compatible with that version. However, it is not possible to roll back to an earlier version of the database.  </p> <p>WHen pulling the phg docker, it is best to indicate the specific version to pull.  While \"latest\" is an available tag, it may be confusing if later you need to know which specifig PHG version was run.  In the example below, the tag is \"1.2\".  Your tag may be different.</p> <pre><code>docker pull maizegenetics/phg:1.2\n\n</code></pre> <p>Run the liquibase plugin. You may need to change the path to the config file or the output directory. At a minimum the config file needs to contain the database access parameters. Also if you have an outputDir= parameter setting in the config file, it does not need to be included on the command line.  Note the tag version in the command below matches the version from the \"docker pull\" command above.</p> <pre><code>WORKING_DIR=local/directory/where/files/will/go/\ndocker run --name create_directory --rm \\\n    -v ${WORKING_DIR}/:/phg/ \\\n    -t maizegenetics/phg:1.2 \\\n    /tassel-5-standalone/run_pipeline.pl -debug -configParameters /phg/config.txt \\\n        -CheckDBVersionPlugin -outputDir /phg/outputDir -endPlugin \\\n        -LiquibaseUpdatePlugin -outputDir /phg/outputDir -endPlugin\n</code></pre> <p>After running the plugin check for the <code>run_yes.txt</code> file that will be output. This means your database is recent enough to allow an update to be applied.  If instead you see a <code>run_no.txt</code> file, your database cannot be updated to the current PHG db schema.  In this case, you can try using an older version of the PHG, one that matches the version used when the db was last created/updated. If it was created with a variants table, the latest version that can be run against it is the PHG 0.0.40 version.</p> <p>If the update was attempted, check the liquibase_output.log and liquibase_error.log files to verify the update has completed successfully.</p>"},{"location":"UserInstructions/UpdatePHGSchema/#details","title":"Details","text":"<p>Because the PHG is under active development, the database schema may change periodically. In most cases when this happens it will be possible to update the existing database with the liquibase migration tool, and the database will not need to be created from scratch.  Note that an update is not possible between 0.0.40 and earlier versions to PHG versions 1.0 and greater.</p>"},{"location":"UserInstructions/UpdatePHGSchema/#kitchen-sink","title":"Kitchen Sink","text":"<p>The PHG uses the liquibase data migration tool to update user databases. This tool works on both sqlite and postgresql databases.</p> <p>When creating a new database, the db is current and there is no need to run the liquibase Docker. However, the liquibase check should be run whenever a user is pulling a new PHG image to run with an existing database. Users may pull the most recent version or any preceding version, though a more recent database cannot be rolled back to an earlier version. </p> <p>If a \"run_no.txt\" file is created, it means your database is too old to be updated. In this scenario, you must start the database fresh. The run_no.txt file will contain a message indicating what schema is missing from the database, which should give an indication of the database versioning.</p> <p>If the run_yes.txt file is present, then liquibase attempted to perform the database migrations.  Check the liquibase_error.log and liquibase_output.log files in your mounted output directory to verify the migrations ran without error.</p> <p>The Quick Start command calls the LiquibaseUpdatePlugin. This plugin runs the liquibase \"update\" command. The software identifies database changes that have already been run against the database and skips them. </p>"},{"location":"UserInstructions/UpdatePHGSchema/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>If you try to update your PHG database and get a <code>run_no.txt</code> file output, your database is likely too old to update. In this case you will need to create a new database because your current database is not guaranteed to work in subsequent imputation steps.</li> </ol> <p>Return to PHG version 1.0 or later home</p> <p>Return to PHG version 0.0.40 or earlier home</p> <p>Return to Wiki Home</p>"},{"location":"UserInstructions/help_biostars/","title":"How to ask questions about the PHG","text":""},{"location":"UserInstructions/help_biostars/#step-0-create-a-userid-on-biostars","title":"Step 0: Create a userID on Biostars","text":"<p>Skip to step 1 if you already have a user ID</p> <ul> <li> <p>Click on the Login tab near the top of the home page or on this link: https://www.biostars.org/accounts/signup/</p> </li> <li> <p>On the login page, sign in with either your Google credentials or use your own custom ID and password.</p> </li> </ul>"},{"location":"UserInstructions/help_biostars/#step-1-ask-your-question","title":"Step 1: Ask your question","text":"<ul> <li> <p>Log in and find the new posts through the \u201cNew Post\u201d button on the top right-hand side of the screen</p> </li> <li> <p>On the New Post page, provide the following:</p> <ul> <li> <p>Post Title: A clearly defined title for your question</p> </li> <li> <p>Post Type: Question</p> </li> <li> <p>Post Tags: To ensure your question is answered by the Buckler Lab staff, please use:</p> <p>phg - questions relating to the practical haplotype graph (PHG) framework</p> <p>rphg - questions relating to the R frontend for the PHG</p> </li> <li> <p>Enter your post below: your clearly stated, unambiguous question in proper formatting.</p> </li> </ul> </li> <li> <p>Click submit </p> </li> </ul>"},{"location":"UserInstructions/help_biostars/#step-2-wait-for-potential-remedies-and-follow-up-questions","title":"Step 2: Wait for potential remedies and follow up questions","text":"<p>To follow the phg or rphg tags or receive email notifications for PHG or rPHG questions:</p> <ul> <li> <p>Click on your userid on the main biostars page</p> </li> <li> <p>Click edit profile. You can enter tags to show up on myTags or email there</p> </li> </ul> <p>Return to Wiki Home</p>"}]}